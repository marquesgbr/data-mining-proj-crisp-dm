{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d6659d1",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o de Bibliotecas e Configura√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7090239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√µes essenciais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c202a3",
   "metadata": {},
   "source": [
    "## 2. Carregamento e An√°lise Inicial dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd2f2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset de sepsis...\n",
      "Shape do dataset: (1241768, 42)\n",
      "Total de registros: 1,241,768\n",
      "\n",
      "Informa√ß√µes do Dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1241768 entries, 0 to 1241767\n",
      "Data columns (total 42 columns):\n",
      " #   Column            Non-Null Count    Dtype  \n",
      "---  ------            --------------    -----  \n",
      " 0   Hour              1241768 non-null  int64  \n",
      " 1   HR                1119123 non-null  float64\n",
      " 2   O2Sat             1079708 non-null  float64\n",
      " 3   Temp              419945 non-null   float64\n",
      " 4   SBP               1060857 non-null  float64\n",
      " 5   MAP               1087236 non-null  float64\n",
      " 6   DBP               852691 non-null   float64\n",
      " 7   Resp              1051181 non-null  float64\n",
      " 8   EtCO2             46047 non-null    float64\n",
      " 9   BaseExcess        67324 non-null    float64\n",
      " 10  HCO3              52334 non-null    float64\n",
      " 11  FiO2              103618 non-null   float64\n",
      " 12  pH                86094 non-null    float64\n",
      " 13  PaCO2             69132 non-null    float64\n",
      " 14  SaO2              42777 non-null    float64\n",
      " 15  AST               20144 non-null    float64\n",
      " 16  BUN               85440 non-null    float64\n",
      " 17  Alkalinephos      19954 non-null    float64\n",
      " 18  Calcium           73269 non-null    float64\n",
      " 19  Chloride          56701 non-null    float64\n",
      " 20  Creatinine        75809 non-null    float64\n",
      " 21  Bilirubin_direct  2393 non-null     float64\n",
      " 22  Glucose           212578 non-null   float64\n",
      " 23  Lactate           33238 non-null    float64\n",
      " 24  Magnesium         78652 non-null    float64\n",
      " 25  Phosphate         50011 non-null    float64\n",
      " 26  Potassium         115900 non-null   float64\n",
      " 27  Bilirubin_total   18518 non-null    float64\n",
      " 28  TroponinI         11743 non-null    float64\n",
      " 29  Hct               109980 non-null   float64\n",
      " 30  Hgb               91759 non-null    float64\n",
      " 31  PTT               36690 non-null    float64\n",
      " 32  WBC               79613 non-null    float64\n",
      " 33  Fibrinogen        8203 non-null     float64\n",
      " 34  Platelets         73790 non-null    float64\n",
      " 35  Age               1241768 non-null  float64\n",
      " 36  Gender            1241768 non-null  float64\n",
      " 37  Unit1             751787 non-null   float64\n",
      " 38  Unit2             751787 non-null   float64\n",
      " 39  HospAdmTime       1241762 non-null  float64\n",
      " 40  ICULOS            1241768 non-null  float64\n",
      " 41  SepsisLabel       1241768 non-null  float64\n",
      "dtypes: float64(41), int64(1)\n",
      "memory usage: 397.9 MB\n",
      "None\n",
      "Shape do dataset: (1241768, 42)\n",
      "Total de registros: 1,241,768\n",
      "\n",
      "Informa√ß√µes do Dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1241768 entries, 0 to 1241767\n",
      "Data columns (total 42 columns):\n",
      " #   Column            Non-Null Count    Dtype  \n",
      "---  ------            --------------    -----  \n",
      " 0   Hour              1241768 non-null  int64  \n",
      " 1   HR                1119123 non-null  float64\n",
      " 2   O2Sat             1079708 non-null  float64\n",
      " 3   Temp              419945 non-null   float64\n",
      " 4   SBP               1060857 non-null  float64\n",
      " 5   MAP               1087236 non-null  float64\n",
      " 6   DBP               852691 non-null   float64\n",
      " 7   Resp              1051181 non-null  float64\n",
      " 8   EtCO2             46047 non-null    float64\n",
      " 9   BaseExcess        67324 non-null    float64\n",
      " 10  HCO3              52334 non-null    float64\n",
      " 11  FiO2              103618 non-null   float64\n",
      " 12  pH                86094 non-null    float64\n",
      " 13  PaCO2             69132 non-null    float64\n",
      " 14  SaO2              42777 non-null    float64\n",
      " 15  AST               20144 non-null    float64\n",
      " 16  BUN               85440 non-null    float64\n",
      " 17  Alkalinephos      19954 non-null    float64\n",
      " 18  Calcium           73269 non-null    float64\n",
      " 19  Chloride          56701 non-null    float64\n",
      " 20  Creatinine        75809 non-null    float64\n",
      " 21  Bilirubin_direct  2393 non-null     float64\n",
      " 22  Glucose           212578 non-null   float64\n",
      " 23  Lactate           33238 non-null    float64\n",
      " 24  Magnesium         78652 non-null    float64\n",
      " 25  Phosphate         50011 non-null    float64\n",
      " 26  Potassium         115900 non-null   float64\n",
      " 27  Bilirubin_total   18518 non-null    float64\n",
      " 28  TroponinI         11743 non-null    float64\n",
      " 29  Hct               109980 non-null   float64\n",
      " 30  Hgb               91759 non-null    float64\n",
      " 31  PTT               36690 non-null    float64\n",
      " 32  WBC               79613 non-null    float64\n",
      " 33  Fibrinogen        8203 non-null     float64\n",
      " 34  Platelets         73790 non-null    float64\n",
      " 35  Age               1241768 non-null  float64\n",
      " 36  Gender            1241768 non-null  float64\n",
      " 37  Unit1             751787 non-null   float64\n",
      " 38  Unit2             751787 non-null   float64\n",
      " 39  HospAdmTime       1241762 non-null  float64\n",
      " 40  ICULOS            1241768 non-null  float64\n",
      " 41  SepsisLabel       1241768 non-null  float64\n",
      "dtypes: float64(41), int64(1)\n",
      "memory usage: 397.9 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Carregar dataset original\n",
    "print(\"Carregando dataset de sepsis...\")\n",
    "df = pd.read_csv('dataset_sepsis_train.csv')\n",
    "\n",
    "print(f\"Shape do dataset: {df.shape}\")\n",
    "print(f\"Total de registros: {len(df):,}\")\n",
    "# Informa√ß√µes b√°sicas\n",
    "print(\"\\nInforma√ß√µes do Dataset:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c898bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise dos valores missing\n",
    "print(\"AN√ÅLISE DE VALORES FALTANTES:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_stats = pd.DataFrame({\n",
    "    'Coluna': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percent': (df.isnull().sum() / len(df)) * 100\n",
    "})\n",
    "\n",
    "missing_stats = missing_stats[missing_stats['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False)\n",
    "print(missing_stats.to_string(index=False))\n",
    "\n",
    "# Visualiza√ß√£o dos valores faltantes\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
    "plt.title('Padr√£o de Valores Faltantes no Dataset', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Colunas')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914894d4",
   "metadata": {},
   "source": [
    "## 3. Cria√ß√£o do Identificador de Paciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b13f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar identificador √∫nico de paciente baseado em Gender + Age\n",
    "print(\"üë§ CRIANDO IDENTIFICADOR DE PACIENTE\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Verificar se temos Gender e Age\n",
    "if 'Gender' in df.columns and 'Age' in df.columns:\n",
    "    # Criar PATIENT_ID como combina√ß√£o de Gender e Age (arredondada)\n",
    "    df['Age_Rounded'] = df['Age'].round(1)  # Arredondar idade para 1 casa decimal\n",
    "    df['PATIENT_ID'] = df['Gender'].astype(str) + '_' + df['Age_Rounded'].astype(str)\n",
    "    \n",
    "    print(f\"‚úÖ PATIENT_ID criado com sucesso!\")\n",
    "    print(f\"üë• Total de pacientes √∫nicos: {df['PATIENT_ID'].nunique():,}\")\n",
    "    print(f\"üìä Registros por paciente (m√©dia): {len(df) / df['PATIENT_ID'].nunique():.1f}\")\n",
    "    \n",
    "    # An√°lise da distribui√ß√£o de registros por paciente\n",
    "    patient_counts = df['PATIENT_ID'].value_counts()\n",
    "    \n",
    "    print(f\"\\nüìà Estat√≠sticas de registros por paciente:\")\n",
    "    print(f\"   M√≠nimo: {patient_counts.min()} registros\")\n",
    "    print(f\"   M√°ximo: {patient_counts.max()} registros\")\n",
    "    print(f\"   Mediana: {patient_counts.median():.1f} registros\")\n",
    "    print(f\"   M√©dia: {patient_counts.mean():.1f} registros\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Colunas Gender ou Age n√£o encontradas!\")\n",
    "    # Criar ID sequencial como fallback\n",
    "    df['PATIENT_ID'] = range(len(df))\n",
    "    print(\"üîÑ Usando ID sequencial como fallback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise da distribui√ß√£o por g√™nero e idade\n",
    "if 'Gender' in df.columns:\n",
    "    print(\"\\nüë• AN√ÅLISE DEMOGR√ÅFICA:\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    # Distribui√ß√£o por g√™nero\n",
    "    gender_dist = df['Gender'].value_counts()\n",
    "    print(\"Distribui√ß√£o por G√™nero:\")\n",
    "    for gender, count in gender_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  {gender}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Estat√≠sticas de idade\n",
    "    if 'Age' in df.columns:\n",
    "        print(f\"\\nüìä Estat√≠sticas de Idade:\")\n",
    "        print(f\"   M√≠nima: {df['Age'].min():.1f} anos\")\n",
    "        print(f\"   M√°xima: {df['Age'].max():.1f} anos\")\n",
    "        print(f\"   M√©dia: {df['Age'].mean():.1f} anos\")\n",
    "        print(f\"   Mediana: {df['Age'].median():.1f} anos\")\n",
    "        \n",
    "        # Visualiza√ß√£o da distribui√ß√£o de idade por g√™nero\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(data=df, x='Age', hue='Gender', bins=30, alpha=0.7)\n",
    "        plt.title('Distribui√ß√£o de Idade por G√™nero', fontweight='bold')\n",
    "        plt.xlabel('Idade')\n",
    "        plt.ylabel('Frequ√™ncia')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(data=df, x='Gender', y='Age')\n",
    "        plt.title('Boxplot de Idade por G√™nero', fontweight='bold')\n",
    "        plt.ylabel('Idade')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becab850",
   "metadata": {},
   "source": [
    "## 4. An√°lise da Vari√°vel Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise da vari√°vel target (SepsisLabel)\n",
    "print(\"üéØ AN√ÅLISE DA VARI√ÅVEL TARGET (SepsisLabel)\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if 'SepsisLabel' in df.columns:\n",
    "    target_dist = df['SepsisLabel'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"Distribui√ß√£o da Vari√°vel Target:\")\n",
    "    for label, count in target_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        status = \"Sem Sepsis\" if label == 0 else \"Com Sepsis\"\n",
    "        print(f\"  {status} ({label}): {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Calcular taxa de desbalanceamento\n",
    "    imbalance_ratio = target_dist.max() / target_dist.min()\n",
    "    print(f\"\\n‚öñÔ∏è  Taxa de Desbalanceamento: {imbalance_ratio:.1f}:1\")\n",
    "    \n",
    "    # An√°lise por paciente √∫nico\n",
    "    patient_sepsis = df.groupby('PATIENT_ID')['SepsisLabel'].max()  # Se teve sepsis em algum momento\n",
    "    patient_target_dist = patient_sepsis.value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\nüë• Distribui√ß√£o por Paciente √önico:\")\n",
    "    for label, count in patient_target_dist.items():\n",
    "        percentage = (count / len(patient_sepsis)) * 100\n",
    "        status = \"Nunca teve Sepsis\" if label == 0 else \"Teve Sepsis\"\n",
    "        print(f\"  {status}: {count:,} pacientes ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Visualiza√ß√£o\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Gr√°fico 1: Distribui√ß√£o geral\n",
    "    target_dist.plot(kind='bar', ax=axes[0], color=['lightblue', 'lightcoral'], alpha=0.8)\n",
    "    axes[0].set_title('Distribui√ß√£o Geral - SepsisLabel', fontweight='bold')\n",
    "    axes[0].set_xlabel('SepsisLabel')\n",
    "    axes[0].set_ylabel('Contagem')\n",
    "    axes[0].set_xticklabels(['Sem Sepsis', 'Com Sepsis'], rotation=0)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 2: Distribui√ß√£o por paciente\n",
    "    patient_target_dist.plot(kind='bar', ax=axes[1], color=['lightgreen', 'salmon'], alpha=0.8)\n",
    "    axes[1].set_title('Distribui√ß√£o por Paciente √önico', fontweight='bold')\n",
    "    axes[1].set_xlabel('Status de Sepsis')\n",
    "    axes[1].set_ylabel('N√∫mero de Pacientes')\n",
    "    axes[1].set_xticklabels(['Nunca teve', 'Teve Sepsis'], rotation=0)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Coluna SepsisLabel n√£o encontrada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958cf9d",
   "metadata": {},
   "source": [
    "## 5. Imputa√ß√£o Inteligente por Paciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc55ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas num√©ricas para imputa√ß√£o (excluindo ID e target)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cols_to_exclude = ['PATIENT_ID', 'SepsisLabel', 'Age_Rounded']\n",
    "numeric_cols = [col for col in numeric_cols if col not in cols_to_exclude]\n",
    "\n",
    "print(\"üîÑ IMPUTA√á√ÉO INTELIGENTE POR PACIENTE\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"üìä Colunas num√©ricas para imputa√ß√£o: {len(numeric_cols)}\")\n",
    "print(f\"üìã Colunas: {numeric_cols[:5]}...\" if len(numeric_cols) > 5 else f\"üìã Colunas: {numeric_cols}\")\n",
    "\n",
    "# Criar c√≥pia do dataframe para imputa√ß√£o\n",
    "df_imputed = df.copy()\n",
    "\n",
    "# Fun√ß√£o de imputa√ß√£o por paciente\n",
    "def impute_patient_data(patient_data):\n",
    "    \"\"\"\n",
    "    Aplica imputa√ß√£o inteligente para um paciente espec√≠fico:\n",
    "    1. Forward fill (propagar √∫ltimo valor v√°lido)\n",
    "    2. Backward fill (propagar pr√≥ximo valor v√°lido)\n",
    "    3. M√©dia do paciente para valores ainda faltantes\n",
    "    \"\"\"\n",
    "    patient_imputed = patient_data.copy()\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in patient_imputed.columns:\n",
    "            # 1. Forward fill\n",
    "            patient_imputed[col] = patient_imputed[col].fillna(method='ffill')\n",
    "            \n",
    "            # 2. Backward fill\n",
    "            patient_imputed[col] = patient_imputed[col].fillna(method='bfill')\n",
    "            \n",
    "            # 3. M√©dia do paciente (se ainda houver NaN)\n",
    "            if patient_imputed[col].isnull().any():\n",
    "                patient_mean = patient_imputed[col].mean()\n",
    "                if not pd.isna(patient_mean):\n",
    "                    patient_imputed[col] = patient_imputed[col].fillna(patient_mean)\n",
    "    \n",
    "    return patient_imputed\n",
    "\n",
    "# Aplicar imputa√ß√£o por paciente\n",
    "print(\"\\nüîÑ Aplicando imputa√ß√£o por paciente...\")\n",
    "\n",
    "imputed_patients = []\n",
    "total_patients = df_imputed['PATIENT_ID'].nunique()\n",
    "\n",
    "for i, (patient_id, patient_data) in enumerate(df_imputed.groupby('PATIENT_ID')):\n",
    "    if i % 1000 == 0:  # Progress indicator\n",
    "        print(f\"   Processando paciente {i+1}/{total_patients} ({(i+1)/total_patients*100:.1f}%)\")\n",
    "    \n",
    "    imputed_patient = impute_patient_data(patient_data)\n",
    "    imputed_patients.append(imputed_patient)\n",
    "\n",
    "# Combinar todos os pacientes imputados\n",
    "df_imputed = pd.concat(imputed_patients, ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Imputa√ß√£o por paciente conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19348a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar missing values antes e depois da imputa√ß√£o por paciente\n",
    "print(\"üìä COMPARA√á√ÉO: ANTES vs DEPOIS DA IMPUTA√á√ÉO POR PACIENTE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "missing_before = df[numeric_cols].isnull().sum()\n",
    "missing_after = df_imputed[numeric_cols].isnull().sum()\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Coluna': numeric_cols,\n",
    "    'Missing_Antes': missing_before.values,\n",
    "    'Missing_Depois': missing_after.values,\n",
    "    'Redu√ß√£o': missing_before.values - missing_after.values,\n",
    "    'Redu√ß√£o_%': ((missing_before.values - missing_after.values) / missing_before.values * 100).round(1)\n",
    "})\n",
    "\n",
    "# Mostrar apenas colunas que tinham missing values\n",
    "comparison_filtered = comparison[comparison['Missing_Antes'] > 0].sort_values('Redu√ß√£o_%', ascending=False)\n",
    "\n",
    "if len(comparison_filtered) > 0:\n",
    "    print(comparison_filtered.to_string(index=False))\n",
    "    \n",
    "    total_missing_before = missing_before.sum()\n",
    "    total_missing_after = missing_after.sum()\n",
    "    total_reduction = ((total_missing_before - total_missing_after) / total_missing_before * 100)\n",
    "    \n",
    "    print(f\"\\nüìà RESUMO GERAL:\")\n",
    "    print(f\"   Total missing antes: {total_missing_before:,}\")\n",
    "    print(f\"   Total missing depois: {total_missing_after:,}\")\n",
    "    print(f\"   Redu√ß√£o total: {total_reduction:.1f}%\")\n",
    "else:\n",
    "    print(\"‚úÖ Nenhum valor faltante encontrado nas colunas num√©ricas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e18d7",
   "metadata": {},
   "source": [
    "## 6. Imputa√ß√£o Global para Valores Remanescentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776958c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para valores que ainda est√£o faltando, usar imputa√ß√£o global\n",
    "print(\"üåê IMPUTA√á√ÉO GLOBAL PARA VALORES REMANESCENTES\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "# Verificar se ainda h√° valores faltantes\n",
    "remaining_missing = df_imputed[numeric_cols].isnull().sum()\n",
    "cols_with_missing = remaining_missing[remaining_missing > 0]\n",
    "\n",
    "if len(cols_with_missing) > 0:\n",
    "    print(f\"üìã Colunas com valores ainda faltantes: {len(cols_with_missing)}\")\n",
    "    print(cols_with_missing.to_string())\n",
    "    \n",
    "    # Estrat√©gias de imputa√ß√£o global\n",
    "    print(\"\\nüîÑ Aplicando estrat√©gias de imputa√ß√£o global...\")\n",
    "    \n",
    "    for col in cols_with_missing.index:\n",
    "        missing_count = cols_with_missing[col]\n",
    "        \n",
    "        # Estrat√©gia 1: Mediana por g√™nero (se dispon√≠vel)\n",
    "        if 'Gender' in df_imputed.columns:\n",
    "            gender_medians = df_imputed.groupby('Gender')[col].median()\n",
    "            \n",
    "            for gender in gender_medians.index:\n",
    "                mask = (df_imputed['Gender'] == gender) & (df_imputed[col].isnull())\n",
    "                df_imputed.loc[mask, col] = gender_medians[gender]\n",
    "        \n",
    "        # Estrat√©gia 2: Mediana global para valores ainda faltantes\n",
    "        global_median = df_imputed[col].median()\n",
    "        df_imputed[col] = df_imputed[col].fillna(global_median)\n",
    "        \n",
    "        print(f\"   ‚úÖ {col}: {missing_count} valores imputados\")\n",
    "    \n",
    "    # Verifica√ß√£o final\n",
    "    final_missing = df_imputed[numeric_cols].isnull().sum().sum()\n",
    "    print(f\"\\nüìä Valores faltantes ap√≥s imputa√ß√£o global: {final_missing}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚úÖ Nenhum valor faltante remanescente - imputa√ß√£o por paciente foi suficiente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f2fad",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb24402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Criar features derivadas\n",
    "print(\"‚öôÔ∏è  FEATURE ENGINEERING\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "df_engineered = df_imputed.copy()\n",
    "\n",
    "# 1. Estat√≠sticas por paciente\n",
    "print(\"üìä Criando estat√≠sticas por paciente...\")\n",
    "\n",
    "patient_stats = df_engineered.groupby('PATIENT_ID')[numeric_cols].agg({\n",
    "    col: ['mean', 'std', 'min', 'max', 'count'] for col in numeric_cols\n",
    "})\n",
    "\n",
    "# Flatten column names\n",
    "patient_stats.columns = ['_'.join(col).strip() for col in patient_stats.columns]\n",
    "\n",
    "# Merge back to original dataframe\n",
    "df_engineered = df_engineered.merge(patient_stats, on='PATIENT_ID', how='left')\n",
    "\n",
    "print(f\"   ‚úÖ {len(patient_stats.columns)} estat√≠sticas por paciente criadas\")\n",
    "\n",
    "# 2. Features de tend√™ncia (se h√° m√∫ltiplas medi√ß√µes por paciente)\n",
    "print(\"\\nüìà Criando features de tend√™ncia...\")\n",
    "\n",
    "def calculate_trends(patient_data):\n",
    "    \"\"\"Calcula tend√™ncias para um paciente\"\"\"\n",
    "    trends = {}\n",
    "    \n",
    "    for col in numeric_cols[:5]:  # Limitar para evitar muitas features\n",
    "        if col in patient_data.columns and len(patient_data) > 1:\n",
    "            values = patient_data[col].dropna()\n",
    "            if len(values) > 1:\n",
    "                # Tend√™ncia linear simples (diferen√ßa entre √∫ltimo e primeiro valor)\n",
    "                trend = values.iloc[-1] - values.iloc[0]\n",
    "                trends[f'{col}_trend'] = trend\n",
    "            else:\n",
    "                trends[f'{col}_trend'] = 0\n",
    "        else:\n",
    "            trends[f'{col}_trend'] = 0\n",
    "    \n",
    "    return pd.Series(trends)\n",
    "\n",
    "# Calcular tend√™ncias por paciente\n",
    "patient_trends = df_engineered.groupby('PATIENT_ID').apply(calculate_trends)\n",
    "patient_trends = patient_trends.reset_index()\n",
    "\n",
    "# Merge trends back\n",
    "df_engineered = df_engineered.merge(patient_trends, on='PATIENT_ID', how='left')\n",
    "\n",
    "print(f\"   ‚úÖ {len([col for col in patient_trends.columns if '_trend' in col])} features de tend√™ncia criadas\")\n",
    "\n",
    "# 3. Features de risco (baseadas em conhecimento m√©dico)\n",
    "print(\"\\nüè• Criando features de risco m√©dico...\")\n",
    "\n",
    "# Idade como fator de risco\n",
    "if 'Age' in df_engineered.columns:\n",
    "    df_engineered['Age_Risk'] = (df_engineered['Age'] > 65).astype(int)\n",
    "    df_engineered['Age_Group'] = pd.cut(df_engineered['Age'], \n",
    "                                       bins=[0, 18, 35, 50, 65, 100], \n",
    "                                       labels=['Child', 'Young_Adult', 'Adult', 'Middle_Age', 'Senior'])\n",
    "    \n",
    "# Combinar sinais vitais (se dispon√≠veis)\n",
    "vital_signs = ['HR', 'Temp', 'SBP', 'MAP', 'Resp']  # Comuns em dados de sepsis\n",
    "available_vitals = [col for col in vital_signs if col in df_engineered.columns]\n",
    "\n",
    "if len(available_vitals) >= 2:\n",
    "    # Score de instabilidade (soma dos z-scores dos sinais vitais)\n",
    "    scaler = StandardScaler()\n",
    "    vitals_scaled = scaler.fit_transform(df_engineered[available_vitals])\n",
    "    df_engineered['Vitals_Instability_Score'] = np.abs(vitals_scaled).sum(axis=1)\n",
    "    \n",
    "    print(f\"   ‚úÖ Score de instabilidade criado usando {len(available_vitals)} sinais vitais\")\n",
    "\n",
    "print(f\"\\nüìä Shape final ap√≥s Feature Engineering: {df_engineered.shape}\")\n",
    "print(f\"üìà Novas features criadas: {df_engineered.shape[1] - df_imputed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c88fc90",
   "metadata": {},
   "source": [
    "## 8. Detec√ß√£o e Tratamento de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detec√ß√£o de outliers usando IQR\n",
    "print(\"üîç DETEC√á√ÉO E TRATAMENTO DE OUTLIERS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "df_no_outliers = df_engineered.copy()\n",
    "\n",
    "# Selecionar colunas num√©ricas originais para an√°lise de outliers\n",
    "original_numeric_cols = [col for col in numeric_cols if col in df_no_outliers.columns]\n",
    "\n",
    "def detect_outliers_iqr(data, column, multiplier=1.5):\n",
    "    \"\"\"Detecta outliers usando m√©todo IQR\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    outliers = (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "    return outliers\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for col in original_numeric_cols[:10]:  # Analisar primeiras 10 colunas\n",
    "    outliers = detect_outliers_iqr(df_no_outliers, col)\n",
    "    outlier_count = outliers.sum()\n",
    "    outlier_percent = (outlier_count / len(df_no_outliers)) * 100\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Coluna': col,\n",
    "        'Outliers': outlier_count,\n",
    "        'Porcentagem': outlier_percent\n",
    "    })\n",
    "    \n",
    "    # Tratar outliers usando capping (winsorizing)\n",
    "    if outlier_percent > 0.5 and outlier_percent < 10:  # S√≥ tratar se entre 0.5% e 10%\n",
    "        Q1 = df_no_outliers[col].quantile(0.25)\n",
    "        Q3 = df_no_outliers[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Capping\n",
    "        df_no_outliers[col] = df_no_outliers[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        \n",
    "        print(f\"   üîß {col}: {outlier_count} outliers ({outlier_percent:.1f}%) tratados por capping\")\n",
    "\n",
    "# Resumo de outliers\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "outlier_df = outlier_df[outlier_df['Outliers'] > 0].sort_values('Porcentagem', ascending=False)\n",
    "\n",
    "if len(outlier_df) > 0:\n",
    "    print(\"\\nüìä Resumo de Outliers Detectados:\")\n",
    "    print(outlier_df.to_string(index=False, float_format='%.2f'))\n",
    "else:\n",
    "    print(\"‚úÖ Nenhum outlier significativo detectado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2c979",
   "metadata": {},
   "source": [
    "## 9. Normaliza√ß√£o e Escalonamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfce092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza√ß√£o dos dados\n",
    "print(\"üìè NORMALIZA√á√ÉO E ESCALONAMENTO\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "df_scaled = df_no_outliers.copy()\n",
    "\n",
    "# Separar features num√©ricas para escalonamento\n",
    "# Excluir colunas categ√≥ricas e ID\n",
    "columns_to_exclude = ['PATIENT_ID', 'SepsisLabel', 'Gender', 'Age_Group', 'Age_Rounded']\n",
    "numeric_features = df_scaled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features = [col for col in numeric_features if col not in columns_to_exclude]\n",
    "\n",
    "print(f\"üìä Colunas num√©ricas para escalonamento: {len(numeric_features)}\")\n",
    "\n",
    "# Usar RobustScaler (menos sens√≠vel a outliers)\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Aplicar escalonamento\n",
    "df_scaled[numeric_features] = scaler.fit_transform(df_scaled[numeric_features])\n",
    "\n",
    "print(f\"‚úÖ Escalonamento aplicado usando RobustScaler\")\n",
    "print(f\"üìà Features escalonadas: {len(numeric_features)}\")\n",
    "\n",
    "# Verificar distribui√ß√£o ap√≥s escalonamento\n",
    "print(\"\\nüìä Estat√≠sticas ap√≥s escalonamento (primeiras 5 colunas):\")\n",
    "print(df_scaled[numeric_features[:5]].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73585c99",
   "metadata": {},
   "source": [
    "## 10. Divis√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6298b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divis√£o estratificada dos dados\n",
    "print(\"üîÄ DIVIS√ÉO DOS DADOS\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Preparar features e target\n",
    "if 'SepsisLabel' in df_scaled.columns:\n",
    "    # Remover colunas n√£o-features\n",
    "    feature_columns = [col for col in df_scaled.columns \n",
    "                      if col not in ['SepsisLabel', 'PATIENT_ID', 'Age_Rounded']]\n",
    "    \n",
    "    X = df_scaled[feature_columns]\n",
    "    y = df_scaled['SepsisLabel']\n",
    "    \n",
    "    print(f\"üìä Shape das features (X): {X.shape}\")\n",
    "    print(f\"üéØ Shape do target (y): {y.shape}\")\n",
    "    \n",
    "    # Divis√£o estratificada\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        stratify=y, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìà Divis√£o realizada:\")\n",
    "    print(f\"   Treino: {X_train.shape[0]:,} amostras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Teste:  {X_test.shape[0]:,} amostras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # Verificar balanceamento\n",
    "    train_dist = y_train.value_counts(normalize=True).sort_index()\n",
    "    test_dist = y_test.value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    print(f\"\\n‚öñÔ∏è  Distribui√ß√£o do target:\")\n",
    "    print(f\"   Treino - Sem Sepsis: {train_dist[0]:.1%}, Com Sepsis: {train_dist[1]:.1%}\")\n",
    "    print(f\"   Teste  - Sem Sepsis: {test_dist[0]:.1%}, Com Sepsis: {test_dist[1]:.1%}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Coluna SepsisLabel n√£o encontrada para divis√£o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5737c8a",
   "metadata": {},
   "source": [
    "## 11. Salvamento dos Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbfd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar datasets processados\n",
    "print(\"üíæ SALVAMENTO DOS DADOS PROCESSADOS\")\n",
    "print(\"=\" * 38)\n",
    "\n",
    "# Salvar dataset completo processado\n",
    "output_file = 'dataset_sepsis_prepared_v2.csv'\n",
    "df_scaled.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Dataset completo salvo: {output_file}\")\n",
    "print(f\"   Shape: {df_scaled.shape}\")\n",
    "\n",
    "if 'SepsisLabel' in df_scaled.columns:\n",
    "    # Salvar conjuntos de treino e teste\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test], axis=1)\n",
    "    \n",
    "    train_file = 'dataset_sepsis_train_v2.csv'\n",
    "    test_file = 'dataset_sepsis_test_v2.csv'\n",
    "    \n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset de treino salvo: {train_file}\")\n",
    "    print(f\"   Shape: {train_data.shape}\")\n",
    "    print(f\"‚úÖ Dataset de teste salvo: {test_file}\")\n",
    "    print(f\"   Shape: {test_data.shape}\")\n",
    "    \n",
    "    # Salvar informa√ß√µes sobre as features\n",
    "    feature_info = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Type': [str(X[col].dtype) for col in feature_columns],\n",
    "        'Missing_Original': [df[col].isnull().sum() if col in df.columns else 0 for col in feature_columns],\n",
    "        'Missing_Final': [df_scaled[col].isnull().sum() for col in feature_columns]\n",
    "    })\n",
    "    \n",
    "    feature_info.to_csv('features_info_v2.csv', index=False)\n",
    "    print(f\"‚úÖ Informa√ß√µes das features salvas: features_info_v2.csv\")\n",
    "\n",
    "print(f\"\\nüéâ PROCESSAMENTO CONCLU√çDO COM SUCESSO!\")\n",
    "print(f\"üìä Total de features: {len(feature_columns) if 'feature_columns' in locals() else 'N/A'}\")\n",
    "print(f\"üè• Total de registros processados: {len(df_scaled):,}\")\n",
    "print(f\"üë• Total de pacientes √∫nicos: {df_scaled['PATIENT_ID'].nunique():,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
