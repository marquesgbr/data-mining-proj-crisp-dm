{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c82248",
   "metadata": {},
   "source": [
    "## 1. Importar Bibliotecas e Carregar Dataset Preprocessado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o das bibliotecas necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, roc_auc_score, roc_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import uniform, randint\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de plotagem\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Scikit-learn: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento e prepara√ß√£o inicial dos dados\n",
    "print(\"Carregando datasets...\")\n",
    "\n",
    "# Carregar datasets pr√©-processados\n",
    "train_data = pd.read_csv('dataset_sepsis_prepared.csv')\n",
    "test_data = pd.read_csv('dataset_sepsis_test_prepared.csv')\n",
    "\n",
    "print(f\"Dataset de treino: {train_data.shape}\")\n",
    "print(f\"Dataset de teste: {test_data.shape}\")\n",
    "\n",
    "# Separar features e target\n",
    "X_train = train_data.drop('SepsisLabel', axis=1)\n",
    "y_train = train_data['SepsisLabel']\n",
    "X_test = test_data.drop('SepsisLabel', axis=1)\n",
    "y_test = test_data['SepsisLabel']\n",
    "\n",
    "# Normaliza√ß√£o dos dados\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nDistribui√ß√£o das classes:\")\n",
    "print(\"Treino:\", y_train.value_counts().to_dict())\n",
    "print(\"Teste:\", y_test.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6614d3dd",
   "metadata": {},
   "source": [
    "## 2. Sampling para Busca de Hiperpar√¢metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# SAMPLING ESTRATIFICADO PARA BUSCA DE HIPERPAR√ÇMETROS\n",
    "# ======================================================================\n",
    "\n",
    "print(\"=== PREPARA√á√ÉO DE AMOSTRA PARA BUSCA DE HIPERPAR√ÇMETROS ===\")\n",
    "\n",
    "# Dataset completo √© muito grande (>1M inst√¢ncias) para busca de hiperpar√¢metros\n",
    "# Vamos usar uma amostra estratificada de 4% para otimiza√ß√£o\n",
    "\n",
    "# Amostra estratificada do dataset de treino\n",
    "_, X_sample, _, y_sample = train_test_split(\n",
    "    X_train_scaled, y_train, \n",
    "    test_size=0.01,  \n",
    "    stratify=y_train,\n",
    "    random_state=10\n",
    ")\n",
    "\n",
    "print(f\"Dataset original de treino: {X_train_scaled.shape[0]:,} amostras\")\n",
    "print(f\"Amostra para busca de hiperpar√¢metros: {X_sample.shape[0]:,} amostras\")\n",
    "print(f\"Redu√ß√£o: {(1 - X_sample.shape[0]/X_train_scaled.shape[0])*100:.1f}%\")\n",
    "\n",
    "print(\"\\nDistribui√ß√£o das classes na amostra:\")\n",
    "print(\"Amostra:\", pd.Series(y_sample).value_counts().to_dict())\n",
    "print(\"Original:\", y_train.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1382fb2",
   "metadata": {},
   "source": [
    "## 3. Fun√ß√µes Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc02e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o auxiliar para c√°lculo do G-Mean\n",
    "def gmean_score(y_true, y_pred):\n",
    "    \"\"\"Calcula o G-Mean (Geometric Mean) para problemas bin√°rios\"\"\"\n",
    "    # Sensitivity (recall da classe positiva - sepsis)\n",
    "    sensitivity = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "    # Specificity (recall da classe negativa - sem sepsis)\n",
    "    specificity = recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "    # G-Mean √© a m√©dia geom√©trica de sensitivity e specificity\n",
    "    return np.sqrt(sensitivity * specificity)\n",
    "\n",
    "# Fun√ß√£o para avaliar modelos\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Avalia um modelo treinado e retorna m√©tricas completas\"\"\"\n",
    "    # Predi√ß√µes\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # M√©tricas de treino\n",
    "    train_metrics = {\n",
    "        'accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'precision': precision_score(y_train, y_train_pred, zero_division=0),\n",
    "        'recall': recall_score(y_train, y_train_pred, zero_division=0),\n",
    "        'f1': f1_score(y_train, y_train_pred, zero_division=0),\n",
    "        'gmean': gmean_score(y_train, y_train_pred)\n",
    "    }\n",
    "    \n",
    "    # M√©tricas de teste\n",
    "    test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'precision': precision_score(y_test, y_test_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_test_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_test_pred, zero_division=0),\n",
    "        'gmean': gmean_score(y_test, y_test_pred)\n",
    "    }\n",
    "    \n",
    "    # AUC-ROC para problemas bin√°rios\n",
    "    try:\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]  # Probabilidade da classe positiva\n",
    "        test_metrics['auc_roc'] = roc_auc_score(y_test, y_test_proba)\n",
    "        \n",
    "        # AUC-ROC para treino tamb√©m\n",
    "        y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "        train_metrics['auc_roc'] = roc_auc_score(y_train, y_train_proba)\n",
    "    except Exception as e:\n",
    "        test_metrics['auc_roc'] = None\n",
    "        train_metrics['auc_roc'] = None\n",
    "    \n",
    "    return train_metrics, test_metrics, y_test_pred\n",
    "\n",
    "# Fun√ß√£o para plotar hist√≥rico de busca\n",
    "def plot_search_history(all_search_results, search_results, model_name, metric='mean_test_score'):\n",
    "    \"\"\"Plota a evolu√ß√£o dos resultados durante a busca de hiperpar√¢metros\"\"\"\n",
    "        \n",
    "    plt.figure(figsize=(15, 6))\n",
    "        \n",
    "    results_df = pd.DataFrame(search_results.cv_results_)\n",
    "    # Extrair melhor score de cada busca\n",
    "    search_scores = []\n",
    "    search_indices = []\n",
    "    \n",
    "    for i, search_result in enumerate(all_search_results):\n",
    "        search_scores.append(search_result['best_score'])\n",
    "        search_indices.append(i + 1)\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(search_indices, search_scores, 'b-o', alpha=0.8, markersize=8)\n",
    "    plt.title(f'{model_name} - Melhor F1-Score por Busca')\n",
    "    plt.xlabel('N√∫mero da Busca')\n",
    "    plt.ylabel('Melhor F1-Score')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Destacar a melhor busca\n",
    "    best_idx = search_scores.index(max(search_scores))\n",
    "    plt.plot(search_indices[best_idx], search_scores[best_idx], 'ro', markersize=12, \n",
    "            label=f'Melhor: {search_scores[best_idx]:.4f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Verificar se existe coluna de treino\n",
    "    if 'mean_train_score' in results_df.columns:\n",
    "        # Plotar treino com sombra\n",
    "        plt.plot(results_df['mean_train_score'], 'g-o', alpha=0.7, label='Treino')\n",
    "        plt.fill_between(range(len(results_df)), \n",
    "                        results_df['mean_train_score'] - results_df['std_train_score'],\n",
    "                        results_df['mean_train_score'] + results_df['std_train_score'], \n",
    "                        color='green', alpha=0.2)\n",
    "    \n",
    "    # Plotar valida√ß√£o com sombra\n",
    "    plt.plot(results_df[metric], 'b-o', alpha=0.7, label='Valida√ß√£o')\n",
    "    plt.fill_between(range(len(results_df)), \n",
    "                    results_df[metric] - results_df['std_test_score'],\n",
    "                    results_df[metric] + results_df['std_test_score'], \n",
    "                    color='blue', alpha=0.3)\n",
    "    \n",
    "    # DESTACAR A MELHOR ITERA√á√ÉO\n",
    "    best_iteration_idx = results_df[metric].idxmax()\n",
    "    best_iteration_score = results_df[metric].iloc[best_iteration_idx]\n",
    "    \n",
    "    plt.plot(best_iteration_idx, best_iteration_score, 'ro', markersize=15, \n",
    "            markeredgecolor='darkred', markeredgewidth=2,\n",
    "            label=f'Melhor itera√ß√£o: #{best_iteration_idx + 1} ({best_iteration_score:.4f})')\n",
    "    \n",
    "    plt.title(f'{model_name} - Treino vs Valida√ß√£o (Melhor Busca)')\n",
    "    plt.xlabel('Itera√ß√£o')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Fun√ß√£o para executar m√∫ltiplas buscas de hiperpar√¢metros\n",
    "def multiple_randomized_search(estimator, param_distributions, X, y, cv_strategy, \n",
    "                              n_searches=20, n_iter_per_search=80, scoring='f1', \n",
    "                              random_state=42, n_jobs=-1, verbose=0):\n",
    "    \"\"\"\n",
    "    Executa m√∫ltiplas buscas RandomizedSearchCV e retorna a melhor configura√ß√£o global\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_searches : int\n",
    "        N√∫mero de execu√ß√µes do RandomizedSearchCV (default: 20)\n",
    "    n_iter_per_search : int  \n",
    "        N√∫mero de itera√ß√µes por execu√ß√£o (default: 80)\n",
    "    \"\"\"\n",
    "    print(f\"Executando {n_searches} buscas com {n_iter_per_search} itera√ß√µes cada...\")\n",
    "    \n",
    "    best_overall_score = -np.inf\n",
    "    best_overall_params = None\n",
    "    best_search_result = None\n",
    "    all_results = []\n",
    "    \n",
    "    for search_idx in range(n_searches):\n",
    "        print(f\"\\nBusca {search_idx + 1}/{n_searches}...\")\n",
    "        \n",
    "        # RandomizedSearchCV para esta execu√ß√£o\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=n_iter_per_search,\n",
    "            scoring=scoring,\n",
    "            cv=cv_strategy,\n",
    "            random_state=None,\n",
    "            n_jobs=n_jobs,\n",
    "            return_train_score=True,\n",
    "            verbose=0  # Menos verbose para m√∫ltiplas execu√ß√µes\n",
    "        )\n",
    "        \n",
    "        search.fit(X, y)\n",
    "        \n",
    "        # Armazenar resultados desta busca\n",
    "        search_results = {\n",
    "            'search_idx': search_idx,\n",
    "            'best_score': search.best_score_,\n",
    "            'best_params': search.best_params_,\n",
    "            'cv_results': search.cv_results_\n",
    "        }\n",
    "        all_results.append(search_results)\n",
    "        \n",
    "        # Verificar se esta √© a melhor busca at√© agora\n",
    "        if search.best_score_ > best_overall_score:\n",
    "            best_overall_score = search.best_score_\n",
    "            best_overall_params = search.best_params_\n",
    "            best_search_result = search\n",
    "            \n",
    "        print(f\"Melhor score desta busca: {search.best_score_:.4f}\")\n",
    "        print(f\"Melhor configura√ß√£o desta busca: {search.best_params_}\")\n",
    "        print(f\"Melhor score geral at√© agora: {best_overall_score:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Busca completa! Melhor score geral: {best_overall_score:.4f}\")\n",
    "    print(f\"Total de configura√ß√µes testadas: {n_searches * n_iter_per_search:,}\")\n",
    "    \n",
    "    return best_search_result, all_results, best_overall_params\n",
    "\n",
    "print(\"Fun√ß√µes auxiliares definidas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b35a9",
   "metadata": {},
   "source": [
    "## 3.1 Definir Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325479c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o da valida√ß√£o cruzada estratificada\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782aacf",
   "metadata": {},
   "source": [
    "## 4. KNN - Busca de Hiperpar√¢metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7769e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir nome do modelo para uso em salvamento e exibi√ß√£o. Necess√°rio Rodar essa c√©lula antes de outras.\n",
    "MODEL_NAME = \"KNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6338def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# 4.1 BUSCA DE HIPERPARAMETROS\n",
    "# ======================================================================\n",
    "\n",
    "print(f\"=== BUSCA DE HIPERPAR√ÇMETROS - {MODEL_NAME} ===\")\n",
    "\n",
    "\n",
    "# 4.3 Defini√ß√£o do Espa√ßo de Hiperpar√¢metros\n",
    "param_distributions = {\n",
    "    'n_neighbors': randint(1, 100),           # N√∫mero de vizinhos\n",
    "    'metric': ['euclidean', 'manhattan'],    # M√©trica de dist√¢ncia\n",
    "    'weights': ['uniform', 'distance'],      # Peso dos vizinhos\n",
    "}\n",
    "\n",
    "# 4.1 M√∫ltiplas execu√ß√µes do RandomizedSearchCV\n",
    "print(f\"Iniciando busca de hiperpar√¢metros para {MODEL_NAME}...\")\n",
    "model_search, model_all_searches, best_params = multiple_randomized_search(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    param_distributions=param_distributions,\n",
    "    X=X_sample,                   \n",
    "    y=y_sample,\n",
    "    cv_strategy=cv_strategy,\n",
    "    n_searches=20,                \n",
    "    n_iter_per_search=40,        \n",
    "    scoring='f1',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 5.1 Sele√ß√£o da Melhor Configura√ß√£o\n",
    "print(f\"\\n--- RESULTADOS {MODEL_NAME} ---\")\n",
    "print(\"Melhores hiperpar√¢metros:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nMelhor F1-Score (CV): {model_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e6a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Registro de Desempenho - plotar evolu√ß√£o\n",
    "plot_search_history(model_search, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180f485",
   "metadata": {},
   "source": [
    "## 5. Salvar Resultados de Busca\n",
    "Se√ß√£o serve somente para salvar o resultado de toda a busca num CSV e os dados essenciais no JSON. \n",
    "\n",
    "Usar se quiser manter os resultados guardados num diret√≥rio por garantia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.1 Salvar Resultados da Busca de Hiperpar√¢metros\n",
    "\n",
    "print(f\"=== SALVANDO RESULTADOS DA BUSCA - {MODEL_NAME} ===\")\n",
    "\n",
    "# Criar pasta se n√£o existir\n",
    "os.makedirs('searches', exist_ok=True)\n",
    "\n",
    "# 1. Salvar resultados detalhados de todas as buscas\n",
    "search_detailed_results = []\n",
    "\n",
    "for i, search_result in enumerate(model_all_searches):\n",
    "    # Extrair informa√ß√µes de cada busca individual\n",
    "    cv_results = search_result['cv_results']\n",
    "    \n",
    "    for j in range(len(cv_results['mean_test_score'])):\n",
    "        search_detailed_results.append({\n",
    "            'search_idx': search_result['search_idx'],\n",
    "            'iteration': j,\n",
    "            'mean_test_score': cv_results['mean_test_score'][j],\n",
    "            'std_test_score': cv_results['std_test_score'][j],\n",
    "            'mean_train_score': cv_results['mean_train_score'][j] if 'mean_train_score' in cv_results else None,\n",
    "            'std_train_score': cv_results['std_train_score'][j] if 'std_train_score' in cv_results else None,\n",
    "            'params': str(cv_results['params'][j]),\n",
    "            **cv_results['params'][j]  # Expandir par√¢metros como colunas separadas\n",
    "        })\n",
    "\n",
    "# Converter para DataFrame e salvar\n",
    "search_df = pd.DataFrame(search_detailed_results)\n",
    "search_df.to_csv(f'searches/{MODEL_NAME.lower()}_all_searches.csv', index=False)\n",
    "\n",
    "print(f\"  Todos os Resultados salvos: searches/{MODEL_NAME.lower()}_all_searches.csv\")\n",
    "print(f\"  Total de configura√ß√µes testadas: {len(search_df):,}\")\n",
    "\n",
    "# 2. Salvar resumo da melhor busca\n",
    "best_search_summary = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'best_overall_score': model_search.best_score_,\n",
    "    'best_overall_params': model_search.best_params_,\n",
    "    'search_config': {\n",
    "        'n_searches': 20,\n",
    "        'n_iter_per_search': 40,\n",
    "        'scoring': 'f1',\n",
    "        'cv_folds': 5,\n",
    "        'total_configurations': len(search_df)\n",
    "    },\n",
    "    'param_space': param_distributions,\n",
    "    'top_10_configs': search_df.nlargest(10, 'mean_test_score')[\n",
    "        ['mean_test_score', 'std_test_score'] + list(best_params.keys())\n",
    "    ].to_dict('records')\n",
    "}\n",
    "\n",
    "# Salvar resumo em JSON\n",
    "with open(f'searches/{MODEL_NAME.lower()}_search_summary.json', 'w') as f:\n",
    "    json.dump(best_search_summary, f, indent=2)\n",
    "\n",
    "print(f\"  Dados necess√°rios para as se√ß√µes seguintes: searches/{MODEL_NAME.lower()}_search_summary.json\")\n",
    "\n",
    "# Mostrar estat√≠sticas da busca\n",
    "print(f\"\\n--- ESTAT√çSTICAS DA BUSCA {MODEL_NAME} ---\")\n",
    "print(f\"Melhor F1-Score: {model_search.best_score_:.4f}\")\n",
    "print(f\"Desvio padr√£o do melhor: {search_df.loc[search_df['mean_test_score'].idxmax(), 'std_test_score']:.4f}\")\n",
    "print(f\"F1-Score m√©dio geral: {search_df['mean_test_score'].mean():.4f}\")\n",
    "print(f\"F1-Score m√≠nimo: {search_df['mean_test_score'].min():.4f}\")\n",
    "print(f\"F1-Score m√°ximo: {search_df['mean_test_score'].max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7244ac",
   "metadata": {},
   "source": [
    "## 6. Carregar Resultados de Busca\n",
    "N√£o precisa rodar caso a busca tenha acabado de ser feita. Rode somente se recuperar resultados salvos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97595706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_search_results(model_name, searches_folder='searches'):\n",
    "    \"\"\"\n",
    "    Carrega resultados de busca salvos anteriormente\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dicion√°rio com todos os resultados carregados\n",
    "    \"\"\"\n",
    "    print(f\"=== CARREGANDO RESULTADOS DE BUSCA - {model_name.upper()} ===\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Carregar DataFrame detalhado\n",
    "    csv_path = os.path.join(searches_folder, f'{model_name.lower()}_all_searches.csv')\n",
    "    if os.path.exists(csv_path):\n",
    "        results['detailed_df'] = pd.read_csv(csv_path)\n",
    "        print(f\"‚úÖ Resultados detalhados carregados: {len(results['detailed_df']):,} configura√ß√µes\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Arquivo n√£o encontrado: {csv_path}\")\n",
    "    \n",
    "    # 2. Carregar resumo JSON\n",
    "    json_path = os.path.join(searches_folder, f'{model_name.lower()}_search_summary.json')\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            results['summary'] = json.load(f)\n",
    "        print(f\"‚úÖ Resumo carregado: F1-Score = {results['summary']['best_overall_score']:.4f}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Arquivo n√£o encontrado: {json_path}\")\n",
    "    \n",
    "    # 3. Carregar backup pickle\n",
    "    pkl_path = os.path.join(searches_folder, f'{model_name.lower()}_full_search.pkl')\n",
    "    if os.path.exists(pkl_path):\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            results['full_backup'] = pickle.load(f)\n",
    "        print(f\"‚úÖ Backup completo carregado\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Arquivo n√£o encontrado: {pkl_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def get_best_params_from_saved(model_name, searches_folder='searches'):\n",
    "    \"\"\"\n",
    "    Recupera os melhores par√¢metros de arquivos salvos\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Melhores par√¢metros encontrados\n",
    "    \"\"\"\n",
    "    # Tentar carregar do JSON primeiro\n",
    "    json_path = os.path.join(searches_folder, f'{model_name.lower()}_search_summary.json')\n",
    "    \n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            summary = json.load(f)\n",
    "        return summary['best_overall_params']\n",
    "    \n",
    "    # Fallback para pickle\n",
    "    pkl_path = os.path.join(searches_folder, f'{model_name.lower()}_full_search.pkl')\n",
    "    if os.path.exists(pkl_path):\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            backup = pickle.load(f)\n",
    "        return backup['best_params']\n",
    "    \n",
    "    print(f\"‚ùå N√£o foi poss√≠vel carregar par√¢metros para {model_name}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae72b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4.2 Carregar Resultados Salvos (Fun√ß√£o Auxiliar)\n",
    "# Exemplo de uso da fun√ß√£o (n√£o executar se j√° temos os resultados)\n",
    "loaded_results = load_search_results(MODEL_NAME)\n",
    "\n",
    "\n",
    "#### 4.3 Recuperar Melhores Par√¢metros para Uso Posterior\n",
    "# Exemplo de uso (descomente se precisar carregar par√¢metros salvos):\n",
    "if 'best_params' not in locals():\n",
    "    best_params = get_best_params_from_saved(MODEL_NAME)\n",
    "    if best_params:\n",
    "        print(f\"‚úÖ Par√¢metros carregados: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9aff73",
   "metadata": {},
   "source": [
    "## 7. Train Final Model and Save It as Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8865d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Treinamento Final com dataset completo usando melhores hiperpar√¢metros\n",
    "\n",
    "\n",
    "best_model = KNeighborsClassifier(**model_search.best_params_) \n",
    "\n",
    "# Armazenar resultados\n",
    "\n",
    "model_results = {\n",
    "    'model': best_model,\n",
    "    'search': model_search,\n",
    "    'best_params': model_search.best_params_,\n",
    "    'best_cv_score': model_search.best_score_\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc19159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Descomentar linhas abaixo para treinar com amostra menor estratificada do dataset completo\n",
    "# =============================================\n",
    "# _, X_sample2, _, y_sample2 = train_test_split(\n",
    "#     X_train_scaled, y_train, \n",
    "#     test_size=0.5,  \n",
    "#     stratify=y_train\n",
    "#     random_state=10\n",
    "# )\n",
    "# X_train_scaled, y_train = X_sample2, y_sample2\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train)  # Treino \n",
    "print(f\"\\nModelo final {MODEL_NAME} treinado com dataset completo: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo treinado\n",
    "\n",
    "# Criar pasta se n√£o existir\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "dump(best_model, f'models/{MODEL_NAME.lower()}_trained.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96273c33",
   "metadata": {},
   "source": [
    "## 8. Load Model, Eval and Save Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a88efd",
   "metadata": {},
   "source": [
    "### 8.1 Load libs and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c2289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo\n",
    "loaded_model = load(f'models/{MODEL_NAME.lower()}_trained.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c494f",
   "metadata": {},
   "source": [
    "### 8.2 Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e45606",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"=== AVALIA√á√ÉO E SALVAMENTO DOS RESULTADOS - {MODEL_NAME} ===\")\n",
    "\n",
    "# Criar pasta se n√£o existir\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# 2. Avalia√ß√£o completa do modelo (usando amostras para economizar tempo)\n",
    "print(\"\\nAvaliando performance do modelo...\")\n",
    "\n",
    "try:\n",
    "    model = best_model\n",
    "except NameError:\n",
    "    model = loaded_model\n",
    "\n",
    "\n",
    "X_train_eval = X_train_scaled\n",
    "y_train_eval = y_train\n",
    "X_test_eval = X_test_scaled\n",
    "y_test_eval = y_test\n",
    "\n",
    "# ===========================================\n",
    "# Usar amostras do dataset se necess√°rio (descomentar linhas abaixo)\n",
    "# ===========================================\n",
    "# _, X_test_eval, _, y_test_eval = train_test_split(\n",
    "# X_test_eval, y_test_eval,\n",
    "# test_size=0.3, \n",
    "# stratify=y_test_eval,\n",
    "# random_state=42\n",
    "# )\n",
    "\n",
    "# _, X_train_eval, _, y_train_eval = train_test_split(\n",
    "# X_train_eval, y_train_eval,\n",
    "# test_size=0.1, \n",
    "# stratify=y_train_eval,\n",
    "# random_state=42\n",
    "# )\n",
    "\n",
    "\n",
    "# Avaliar modelo\n",
    "train_metrics, test_metrics, y_pred = evaluate_model(\n",
    "    model, X_train_eval, X_test_eval, y_train_eval, y_test_eval, MODEL_NAME\n",
    ")\n",
    "\n",
    "# 3. Compilar todos os resultados\n",
    "model_final_results = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'best_params': model_search.best_params_,\n",
    "    'best_cv_score': model_search.best_score_,\n",
    "    'train_metrics': train_metrics,\n",
    "    'test_metrics': test_metrics,\n",
    "    'predictions': y_pred.tolist(), # Converter para lista no JSON\n",
    "    'test_labels': y_test_eval.tolist(),\n",
    "    'evaluation_info': {\n",
    "        'train_samples_used': len(X_train_eval),\n",
    "        'test_samples_used': len(X_test_eval),\n",
    "        'total_train_samples': len(X_train_scaled),\n",
    "        'total_test_samples': len(X_test_scaled)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df113f5f",
   "metadata": {},
   "source": [
    "### 8.3 Save Results for Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc58ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results/{MODEL_NAME.lower()}_results.json', 'w') as f:\n",
    "    json.dump(model_final_results, f, indent=2)\n",
    "\n",
    "print(f\"Resultados {MODEL_NAME} salvos em: results/{MODEL_NAME.lower()}_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48748eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================\n",
    "# Rode se quiser salvar um backup em pickle tamb√©m ou caso anterior JSON falhe\n",
    "# ================\n",
    "\n",
    "with open(f'results/{MODEL_NAME.lower()}_results.pkl', 'wb') as f:\n",
    "    pickle.dump(model_final_results, f)\n",
    "\n",
    "print(f\"Backup dos resultados salvo em: results/{MODEL_NAME.lower()}_results.pkl\")\n",
    "\n",
    "# Mostrar resumo para verifica√ß√£o r√°pida\n",
    "print(f\"\\n--- RESUMO {MODEL_NAME} ---\")\n",
    "print(f\"F1-Score CV: {model_final_results['best_cv_score']:.4f}\")\n",
    "print(f\"F1-Score Teste: {test_metrics['f1']:.4f}\")\n",
    "print(f\"Precis√£o Teste: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall Teste: {test_metrics['recall']:.4f}\")\n",
    "print(f\"G-Mean Teste: {test_metrics['gmean']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
