{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f2aec6",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o de Bibliotecas e Configura√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8d4bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Importa√ß√£o das bibliotecas necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import ast\n",
    "from joblib import load\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de plotagem\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cacdc9",
   "metadata": {},
   "source": [
    "## 2. Fun√ß√µes Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f392cf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fun√ß√µes auxiliares definidas!\n"
     ]
    }
   ],
   "source": [
    "# Fun√ß√£o auxiliar para c√°lculo do G-Mean\n",
    "def gmean_score(y_true, y_pred):\n",
    "    \"\"\"Calcula o G-Mean (Geometric Mean) para problemas bin√°rios\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    return np.sqrt(sensitivity * specificity)\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Avalia um modelo treinado em conjuntos de treino e teste\"\"\"\n",
    "    print(f\"Avaliando {model_name}...\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # M√©tricas de treino\n",
    "    train_metrics = {\n",
    "        'accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'precision': precision_score(y_train, y_train_pred),\n",
    "        'recall': recall_score(y_train, y_train_pred),\n",
    "        'f1': f1_score(y_train, y_train_pred),\n",
    "        'gmean': gmean_score(y_train, y_train_pred)\n",
    "    }\n",
    "    \n",
    "    # M√©tricas de teste\n",
    "    test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'precision': precision_score(y_test, y_test_pred),\n",
    "        'recall': recall_score(y_test, y_test_pred),\n",
    "        'f1': f1_score(y_test, y_test_pred),\n",
    "        'gmean': gmean_score(y_test, y_test_pred)\n",
    "    }\n",
    "    \n",
    "    # AUC se o modelo suporta predict_proba\n",
    "    try:\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "        test_metrics['auc_roc'] = roc_auc_score(y_test, y_test_proba)\n",
    "        train_y_proba = model.predict_proba(X_train)[:, 1]\n",
    "        train_metrics['auc_roc'] = roc_auc_score(y_train, train_y_proba)\n",
    "    except:\n",
    "        test_metrics['auc_roc'] = None\n",
    "        train_metrics['auc_roc'] = None\n",
    "    \n",
    "    return train_metrics, test_metrics, y_test_pred\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes auxiliares definidas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea25dce",
   "metadata": {},
   "source": [
    "## 3. Carregamento dos Dados de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b607261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Carregando dados de teste...\n",
      "Dataset de treino: (853006, 19)\n",
      "Dataset de teste: (215171, 19)\n",
      "Dataset de treino: (853006, 19)\n",
      "Dataset de teste: (215171, 19)\n",
      "\n",
      "üìä Distribui√ß√£o das classes:\n",
      "Treino: {0.0: 831112, 1.0: 21894}\n",
      "Teste: {0.0: 209675, 1.0: 5496}\n",
      "\n",
      "üìä Distribui√ß√£o das classes:\n",
      "Treino: {0.0: 831112, 1.0: 21894}\n",
      "Teste: {0.0: 209675, 1.0: 5496}\n"
     ]
    }
   ],
   "source": [
    "# Carregamento dos dados de teste\n",
    "print(\"üì• Carregando dados de teste...\")\n",
    "\n",
    "try:\n",
    "    # Carregar datasets pr√©-processados\n",
    "    train_data = pd.read_csv('dataset_sepsis_prepared.csv')\n",
    "    test_data = pd.read_csv('dataset_sepsis_test_prepared.csv')\n",
    "    \n",
    "    print(f\"Dataset de treino: {train_data.shape}\")\n",
    "    print(f\"Dataset de teste: {test_data.shape}\")\n",
    "    \n",
    "    # Separar features e target\n",
    "    X_train = train_data.drop('SepsisLabel', axis=1)\n",
    "    y_train = train_data['SepsisLabel']\n",
    "    X_test = test_data.drop('SepsisLabel', axis=1)\n",
    "    y_test = test_data['SepsisLabel']\n",
    "    \n",
    "    # Normaliza√ß√£o dos dados\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"\\nüìä Distribui√ß√£o das classes:\")\n",
    "    print(\"Treino:\", y_train.value_counts().to_dict())\n",
    "    print(\"Teste:\", y_test.value_counts().to_dict())\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Erro: Arquivo n√£o encontrado - {e}\")\n",
    "    print(\"Certifique-se de que os arquivos dataset_sepsis_prepared.csv e dataset_sepsis_test_prepared.csv existem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e61e50c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Amostras para avalia√ß√£o:\n",
      "Treino: 426,503 amostras\n",
      "Teste: 107,586 amostras\n"
     ]
    }
   ],
   "source": [
    "# Sampling para avalia√ß√£o mais r√°pida\n",
    "_, X_train_sample, _, y_train_sample = train_test_split(\n",
    "    X_train_scaled, y_train, \n",
    "    test_size=0.5, \n",
    "    stratify=y_train,\n",
    "    random_state=10\n",
    ")\n",
    "\n",
    "_, X_test_sample, _, y_test_sample = train_test_split(\n",
    "    X_test_scaled, y_test, \n",
    "    test_size=0.5, \n",
    "    stratify=y_test,\n",
    "    random_state=10\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Amostras para avalia√ß√£o:\")\n",
    "print(f\"Treino: {X_train_sample.shape[0]:,} amostras\")\n",
    "print(f\"Teste: {X_test_sample.shape[0]:,} amostras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8683fd",
   "metadata": {},
   "source": [
    "## Definir LVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c22c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# 4.2 LEARNING VECTOR QUANTIZATION (LVQ) - IMPLEMENTA√á√ÉO E BUSCA\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "# Implementa√ß√£o do LVQ como Estimador compat√≠vel com scikit-learn\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class LVQClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, prototypes_per_class=1, n_epochs=100, learning_rate=0.01, random_state=None):\n",
    "        \"\"\"\n",
    "        prototypes_per_class : int\n",
    "            N√∫mero de prot√≥tipos a serem usados para cada classe.\n",
    "        n_epochs : int\n",
    "            N√∫mero de √©pocas (itera√ß√µes completas sobre os dados de treinamento).\n",
    "        learning_rate : float\n",
    "            Taxa de aprendizado utilizada para atualizar os prot√≥tipos.\n",
    "        random_state : int ou None\n",
    "            Semente para reprodutibilidade.\n",
    "        \"\"\"\n",
    "        self.prototypes_per_class = prototypes_per_class\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "            \n",
    "        # Determina as classes √∫nicas\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Inicializa os prot√≥tipos\n",
    "        prototypes = []\n",
    "        prototype_labels = []\n",
    "        for c in self.classes_:\n",
    "            X_c = X[y == c]\n",
    "            # Se a quantidade de exemplos dessa classe for menor que o n√∫mero de prot√≥tipos desejados\n",
    "            replace_flag = X_c.shape[0] < self.prototypes_per_class\n",
    "            indices = np.random.choice(X_c.shape[0], size=self.prototypes_per_class, replace=replace_flag)\n",
    "            prototypes.append(X_c[indices])\n",
    "            prototype_labels.extend([c] * self.prototypes_per_class)\n",
    "            \n",
    "        self.prototypes_ = np.vstack(prototypes)\n",
    "        self.prototype_labels_ = np.array(prototype_labels)\n",
    "\n",
    "        # Treinamento ‚Äì Algoritmo LVQ\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Embaralha os √≠ndices dos exemplos\n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            for i in indices:\n",
    "                xi = X[i]\n",
    "                yi = y[i]\n",
    "                # Calcula as dist√¢ncias euclidianas do exemplo xi a todos os prot√≥tipos\n",
    "                distances = np.linalg.norm(self.prototypes_ - xi, axis=1)\n",
    "                # Encontra o √≠ndice do prot√≥tipo mais pr√≥ximo\n",
    "                j = np.argmin(distances)\n",
    "                # Atualiza o prot√≥tipo:\n",
    "                if self.prototype_labels_[j] == yi:\n",
    "                    # Se a classe coincide, aproxima o prot√≥tipo do exemplo\n",
    "                    self.prototypes_[j] += self.learning_rate * (xi - self.prototypes_[j])\n",
    "                else:\n",
    "                    # Se as classes forem diferentes, afasta o prot√≥tipo do exemplo\n",
    "                    self.prototypes_[j] -= self.learning_rate * (xi - self.prototypes_[j])\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        y_pred = []\n",
    "        # Para cada exemplo, calcula a dist√¢ncia para cada prot√≥tipo\n",
    "        for xi in X:\n",
    "            distances = np.linalg.norm(self.prototypes_ - xi, axis=1)\n",
    "            j = np.argmin(distances)\n",
    "            y_pred.append(self.prototype_labels_[j])\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return accuracy_score(y, self.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd346c30",
   "metadata": {},
   "source": [
    "## 4. Recupera√ß√£o dos Resultados dos Modelos Treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af652199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Carregando resultados dos modelos...\n",
      "‚ö†Ô∏è  Arquivo n√£o encontrado: knn_results.json\n",
      "‚ö†Ô∏è  Arquivo n√£o encontrado: lvq_results.json\n",
      "‚ö†Ô∏è  Arquivo n√£o encontrado: svm_results.json\n",
      "‚ö†Ô∏è  Arquivo n√£o encontrado: rf_results.json\n",
      "‚ö†Ô∏è  Arquivo n√£o encontrado: dt_results.json\n",
      "\n",
      "üîÑ Carregando modelos treinados...\n",
      "‚ö†Ô∏è  Modelo n√£o encontrado: knn_trained.joblib\n",
      "‚úÖ Modelo LVQ: Carregado com sucesso\n",
      "‚úÖ Modelo SVM: Carregado com sucesso\n",
      "‚ö†Ô∏è  Modelo n√£o encontrado: rf_trained.joblib\n",
      "‚úÖ Modelo Decision Tree: Carregado com sucesso\n",
      "‚úÖ Scaler: Carregado com sucesso\n",
      "\n",
      "üìä Total de resultados carregados: 0\n",
      "üìä Total de modelos carregados: 3\n",
      "‚ö†Ô∏è Resultados faltando: ['KNN', 'LVQ', 'SVM', 'Decision Tree', 'Random Forest']\n",
      "‚ö†Ô∏è Modelos faltando: ['KNN', 'Random Forest']\n",
      "\n",
      "‚ö†Ô∏è Execute os notebooks individuais correspondentes primeiro.\n",
      "{'LVQ': LVQClassifier(learning_rate=np.float64(0.2189698132600466), n_epochs=879,\n",
      "              prototypes_per_class=78), 'SVM': SVC(C=np.float64(413.1003988473893), gamma=np.float64(18.059431965634634),\n",
      "    kernel='sigmoid'), 'Decision Tree': DecisionTreeClassifier(criterion='entropy', max_depth=38, min_samples_leaf=3,\n",
      "                       min_samples_split=14), 'scaler': StandardScaler()}\n"
     ]
    }
   ],
   "source": [
    "def load_model_results(results_folder='results'):\n",
    "    \"\"\"Carrega todos os resultados de avalia√ß√£o salvos dos notebooks individuais\"\"\"\n",
    "    all_results = {}\n",
    "\n",
    "    if not os.path.exists(results_folder):\n",
    "        print(f\"‚ùå Pasta {results_folder} n√£o encontrada!\")\n",
    "        print(\"Execute os notebooks individuais dos modelos primeiro.\")\n",
    "        return all_results\n",
    "\n",
    "    # Mapear arquivos para modelos\n",
    "    result_files = {\n",
    "        'knn_results.json': 'KNN',\n",
    "        'lvq_results.json': 'LVQ', \n",
    "        'svm_results.json': 'SVM',\n",
    "        'rf_results.json': 'Random Forest',\n",
    "        'dt_results.json': 'Decision Tree'\n",
    "    }\n",
    "\n",
    "    for filename, model_name in result_files.items():\n",
    "        filepath = os.path.join(results_folder, filename)\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            try:\n",
    "                with open(filepath, 'r') as f:\n",
    "                    result_data = json.load(f)\n",
    "                all_results[model_name] = result_data\n",
    "                print(f\"‚úÖ {model_name}: Carregado com sucesso\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erro ao carregar {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Arquivo n√£o encontrado: {filename}\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def load_trained_models(models_folder='models'):\n",
    "    \"\"\"Carrega os modelos treinados salvos\"\"\"\n",
    "    trained_models = {}\n",
    "    \n",
    "    if not os.path.exists(models_folder):\n",
    "        print(f\"‚ùå Pasta {models_folder} n√£o encontrada!\")\n",
    "        return trained_models\n",
    "    \n",
    "    model_files = {\n",
    "        'knn_trained.joblib': 'KNN',\n",
    "        'lvq_trained.joblib': 'LVQ',\n",
    "        'svm_trained.joblib': 'SVM', \n",
    "        'rf_trained.joblib': 'Random Forest',\n",
    "        'dt_trained.joblib': 'Decision Tree'\n",
    "    }\n",
    "    \n",
    "    for filename, model_name in model_files.items():\n",
    "        filepath = os.path.join(models_folder, filename)\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            try:\n",
    "                model = load(filepath)\n",
    "                trained_models[model_name] = model\n",
    "                print(f\"‚úÖ Modelo {model_name}: Carregado com sucesso\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erro ao carregar modelo {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Modelo n√£o encontrado: {filename}\")\n",
    "    \n",
    "    # Carregar scaler se existir\n",
    "    scaler_path = os.path.join(models_folder, 'scaler.joblib')\n",
    "    if os.path.exists(scaler_path):\n",
    "        try:\n",
    "            trained_models['scaler'] = load(scaler_path)\n",
    "            print(\"‚úÖ Scaler: Carregado com sucesso\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao carregar scaler: {e}\")\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "print(\"üîÑ Carregando resultados dos modelos...\")\n",
    "final_results = load_model_results()\n",
    "\n",
    "print(\"\\nüîÑ Carregando modelos treinados...\")\n",
    "trained_models = load_trained_models()\n",
    "\n",
    "print(f\"\\nüìä Total de resultados carregados: {len(final_results)}\")\n",
    "print(f\"üìä Total de modelos carregados: {len([k for k in trained_models.keys() if k != 'scaler'])}\")\n",
    "\n",
    "# Verificar se todos os modelos foram carregados\n",
    "expected_models = ['KNN', 'LVQ', 'SVM', 'Decision Tree', 'Random Forest']\n",
    "missing_results = [model for model in expected_models if model not in final_results]\n",
    "missing_models = [model for model in expected_models if model not in trained_models]\n",
    "\n",
    "if missing_results:\n",
    "    print(f\"‚ö†Ô∏è Resultados faltando: {missing_results}\")\n",
    "if missing_models:\n",
    "    print(f\"‚ö†Ô∏è Modelos faltando: {missing_models}\")\n",
    "    \n",
    "if not missing_results and not missing_models:\n",
    "    print(\"\\n‚úÖ Todos os modelos e resultados foram carregados com sucesso!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Execute os notebooks individuais correspondentes primeiro.\")\n",
    "print(trained_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4e8696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq = LVQClassifier(learning_rate=np.float64(0.002749027093115263), n_epochs=131,\n",
    "              prototypes_per_class=5, random_state=42)\n",
    "\n",
    "_, X_amostra, _, y_amostra = train_test_split(\n",
    "    X_train_scaled, y_train, \n",
    "    test_size=0.1,  \n",
    "    stratify=y_train,\n",
    "    random_state=10\n",
    ")\n",
    "lvq.fit(X_amostra, y_amostra)  # Treino com dataset completo\n",
    "trained_models['LVQ'] = lvq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b09a51",
   "metadata": {},
   "source": [
    "## 5. Avalia√ß√£o Final dos Modelos no Conjunto de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54272a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AVALIA√á√ÉO FINAL DOS MODELOS NO CONJUNTO DE TESTE ===\n",
      "\n",
      "--- Avaliando LVQ ---\n",
      "Avaliando LVQ...\n",
      "Acur√°cia Treino: 0.9742\n",
      "F1-Score Treino: 0.0036\n",
      "F1-Score Teste: 0.0007\n",
      "Recall Teste: 0.0004\n",
      "Precis√£o Teste: 0.5000\n",
      "G-Mean Teste: 0.0191\n",
      "\n",
      "--- Avaliando SVM ---\n",
      "Avaliando SVM...\n",
      "Acur√°cia Treino: 0.9742\n",
      "F1-Score Treino: 0.0036\n",
      "F1-Score Teste: 0.0007\n",
      "Recall Teste: 0.0004\n",
      "Precis√£o Teste: 0.5000\n",
      "G-Mean Teste: 0.0191\n",
      "\n",
      "--- Avaliando SVM ---\n",
      "Avaliando SVM...\n",
      "Acur√°cia Treino: 0.9492\n",
      "F1-Score Treino: 0.0178\n",
      "F1-Score Teste: 0.0000\n",
      "Recall Teste: 0.0000\n",
      "Precis√£o Teste: 0.0000\n",
      "G-Mean Teste: 0.0000\n",
      "\n",
      "--- Avaliando Decision Tree ---\n",
      "Avaliando Decision Tree...\n",
      "Acur√°cia Treino: 0.9492\n",
      "F1-Score Treino: 0.0178\n",
      "F1-Score Teste: 0.0000\n",
      "Recall Teste: 0.0000\n",
      "Precis√£o Teste: 0.0000\n",
      "G-Mean Teste: 0.0000\n",
      "\n",
      "--- Avaliando Decision Tree ---\n",
      "Avaliando Decision Tree...\n",
      "Acur√°cia Treino: 0.9843\n",
      "F1-Score Treino: 0.6400\n",
      "F1-Score Teste: 0.1907\n",
      "Recall Teste: 0.1878\n",
      "Precis√£o Teste: 0.1937\n",
      "G-Mean Teste: 0.4289\n",
      "AUC-ROC Teste: 0.6227\n",
      "\n",
      "‚úÖ Avalia√ß√£o de todos os modelos conclu√≠da!\n",
      "Acur√°cia Treino: 0.9843\n",
      "F1-Score Treino: 0.6400\n",
      "F1-Score Teste: 0.1907\n",
      "Recall Teste: 0.1878\n",
      "Precis√£o Teste: 0.1937\n",
      "G-Mean Teste: 0.4289\n",
      "AUC-ROC Teste: 0.6227\n",
      "\n",
      "‚úÖ Avalia√ß√£o de todos os modelos conclu√≠da!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== AVALIA√á√ÉO FINAL DOS MODELOS NO CONJUNTO DE TESTE ===\")\n",
    "\n",
    "# Dicion√°rio para armazenar resultados da avalia√ß√£o atual\n",
    "current_evaluation = {}\n",
    "\n",
    "# Avaliar cada modelo carregado\n",
    "for model_name, model in trained_models.items():\n",
    "    if model_name == 'scaler':\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n--- Avaliando {model_name} ---\")\n",
    "    if model_name == 'Decision Tree': # alterar max_features\n",
    "        model.max_features = 'sqrt'\n",
    "    try:\n",
    "        # Avalia√ß√£o no conjunto de teste amostrado\n",
    "        train_metrics, test_metrics, y_pred = evaluate_model(\n",
    "            model, X_train_sample, X_test_sample, y_train_sample, y_test_sample, model_name\n",
    "        )\n",
    "        \n",
    "        # Armazenar resultados\n",
    "        current_evaluation[model_name] = {\n",
    "            'train_metrics': train_metrics,\n",
    "            'test_metrics': test_metrics,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        \n",
    "        # Exibir m√©tricas principais\n",
    "        print(f\"Acur√°cia Treino: {train_metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1-Score Treino: {train_metrics['f1']:.4f}\")\n",
    "        print(f\"F1-Score Teste: {test_metrics['f1']:.4f}\")\n",
    "        print(f\"Recall Teste: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"Precis√£o Teste: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"G-Mean Teste: {test_metrics['gmean']:.4f}\")\n",
    "        if test_metrics['auc_roc']:\n",
    "            print(f\"AUC-ROC Teste: {test_metrics['auc_roc']:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao avaliar {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n‚úÖ Avalia√ß√£o de todos os modelos conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14802130",
   "metadata": {},
   "source": [
    "## 6. Consolida√ß√£o de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar resultados carregados com avalia√ß√£o atual\n",
    "print(\"=== CONSOLIDANDO RESULTADOS ===\")\n",
    "\n",
    "consolidated_results = {}\n",
    "\n",
    "for model_name in expected_models:\n",
    "    consolidated_results[model_name] = {\n",
    "        'cv_score': final_results.get(model_name, {}).get('cv_score', 0.0),\n",
    "        'best_params': final_results.get(model_name, {}).get('best_params', {}),\n",
    "        'train_metrics': current_evaluation.get(model_name, {}).get('train_metrics', {}),\n",
    "        'test_metrics': current_evaluation.get(model_name, {}).get('test_metrics', {}),\n",
    "        'predictions': current_evaluation.get(model_name, {}).get('predictions', [])\n",
    "    }\n",
    "\n",
    "# Criar DataFrame com m√©tricas para compara√ß√£o\n",
    "metrics_data = []\n",
    "for model_name, results in consolidated_results.items():\n",
    "    if not results['test_metrics']:  # Pular se n√£o h√° m√©tricas de teste\n",
    "        continue\n",
    "        \n",
    "    train_metrics = results.get('train_metrics', {})\n",
    "    test_metrics = results.get('test_metrics', {})\n",
    "    \n",
    "    metrics_data.append({\n",
    "        'Modelo': model_name,\n",
    "        'F1_CV': results.get('cv_score', 0.0),\n",
    "        'F1_Treino': train_metrics.get('f1', 0.0),\n",
    "        'F1_Teste': test_metrics.get('f1', 0.0),\n",
    "        'Precis√£o_Teste': test_metrics.get('precision', 0.0),\n",
    "        'Recall_Teste': test_metrics.get('recall', 0.0),\n",
    "        'G-Mean_Teste': test_metrics.get('gmean', 0.0),\n",
    "        'AUC_ROC_Teste': test_metrics.get('auc_roc', 0.0) if test_metrics.get('auc_roc') else 0.0\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "print(\"=== RESUMO COMPARATIVO DOS MODELOS ===\")\n",
    "print(metrics_df.round(4))\n",
    "\n",
    "# Salvar resultados consolidados\n",
    "metrics_df.to_csv('main_evaluation_results.csv', index=False)\n",
    "print(\"\\nüíæ Resultados salvos em 'main_evaluation_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb87ca",
   "metadata": {},
   "source": [
    "## 7. Visualiza√ß√µes Comparativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018cfd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°ficos de compara√ß√£o\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "# Gr√°fico 1: Compara√ß√£o de F1-Score\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(metrics_df))\n",
    "ax1.bar(x_pos - 0.2, metrics_df['F1_CV'], 0.4, label='F1-Score CV', alpha=0.8, color='skyblue')\n",
    "ax1.bar(x_pos + 0.2, metrics_df['F1_Teste'], 0.4, label='F1-Score Teste', alpha=0.8, color='orange')\n",
    "ax1.set_xlabel('Modelos')\n",
    "ax1.set_ylabel('F1-Score')\n",
    "ax1.set_title('Compara√ß√£o F1-Score: CV vs Teste')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(metrics_df['Modelo'], rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico 2: Compara√ß√£o de m√∫ltiplas m√©tricas\n",
    "ax2 = axes[0, 1]\n",
    "metrics_to_plot = ['F1_Teste', 'Precis√£o_Teste', 'Recall_Teste', 'G-Mean_Teste']\n",
    "x = np.arange(len(metrics_df))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    ax2.bar(x + i*width, metrics_df[metric], width, label=metric.replace('_', ' '), alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Modelos')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Compara√ß√£o de M√∫ltiplas M√©tricas')\n",
    "ax2.set_xticks(x + width * 1.5)\n",
    "ax2.set_xticklabels(metrics_df['Modelo'], rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico 3: An√°lise de Overfitting\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(metrics_df['Modelo'], metrics_df['F1_Treino'], 'ro-', label='F1-Score Treino', alpha=0.7)\n",
    "ax3.plot(metrics_df['Modelo'], metrics_df['F1_Teste'], 'bo-', label='F1-Score Teste', alpha=0.7)\n",
    "ax3.set_xlabel('Modelos')\n",
    "ax3.set_title('An√°lise de Overfitting: F1-Score Treino vs Teste')\n",
    "ax3.set_ylabel('F1-Score')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico 4: AUC-ROC Comparison (para modelos que suportam)\n",
    "ax4 = axes[1, 1]\n",
    "auc_data = metrics_df[metrics_df['AUC_ROC_Teste'] > 0]\n",
    "if not auc_data.empty:\n",
    "    ax4.bar(auc_data['Modelo'], auc_data['AUC_ROC_Teste'], alpha=0.8, color='purple')\n",
    "    ax4.set_xlabel('Modelos')\n",
    "    ax4.set_ylabel('AUC-ROC')\n",
    "    ax4.set_title('Compara√ß√£o AUC-ROC')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Nenhum modelo\\ncom AUC-ROC dispon√≠vel', \n",
    "             ha='center', va='center', transform=ax4.transAxes, fontsize=12)\n",
    "    ax4.set_title('AUC-ROC n√£o dispon√≠vel')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('main_evaluation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Gr√°ficos de compara√ß√£o gerados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02819a6a",
   "metadata": {},
   "source": [
    "## 8. Matrizes de Confus√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar matrizes de confus√£o para todos os modelos\n",
    "n_models = len([m for m in consolidated_results.keys() if consolidated_results[m]['test_metrics']])\n",
    "if n_models > 0:\n",
    "    cols = min(3, n_models)\n",
    "    rows = (n_models + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    plot_idx = 0\n",
    "    for model_name, results in consolidated_results.items():\n",
    "        if not results['test_metrics']:\n",
    "            continue\n",
    "            \n",
    "        row = plot_idx // cols\n",
    "        col = plot_idx % cols\n",
    "        ax = axes[row, col] if rows > 1 else axes[col]\n",
    "        \n",
    "        # Calcular matriz de confus√£o\n",
    "        y_pred = results['predictions']\n",
    "        cm = confusion_matrix(y_test_sample, y_pred)\n",
    "        \n",
    "        # Plotar matriz de confus√£o\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "        ax.set_title(f'Matriz de Confus√£o - {model_name}')\n",
    "        ax.set_xlabel('Predito')\n",
    "        ax.set_ylabel('Real')\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Remover subplots extras\n",
    "    for i in range(plot_idx, rows * cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        ax = axes[row, col] if rows > 1 else axes[col]\n",
    "        ax.remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Matrizes de confus√£o geradas!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum modelo com resultados para plotar matrizes de confus√£o.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2fe21",
   "metadata": {},
   "source": [
    "## 9. Curvas ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar curvas ROC para modelos que suportam predict_proba\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "models_with_roc = []\n",
    "for model_name, model in trained_models.items():\n",
    "    if model_name == 'scaler':\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_proba = model.predict_proba(X_test_sample)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_test_sample, y_proba)\n",
    "            auc = roc_auc_score(y_test_sample, y_proba)\n",
    "            \n",
    "            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})', linewidth=2)\n",
    "            models_with_roc.append(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è N√£o foi poss√≠vel plotar ROC para {model_name}: {e}\")\n",
    "\n",
    "if models_with_roc:\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Linha Aleat√≥ria', alpha=0.5)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taxa de Falsos Positivos')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "    plt.title('Curvas ROC - Compara√ß√£o dos Modelos')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Curvas ROC geradas para {len(models_with_roc)} modelos!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum modelo suporta predict_proba para curvas ROC.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1bc0c",
   "metadata": {},
   "source": [
    "## 10. An√°lise de Overfitting e Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== AN√ÅLISE DE OVERFITTING VS UNDERFITTING ===\")\n",
    "\n",
    "overfitting_analysis = []\n",
    "\n",
    "for model_name, results in consolidated_results.items():\n",
    "    if not results['test_metrics'] or not results['train_metrics']:\n",
    "        continue\n",
    "        \n",
    "    f1_train = results['train_metrics'].get('f1', 0.0)\n",
    "    f1_test = results['test_metrics'].get('f1', 0.0)\n",
    "    difference = f1_train - f1_test\n",
    "    \n",
    "    if difference > 0.1:\n",
    "        status = \"üî¥ POSS√çVEL OVERFITTING\"\n",
    "    elif difference < -0.05:\n",
    "        status = \"üîµ POSS√çVEL UNDERFITTING\"  \n",
    "    else:\n",
    "        status = \"‚úÖ BOM EQUIL√çBRIO\"\n",
    "    \n",
    "    overfitting_analysis.append({\n",
    "        'Modelo': model_name,\n",
    "        'F1_Treino': f1_train,\n",
    "        'F1_Teste': f1_test,\n",
    "        'Diferen√ßa': difference,\n",
    "        'Status': status\n",
    "    })\n",
    "    \n",
    "    print(f\"{model_name:15} | Treino: {f1_train:.4f} | Teste: {f1_test:.4f} | Diff: {difference:+.4f} | {status}\")\n",
    "\n",
    "overfitting_df = pd.DataFrame(overfitting_analysis)\n",
    "overfitting_df.to_csv('overfitting_analysis.csv', index=False)\n",
    "print(\"\\nüíæ An√°lise de overfitting salva em 'overfitting_analysis.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf144e1",
   "metadata": {},
   "source": [
    "## 11. Identifica√ß√£o do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== IDENTIFICA√á√ÉO DO MELHOR MODELO ===\")\n",
    "\n",
    "if not metrics_df.empty:\n",
    "    # Encontrar o melhor modelo baseado no F1-Score de teste\n",
    "    best_model_idx = metrics_df['F1_Teste'].idxmax()\n",
    "    best_model_name = metrics_df.loc[best_model_idx, 'Modelo']\n",
    "    best_model_f1 = metrics_df.loc[best_model_idx, 'F1_Teste']\n",
    "    \n",
    "    print(f\"\\nüèÜ MELHOR MODELO: {best_model_name}\")\n",
    "    print(f\"F1-Score no teste: {best_model_f1:.4f}\")\n",
    "    \n",
    "    # Detalhes do melhor modelo\n",
    "    best_model_results = consolidated_results[best_model_name]\n",
    "    print(f\"\\nüìä DETALHES DO MELHOR MODELO ({best_model_name}):\")\n",
    "    print(f\"F1-Score CV: {best_model_results.get('cv_score', 0.0):.4f}\")\n",
    "    print(f\"Acur√°cia Teste: {best_model_results['test_metrics'].get('accuracy', 0.0):.4f}\")\n",
    "    print(f\"Precis√£o Teste: {best_model_results['test_metrics'].get('precision', 0.0):.4f}\")\n",
    "    print(f\"Recall Teste: {best_model_results['test_metrics'].get('recall', 0.0):.4f}\")\n",
    "    print(f\"G-Mean Teste: {best_model_results['test_metrics'].get('gmean', 0.0):.4f}\")\n",
    "    \n",
    "    if best_model_results['test_metrics'].get('auc_roc'):\n",
    "        print(f\"AUC-ROC Teste: {best_model_results['test_metrics']['auc_roc']:.4f}\")\n",
    "    \n",
    "    # Melhores hiperpar√¢metros\n",
    "    best_params = best_model_results.get('best_params', {})\n",
    "    if best_params:\n",
    "        print(f\"\\nüéØ MELHORES HIPERPAR√ÇMETROS:\")\n",
    "        for param, value in best_params.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {param}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # An√°lise qualitativa do desempenho\n",
    "    performance_quality = \"boa\" if best_model_f1 > 0.7 else \"moderada\" if best_model_f1 > 0.3 else \"baixa\"\n",
    "    print(f\"\\nüìà AVALIA√á√ÉO: F1-Score de {best_model_f1:.4f} indica {performance_quality} capacidade de predi√ß√£o\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è N√£o h√° dados suficientes para identificar o melhor modelo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1eb727",
   "metadata": {},
   "source": [
    "## 12. Resumo Final e Conclus√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a580be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RESUMO FINAL DAS M√âTRICAS ===\")\n",
    "\n",
    "if not metrics_df.empty:\n",
    "    print(f\"{'Modelo':<15} {'F1':<8} {'Acur√°cia':<10} {'Precis√£o':<10} {'Recall':<8} {'G-Mean':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for _, row in metrics_df.iterrows():\n",
    "        print(f\"{row['Modelo']:<15} \"\n",
    "              f\"{row['F1_Teste']:<8.4f} \"\n",
    "              f\"{row.get('Acur√°cia_Teste', 0.0):<10.4f} \"\n",
    "              f\"{row['Precis√£o_Teste']:<10.4f} \"\n",
    "              f\"{row['Recall_Teste']:<8.4f} \"\n",
    "              f\"{row['G-Mean_Teste']:<8.4f}\")\n",
    "\n",
    "print(\"\\n=== CONCLUS√ïES FINAIS ===\")\n",
    "if not metrics_df.empty:\n",
    "    best_model_name = metrics_df.loc[metrics_df['F1_Teste'].idxmax(), 'Modelo']\n",
    "    best_model_f1 = metrics_df['F1_Teste'].max()\n",
    "    \n",
    "    print(f\"‚Ä¢ O modelo {best_model_name} apresentou o melhor desempenho em dados n√£o vistos\")\n",
    "    print(f\"‚Ä¢ F1-Score de {best_model_f1:.4f} indica {'boa' if best_model_f1 > 0.7 else 'moderada' if best_model_f1 > 0.3 else 'baixa'} capacidade de predi√ß√£o\")\n",
    "    print(\"‚Ä¢ A busca de hiperpar√¢metros com RandomizedSearchCV e valida√ß√£o cruzada estratificada\")\n",
    "    print(\"  garantiu sele√ß√£o robusta dos melhores par√¢metros\")\n",
    "    print(\"‚Ä¢ M√©tricas como G-Mean s√£o importantes para datasets desbalanceados como este\")\n",
    "    print(f\"‚Ä¢ Total de {len(metrics_df)} modelos foram comparados nesta an√°lise\")\n",
    "else:\n",
    "    print(\"‚Ä¢ N√£o foi poss√≠vel realizar compara√ß√£o completa dos modelos\")\n",
    "    print(\"‚Ä¢ Execute os notebooks individuais dos modelos primeiro\")\n",
    "\n",
    "print(\"\\nüìÑ ARQUIVOS GERADOS:\")\n",
    "print(\"‚Ä¢ main_evaluation_results.csv - Resultados comparativos\")\n",
    "print(\"‚Ä¢ overfitting_analysis.csv - An√°lise de overfitting\")\n",
    "print(\"‚Ä¢ main_evaluation_comparison.png - Gr√°ficos comparativos\")\n",
    "print(\"‚Ä¢ confusion_matrices.png - Matrizes de confus√£o\")\n",
    "print(\"‚Ä¢ roc_curves.png - Curvas ROC (se dispon√≠vel)\")\n",
    "\n",
    "print(\"\\n‚úÖ AN√ÅLISE PRINCIPAL CONCLU√çDA!\")\n",
    "print(f\"üìä Total de modelos analisados: {len(consolidated_results)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
