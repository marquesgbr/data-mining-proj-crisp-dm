{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a2846b",
   "metadata": {},
   "source": [
    "# Data Preparation - Dataset de Sepsis\n",
    "## CRISP-DM Fase 3: Preparação dos Dados\n",
    "\n",
    "**Objetivo da Fase:**\n",
    "* Transformar dados brutos em formato adequado para modelagem\n",
    "* Implementar estratégias de limpeza e tratamento baseadas nos insights da EDA\n",
    "* Criar features derivadas com relevância clínica\n",
    "* Preparar datasets finais para algoritmos de machine learning\n",
    "\n",
    "**Baseado nos Insights da EDA:**\n",
    "* 37/41 variáveis apresentam missing values (68.37% do dataset)\n",
    "* 27 variáveis com >80% missing (candidatas à remoção)\n",
    "* Dataset altamente desbalanceado: 98.2% não-sepsis vs 1.8% sepsis\n",
    "* Estrutura temporal importante: risco aumenta após 100h na UTI\n",
    "* Variáveis categóricas bem definidas: Gender, Unit1, Unit2\n",
    "\n",
    "**Tarefas CRISP-DM a serem executadas:**\n",
    "1. **Seleção dos Dados**: Escolher variáveis mais relevantes\n",
    "2. **Limpeza dos Dados**: Tratar inconsistências e valores ausentes  \n",
    "3. **Construção dos Dados**: Criar features derivadas e engenharia\n",
    "4. **Integração dos Dados**: Combinar fontes (não aplicável aqui)\n",
    "5. **Formatação dos Dados**: Preparar formato final para modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21555a66",
   "metadata": {},
   "source": [
    "## Configuração do Ambiente Google Colab\n",
    "\n",
    "Para funcionar no Google Colab, é necessário criar um atalho do diretório MDA no seu próprio Drive e então rodar os dois comandos abaixo e conceder permissão ao seu drive quando rodar a célula logo abaixo.\n",
    "\n",
    "[Link](https://towardsdatascience.com/simplify-file-sharing-44bde79a8a18/) detalhando como funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f059104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modificar para o diretorio que contém os dados de teste e treino\n",
    "%cd /content/drive/MyDrive/MDA/Train\\ and\\ test\\ data\\ -\\ Proj\\ DM/\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fed35e",
   "metadata": {},
   "source": [
    "## 1. Importação das Bibliotecas\n",
    "\n",
    "Importação de todas as bibliotecas necessárias para preparação dos dados, incluindo bibliotecas específicas para pré-processamento, feature engineering e balanceamento de classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca5c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas essenciais para manipulação de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Bibliotecas para pré-processamento\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# Bibliotecas para balanceamento de classes\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Configurações gerais\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68cfe4b",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados e Insights da EDA\n",
    "\n",
    "Carregamento dos datasets de treino e teste, seguido da documentação dos principais insights obtidos na análise exploratória que guiarão as decisões de preparação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f4132",
   "metadata": {},
   "source": [
    "### 2.1 Carregamento do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "945c328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1241768, 41) | y_train: (1241768,)\n",
      "SepsisLabel\n",
      "0.0    0.982015\n",
      "1.0    0.017985\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('dataset_sepsis_train.csv')\n",
    "\n",
    "# Separar features e target\n",
    "X_train = train_df.drop('SepsisLabel', axis=1)\n",
    "y_train = train_df['SepsisLabel']\n",
    "\n",
    "# Forma final\n",
    "print(f\"X_train: {X_train.shape} | y_train: {y_train.shape}\")\n",
    "\n",
    "# Distribuição das classes no treino\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a29a8",
   "metadata": {},
   "source": [
    "## 3. TAREFA 1: Seleção dos Dados\n",
    "\n",
    "**Objetivo:** Escolher as variáveis mais relevantes para o modelo de mineração, removendo features com baixo potencial preditivo ou problemas graves de qualidade.\n",
    "\n",
    "**Critérios de seleção:**\n",
    "* Relevância clínica para detecção de sepsis\n",
    "* Percentual de missing values aceitável\n",
    "* Separabilidade entre classes (baseada na EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90a9f6",
   "metadata": {},
   "source": [
    "### 3.1 Mapeamento de Variáveis com Excesso de Missing Values para Remoção\n",
    "\n",
    "Vamos refazer a análise, mais objetiva e breve, das variáveis com >60% missing values para decidir quais manter, tratar ou remover baseado no critério de separabilidade de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e4c277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMPUTAR_SIMPLES (2 variáveis):\n",
      "  • Temp: Sep=0.393 | Missing=66.2% | n=419,945\n",
      "  • BUN: Sep=0.353 | Missing=93.1% | n=85,440\n",
      "\n",
      "IMPUTAR_AVANCADA (2 variáveis):\n",
      "  • Platelets: Sep=0.203 | Missing=94.1% | n=73,790\n",
      "  • WBC: Sep=0.194 | Missing=93.6% | n=79,613\n",
      "\n",
      "DESCARTAR (24 variáveis):\n",
      "  • Hgb: Sep=0.152 | Missing=92.6% | n=91,759\n",
      "  • Creatinine: Sep=0.151 | Missing=93.9% | n=75,809\n",
      "  • pH: Sep=0.135 | Missing=93.1% | n=86,094\n",
      "  • Fibrinogen: Sep=0.134 | Missing=99.3% | n=8,203\n",
      "  • Hct: Sep=0.091 | Missing=91.1% | n=109,980\n",
      "  • PTT: Sep=0.085 | Missing=97.0% | n=36,690\n",
      "  • Calcium: Sep=0.083 | Missing=94.1% | n=73,269\n",
      "  • Bilirubin_total: Sep=0.077 | Missing=98.5% | n=18,518\n",
      "  • Alkalinephos: Sep=0.071 | Missing=98.4% | n=19,954\n",
      "  • Phosphate: Sep=0.070 | Missing=96.0% | n=50,011\n",
      "  • Bilirubin_direct: Sep=0.062 | Missing=99.8% | n=2,393\n",
      "  • Lactate: Sep=0.048 | Missing=97.3% | n=33,238\n",
      "  • Glucose: Sep=0.039 | Missing=82.9% | n=212,578\n",
      "  • AST: Sep=0.015 | Missing=98.4% | n=20,144\n",
      "  • Potassium: Sep=0.008 | Missing=90.7% | n=115,900\n",
      "  • TroponinI: Sep=0.000 | Missing=99.1% | n=11,743\n",
      "  • EtCO2: Sep=0.000 | Missing=96.3% | n=46,047\n",
      "  • BaseExcess: Sep=0.000 | Missing=94.6% | n=67,324\n",
      "  • HCO3: Sep=0.000 | Missing=95.8% | n=52,334\n",
      "  • FiO2: Sep=0.000 | Missing=91.7% | n=103,618\n",
      "  • PaCO2: Sep=0.000 | Missing=94.4% | n=69,132\n",
      "  • SaO2: Sep=0.000 | Missing=96.6% | n=42,777\n",
      "  • Chloride: Sep=0.000 | Missing=95.4% | n=56,701\n",
      "  • Magnesium: Sep=0.000 | Missing=93.7% | n=78,652\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identificar variáveis com >60% missing\n",
    "high_missing_vars = []\n",
    "for col in X_train.select_dtypes(include=[np.number]).columns:\n",
    "    missing_pct = (X_train[col].isnull().sum() / len(X_train)) * 100\n",
    "    if missing_pct > 60:\n",
    "        high_missing_vars.append({\n",
    "            'variavel': col,\n",
    "            'missing_pct': missing_pct\n",
    "        })\n",
    "\n",
    "# Calcular separabilidade para cada variável\n",
    "separability_results = {\n",
    "    'IMPUTAR_SIMPLES': [],     # Separabilidade > 0.3: Alta discriminação\n",
    "    'IMPUTAR_AVANCADA': [],    # Separabilidade 0.16 - 0.3: Discriminação moderada  \n",
    "    'DESCARTAR': []            # Separabilidade < 0.16: Baixa discriminação\n",
    "}\n",
    "\n",
    "for var_info in high_missing_vars:\n",
    "    col = var_info['variavel']\n",
    "    missing_pct = var_info['missing_pct']\n",
    "    \n",
    "    # Criar DataFrame temporário sem valores faltantes\n",
    "    temp_df = pd.DataFrame({\n",
    "        'feature': X_train[col],\n",
    "        'target': y_train\n",
    "    }).dropna()\n",
    "\n",
    "    # Separar por classe\n",
    "    no_sepsis_data = temp_df[temp_df['target'] == 0]['feature']\n",
    "    sepsis_data = temp_df[temp_df['target'] == 1]['feature']\n",
    "\n",
    "    # Calcular separabilidade (diferença de medianas / desvio padrão)\n",
    "    median_diff = abs(sepsis_data.median() - no_sepsis_data.median())\n",
    "    pooled_std = no_sepsis_data.std() if no_sepsis_data.std() > 0 else 1\n",
    "    separability = median_diff / pooled_std\n",
    "    \n",
    "    # Classificar baseado na separabilidade\n",
    "    var_result = {\n",
    "        'variavel': col,\n",
    "        'missing_pct': missing_pct,\n",
    "        'separabilidade': separability,\n",
    "        'n_amostras': len(temp_df)\n",
    "    }\n",
    "    \n",
    "    if separability > 0.3:\n",
    "        separability_results['IMPUTAR_SIMPLES'].append(var_result)\n",
    "    elif separability >= 0.16:\n",
    "        separability_results['IMPUTAR_AVANCADA'].append(var_result)\n",
    "    else:\n",
    "        separability_results['DESCARTAR'].append(var_result)\n",
    "\n",
    "# Exibir resultados \n",
    "\n",
    "for categoria, vars_list in separability_results.items():\n",
    "    print(f\"\\n{categoria} ({len(vars_list)} variáveis):\")\n",
    "    for var in sorted(vars_list, key=lambda x: x['separabilidade'], reverse=True):\n",
    "        print(f\"  • {var['variavel']}: Sep={var['separabilidade']:.3f} | Missing={var['missing_pct']:.1f}% | n={var['n_amostras']:,}\")\n",
    "\n",
    "variables_to_keep = [var['variavel'] for var in separability_results['IMPUTAR_SIMPLES']]\n",
    "variables_to_treat = [var['variavel'] for var in separability_results['IMPUTAR_AVANCADA']]  \n",
    "variables_to_discard = [var['variavel'] for var in separability_results['DESCARTAR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9bb2c",
   "metadata": {},
   "source": [
    "### 3.2 Análise de Separabilidade Estatística\n",
    "\n",
    "Avaliação da capacidade discriminativa das variáveis que não foram selecionadas para exclusão, a fim de confirmar e justificar as decisões antes de fazer a remoção, usando testes estatísticos e métricas de separação entre classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a8bc6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANÁLISE DE VARIÁVEIS NUMÉRICAS:\n",
      "Variável        Missing%   Separab.   p-value_MW   Mutual Info  N_samples \n",
      "\n",
      "Hour            0.0        0.303      0.00000      0.0038413    1,241,768 \n",
      "HR              9.9        0.386      0.00000      0.0010304    1,119,123 \n",
      "O2Sat           13.1       0.000      0.00079      0.0000634    1,079,708 \n",
      "Temp            66.2       0.326      0.00000      0.0020082    419,945   \n",
      "SBP             14.6       0.124      0.00000      0.0001972    1,060,857 \n",
      "MAP             12.4       0.121      0.00000      0.0002270    1,087,236 \n",
      "DBP             31.3       0.143      0.00000      0.0001955    852,691   \n",
      "Resp            15.3       0.356      0.00000      0.0010005    1,051,181 \n",
      "BUN             93.1       0.328      0.00000      0.0018243    85,440    \n",
      "WBC             93.6       0.166      0.00000      0.0009935    79,613    \n",
      "Platelets       94.1       0.189      0.00000      0.0007498    73,790    \n",
      "Age             0.0        0.000      0.43679      0.0000031    1,241,768 \n",
      "HospAdmTime     0.0        0.017      0.00000      0.0010616    1,241,762 \n",
      "ICULOS          0.0        0.302      0.00000      0.0040386    1,241,768 \n",
      "\n",
      "ANÁLISE DE VARIÁVEIS CATEGÓRICAS:\n",
      "Variável        Missing%   p-value_Chi2 Mutual Info  N_samples \n",
      "\n",
      "Gender          0.0        0.00000      0.0000457    1,241,768 \n",
      "Unit1           39.5       0.00000      0.0002854    751,787   \n",
      "Unit2           39.5       0.00000      0.0002854    751,787   \n",
      "\n",
      "Separabilidades Numéricas:\n",
      "  • Variáveis com Sep > 0.3: 6\n",
      "  • Variáveis com Sep > 0.16: 8\n",
      "\n",
      "SIGNIFICÂNCIA ESTATÍSTICA (Numéricas):\n",
      "  • p < 0.001 (altamente significativo): 13 variáveis\n",
      "  • 0.001 ≤ p < 0.01 (muito significativo): 0 variáveis\n",
      "  • 0.01 ≤ p < 0.05 (significativo): 0 variáveis\n",
      "  • p ≥ 0.05 (não significativo): 1 variáveis\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "X_train_not_discard = X_train.drop(columns=variables_to_discard)\n",
    "# Separar variáveis numéricas e categóricas\n",
    "categorical_vars = ['Gender', 'Unit1', 'Unit2']  \n",
    "# Numéricas são todas as colunas MENOS as categóricas\n",
    "numeric_vars = [col for col in X_train_not_discard.columns if col not in categorical_vars]\n",
    "\n",
    "\n",
    "# Análise para variáveis numéricas\n",
    "all_separability_results = []\n",
    "\n",
    "print(f\"\\nANÁLISE DE VARIÁVEIS NUMÉRICAS:\")\n",
    "print(f\"{'Variável':<15} {'Missing%':<10} {'Separab.':<10} {'p-value_MW':<12} {'Mutual Info':<12} {'N_samples':<10}\\n\")\n",
    "\n",
    "for var in numeric_vars:\n",
    "    missing_pct = (X_train_not_discard[var].isnull().sum() / len(X_train_not_discard)) * 100\n",
    "    \n",
    "    # Criar DataFrame temporário sem valores faltantes\n",
    "    temp_df = pd.DataFrame({\n",
    "        'feature': X_train_not_discard[var],\n",
    "        'target': y_train\n",
    "    }).dropna()\n",
    "    \n",
    "    # Separar por classe\n",
    "    no_sepsis_data = temp_df[temp_df['target'] == 0]['feature']\n",
    "    sepsis_data = temp_df[temp_df['target'] == 1]['feature']\n",
    "\n",
    "    # Calcular separabilidade (diferença de medianas / desvio padrão)\n",
    "    median_diff = abs(sepsis_data.median() - no_sepsis_data.median())\n",
    "    pooled_std = np.sqrt(((no_sepsis_data.std()**2 + sepsis_data.std()**2) / 2))\n",
    "    separability = median_diff / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    # Teste U de Mann-Whitney (não-paramétrico)\n",
    "    try:\n",
    "        stat, p_value = stats.mannwhitneyu(sepsis_data, no_sepsis_data, alternative='two-sided')\n",
    "        mann_whitney_pval = p_value\n",
    "    except:\n",
    "        mann_whitney_pval = 1.0  # p-value máximo para casos de erro\n",
    "\n",
    "    # Informação mútua\n",
    "    try:\n",
    "        # Discretizar para mutual info (usar quintis)\n",
    "        temp_df['feature_disc'] = pd.qcut(temp_df['feature'], q=5, labels=False, duplicates='drop')\n",
    "        mutual_info = mutual_info_score(temp_df['target'], temp_df['feature_disc'])\n",
    "    except:\n",
    "        mutual_info = 0\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    result = {\n",
    "        'variavel': var,\n",
    "        'missing_pct': missing_pct,\n",
    "        'separabilidade': separability,\n",
    "        'mann_whitney': mann_whitney_pval,\n",
    "        'mutual_info': mutual_info,\n",
    "        'n_amostras': len(temp_df)\n",
    "    }\n",
    "    all_separability_results.append(result)\n",
    "    \n",
    "    # Exibir resultado\n",
    "    print(f\"{var:<15} {missing_pct:<10.1f} {separability:<10.3f} {mann_whitney_pval:<12.5f} {mutual_info:<12.7f} {len(temp_df):<10,}\")\n",
    "\n",
    "\n",
    "# Análise para variáveis categóricas\n",
    "print(f\"\\nANÁLISE DE VARIÁVEIS CATEGÓRICAS:\")\n",
    "print(f\"{'Variável':<15} {'Missing%':<10} {'p-value_Chi2':<12} {'Mutual Info':<12} {'N_samples':<10}\\n\")\n",
    "\n",
    "for var in categorical_vars:\n",
    "    missing_pct = (X_train_not_discard[var].isnull().sum() / len(X_train_not_discard)) * 100\n",
    "    \n",
    "    # Criar DataFrame temporário sem valores faltantes\n",
    "    temp_df = pd.DataFrame({\n",
    "        'feature': X_train_not_discard[var],\n",
    "        'target': y_train\n",
    "    }).dropna()\n",
    "    \n",
    "    # Teste Qui-quadrado\n",
    "    try:\n",
    "        contingency_table = pd.crosstab(temp_df['feature'], temp_df['target'])\n",
    "        chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "        chi2_pval = p_value\n",
    "    except:\n",
    "        chi2_pval = 1.0  # p-value máximo para casos de erro\n",
    "    \n",
    "    # Informação mútua\n",
    "    try:\n",
    "        mutual_info = mutual_info_score(temp_df['target'], temp_df['feature'])\n",
    "    except:\n",
    "        mutual_info = 0\n",
    "    \n",
    "    result = {\n",
    "        'variavel': var,\n",
    "        'missing_pct': missing_pct,\n",
    "        'chi2_sig': chi2_pval,\n",
    "        'mutual_info': mutual_info,\n",
    "        'n_amostras': len(temp_df),\n",
    "        'tipo': 'categorical'\n",
    "    }\n",
    "    all_separability_results.append(result)\n",
    "    \n",
    "    print(f\"{var:<15} {missing_pct:<10.1f} {chi2_pval:<12.5f} {mutual_info:<12.7f} {len(temp_df):<10,}\")\n",
    "\n",
    "\n",
    "# Ranking por separabilidade (variáveis numéricas)\n",
    "numeric_results = [r for r in all_separability_results if 'separabilidade' in r]\n",
    "numeric_results_sorted = sorted(numeric_results, key=lambda x: x['separabilidade'], reverse=True)\n",
    "\n",
    "print(f\"\\nSeparabilidades Numéricas:\")\n",
    "separabilities = [r['separabilidade'] for r in numeric_results]\n",
    "print(f\"  • Variáveis com Sep > 0.3: {sum(1 for s in separabilities if s > 0.3)}\")\n",
    "print(f\"  • Variáveis com Sep > 0.16: {sum(1 for s in separabilities if s > 0.16)}\")\n",
    "\n",
    "# Análise de significância estatística\n",
    "print(f\"\\nSIGNIFICÂNCIA ESTATÍSTICA (Numéricas):\")\n",
    "sig_001 = sum(1 for r in numeric_results if r['mann_whitney'] < 0.001)\n",
    "sig_01 = sum(1 for r in numeric_results if 0.001 <= r['mann_whitney'] < 0.01)\n",
    "sig_05 = sum(1 for r in numeric_results if 0.01 <= r['mann_whitney'] < 0.05)\n",
    "not_sig = sum(1 for r in numeric_results if r['mann_whitney'] >= 0.05)\n",
    "print(f\"  • p < 0.001 (altamente significativo): {sig_001} variáveis\")\n",
    "print(f\"  • 0.001 ≤ p < 0.01 (muito significativo): {sig_01} variáveis\")  \n",
    "print(f\"  • 0.01 ≤ p < 0.05 (significativo): {sig_05} variáveis\")\n",
    "print(f\"  • p ≥ 0.05 (não significativo): {not_sig} variáveis\")\n",
    "\n",
    "# Salvar resultados para uso posterior\n",
    "statistical_analysis_results = {\n",
    "    'numeric_results': numeric_results_sorted,\n",
    "    'all_results': all_separability_results\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ec87c",
   "metadata": {},
   "source": [
    "#### Alteração Após Análise \n",
    "Percebe-se que ainda é possível remover `Age` do escopo de features visto que não há nenhuma métrica que aponte essa variável como algo relevante apesar do que diz a literatura sobre sepsis e o baixo percentual de missing values. Ela possui baixa separabilidade, um Man Whitney não significativo e Mutual Info demonstra zero informação sobre sepse\n",
    "\n",
    "| Variável | Missing% | Separabilidade |  p-value_MW | Mutual Info |\n",
    "|----------|----------|----------------|--------------|-------------| \n",
    "| **Age** | 0.0 | 0.000 |  0.4367935 | 0.00000 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977f647",
   "metadata": {},
   "source": [
    "### 3.3 Aplicação das Decisões de Separabilidade\n",
    "\n",
    "Implementação prática da remoção de variáveis com baixa separabilidade e organização das listas para tratamento adiante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d39f998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo 25 variáveis com baixa separabilidade...\n",
      "Variáveis removidas:\n",
      "  • EtCO2: 96.3% missing\n",
      "  • BaseExcess: 94.6% missing\n",
      "  • HCO3: 95.8% missing\n",
      "  • FiO2: 91.7% missing\n",
      "  • pH: 93.1% missing\n",
      "  • PaCO2: 94.4% missing\n",
      "  • SaO2: 96.6% missing\n",
      "  • AST: 98.4% missing\n",
      "  • Alkalinephos: 98.4% missing\n",
      "  • Calcium: 94.1% missing\n",
      "  • Chloride: 95.4% missing\n",
      "  • Creatinine: 93.9% missing\n",
      "  • Bilirubin_direct: 99.8% missing\n",
      "  • Glucose: 82.9% missing\n",
      "  • Lactate: 97.3% missing\n",
      "  • Magnesium: 93.7% missing\n",
      "  • Phosphate: 96.0% missing\n",
      "  • Potassium: 90.7% missing\n",
      "  • Bilirubin_total: 98.5% missing\n",
      "  • TroponinI: 99.1% missing\n",
      "  • Hct: 91.1% missing\n",
      "  • Hgb: 92.6% missing\n",
      "  • PTT: 97.0% missing\n",
      "  • Fibrinogen: 99.3% missing\n",
      "  • Age: 0.0% missing (removida por baixa discriminação)\n",
      "\n",
      "Dimensões do dataset:\n",
      "  • Original: (1241768, 41)\n",
      "  • Após seleção: (1241768, 16)\n"
     ]
    }
   ],
   "source": [
    "# Adicionar Age às variáveis a descartar \n",
    "variables_to_discard.append('Age')\n",
    "\n",
    "# Remover variáveis com baixa separabilidade e alto missing do dataset principal\n",
    "total_to_remove = len(variables_to_discard)\n",
    "print(f\"Removendo {total_to_remove} variáveis com baixa separabilidade...\")\n",
    "\n",
    "X_train_selected = X_train.drop(columns=variables_to_discard)\n",
    "\n",
    "print(\"Variáveis removidas:\")\n",
    "for var in variables_to_discard:\n",
    "    missing_pct = (X_train[var].isnull().sum() / len(X_train)) * 100\n",
    "    if var == 'Age':\n",
    "        print(f\"  • {var}: {missing_pct:.1f}% missing (removida por baixa discriminação)\")\n",
    "    else:\n",
    "        print(f\"  • {var}: {missing_pct:.1f}% missing\")\n",
    "\n",
    "print(f\"\\nDimensões do dataset:\")\n",
    "print(f\"  • Original: {X_train.shape}\")\n",
    "print(f\"  • Após seleção: {X_train_selected.shape}\")\n",
    "\n",
    "# Organizar variáveis por estratégia de tratamento\n",
    "high_missing_strategy = {\n",
    "    'imputacao_simples': variables_to_keep,      \n",
    "    'imputacao_avancada': variables_to_treat,    \n",
    "    'removidas': variables_to_discard           \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63353242",
   "metadata": {},
   "source": [
    "### 3.4 Síntese das Decisões de Seleção de Variáveis\n",
    "\n",
    "**Documentação completa das decisões tomadas na Tarefa 1 (Seleção dos Dados) com respectivas justificativas:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759d1bb",
   "metadata": {},
   "source": [
    "\n",
    "RESULTADOS FINAIS DA SELEÇÃO DE VARIÁVEIS\n",
    "\n",
    "CRITÉRIOS DE SELEÇÃO APLICADOS\n",
    "\n",
    "1. **Critério de Missing Values**: Variáveis com >60% de valores ausentes analisadas\n",
    "2. **Critério de Separabilidade**: Capacidade discriminativa entre classes (limite: 0.16)\n",
    "3. **Critério Estatístico**: Significância nos testes Mann-Whitney U e Chi-quadrado\n",
    "\n",
    "IMPACTO FINAL DAS DECISÕES\n",
    "\n",
    "**Redução Dimensional Efetiva:**\n",
    "- **Dataset original**: 1,241,768 × 41 variáveis\n",
    "- **Dataset final**: 1,241,768 × 16 variáveis  \n",
    "- **Redução**: 61% das variáveis removidas (25/41)\n",
    "- **Taxa de compressão**: 2.6:1\n",
    "\n",
    "ESTRATÉGIAS DE TRATAMENTO DEFINIDAS\n",
    "\n",
    "**IMPUTAÇÃO Cuidadosa** (1 variáveis - Separabilidade > 0.3)\n",
    "\n",
    "**Estratégia**: Imputação com medidas robustas (mediana) + validação clínica\n",
    "\n",
    "| Variável | Missing% | Separabilidade | p-value | Justificativa Médica |\n",
    "|----------|----------|----------------|---------|---------------------|\n",
    "| **Temp** | 66.2% | 0.326 | < 0.001 | Temperatura corporal: indicador direto de resposta inflamatória |\n",
    "\n",
    "**IMPUTAÇÃO Específica** (3 variáveis - Separabilidade 0.16-0.3 ou Separabilidade>0.3 e Missing>90%)  \n",
    "\n",
    "**Estratégia**: Técnicas sofisticadas (KNN, regressão) devido à considerável relevância clínica \n",
    "\n",
    "| Variável | Missing% | Separabilidade | p-value | Justificativa Médica |\n",
    "|----------|----------|----------------|---------|---------------------|\n",
    "| **BUN** | 93.1% | 0.328 | < 0.001 | Função renal: biomarcador de disfunção orgânica na sepsis |\n",
    "| **Platelets** | 94.1% | 0.189 | < 0.001 | Coagulação: trombocitopenia marca disfunção hemostática |\n",
    "| **WBC** | 93.6% | 0.166 | < 0.001 | Sistema imune: resposta leucocitária à infecção |\n",
    "\n",
    "**REMOVIDAS** (25 variáveis - Separabilidade < 0.16)\n",
    "\n",
    "**Critério duplo**: Baixa discriminação + Alto missing (>60%)\n",
    "\n",
    "**Destaques das remoções:**\n",
    "- **Age**: 0.0% missing, Sep: 0.000, p-value: 0.437 (única exceção por baixa discriminação)\n",
    "- **24 variáveis** com >80% missing + separabilidade < 0.16\n",
    "- **Maior redução**: TroponinI (99.1% missing), Bilirubin_direct (99.8% missing)\n",
    "\n",
    "VALIDAÇÃO ESTATÍSTICA FINAL\n",
    "\n",
    "**Testes Aplicados:**\n",
    "- **Mann-Whitney U**: Para variáveis numéricas (não-paramétrico)\n",
    "- **Qui-quadrado**: Para variáveis categóricas\n",
    "- **Informação Mútua**: Medida de dependência entre variáveis\n",
    "\n",
    "**Significância dos Testes:**\n",
    "- **Variáveis numéricas significativas**: 13/14 (p < 0.05)\n",
    "- **Variáveis categóricas significativas**: 3/3 (p < 0.001)  \n",
    "- **Taxa de significância geral**: 94.1% (16/17 variáveis)\n",
    "\n",
    "**Estratégias de Tratamento Definidas:**\n",
    "- **Imputação cuidadosa**: 2 variáveis de alta relevância\n",
    "- **Imputação específica**: 6 variáveis de relevância moderada\n",
    "- **Manutenção**: 13 variáveis com baixo missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995bf50",
   "metadata": {},
   "source": [
    "## 4. TAREFA 2: Limpeza dos Dados\n",
    "\n",
    "**Objetivo:** Corrigir ou remover dados inconsistentes, duplicados ou ausentes através de estratégias específicas para cada tipo de variável.\n",
    "\n",
    "**Estratégias por tipo de missing:**\n",
    "* Missing < 20%: Imputação simples (mediana/moda)\n",
    "* Missing >= 20%: Imputação baseada em modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34539dc5",
   "metadata": {},
   "source": [
    "### 4.1 Detecção e Remoção de Duplicatas\n",
    "\n",
    "Identificação de registros duplicados exatos e tratamento adequado considerando a natureza temporal dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44273b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECÇÃO DE DUPLICATAS:\n",
      "Dataset atual: (1241768, 16)\n",
      "Duplicatas exatas encontradas: 32,571\n",
      "Remover 32,571 duplicatas exatas\n",
      "Duplicatas exatas encontradas: 32,571\n",
      "Remover 32,571 duplicatas exatas\n",
      "Dataset após remoção: (1209197, 16)\n",
      "\n",
      "PROPORÇÃO DAS CLASSES APÓS REMOÇÃO:\n",
      "Antes da remoção de duplicatas:\n",
      "  • SepsisLabel = 0: 1,219,435 (0.9820)\n",
      "  • SepsisLabel = 1: 22,333 (0.0180)\n",
      "Após remoção de duplicatas:\n",
      "  • SepsisLabel = 0: 1,187,303 (0.9819)\n",
      "  • SepsisLabel = 1: 21,894 (0.0181)\n",
      "\n",
      "IMPACTO NA PROPORÇÃO:\n",
      "  • Mudança SepsisLabel = 0: -0.0001\n",
      "  • Mudança SepsisLabel = 1: +0.0001\n",
      "Dataset limpo final: (1209197, 16)\n",
      "Dataset após remoção: (1209197, 16)\n",
      "\n",
      "PROPORÇÃO DAS CLASSES APÓS REMOÇÃO:\n",
      "Antes da remoção de duplicatas:\n",
      "  • SepsisLabel = 0: 1,219,435 (0.9820)\n",
      "  • SepsisLabel = 1: 22,333 (0.0180)\n",
      "Após remoção de duplicatas:\n",
      "  • SepsisLabel = 0: 1,187,303 (0.9819)\n",
      "  • SepsisLabel = 1: 21,894 (0.0181)\n",
      "\n",
      "IMPACTO NA PROPORÇÃO:\n",
      "  • Mudança SepsisLabel = 0: -0.0001\n",
      "  • Mudança SepsisLabel = 1: +0.0001\n",
      "Dataset limpo final: (1209197, 16)\n"
     ]
    }
   ],
   "source": [
    "# Verificar duplicatas no dataset selecionado\n",
    "print(\"DETECÇÃO DE DUPLICATAS:\")\n",
    "print(f\"Dataset atual: {X_train_selected.shape}\")\n",
    "\n",
    "# Verificar duplicatas exatas (todas as colunas)\n",
    "duplicatas_exatas = X_train_selected.duplicated().sum()\n",
    "print(f\"Duplicatas exatas encontradas: {duplicatas_exatas:,}\")\n",
    "\n",
    "print(f\"Remover {duplicatas_exatas:,} duplicatas exatas\")\n",
    "X_train_cleaned = X_train_selected.drop_duplicates()\n",
    "y_train_cleaned = y_train.loc[X_train_cleaned.index]\n",
    "print(f\"Dataset após remoção: {X_train_cleaned.shape}\")\n",
    "\n",
    "# Verificar proporção das classes após remoção de duplicatas\n",
    "print(f\"\\nPROPORÇÃO DAS CLASSES APÓS REMOÇÃO:\")\n",
    "print(\"Antes da remoção de duplicatas:\")\n",
    "original_counts = y_train.value_counts()\n",
    "original_props = y_train.value_counts(normalize=True)\n",
    "print(f\"  • SepsisLabel = 0: {original_counts[0]:,} ({original_props[0]:.4f})\")\n",
    "print(f\"  • SepsisLabel = 1: {original_counts[1]:,} ({original_props[1]:.4f})\")\n",
    "\n",
    "print(\"Após remoção de duplicatas:\")\n",
    "cleaned_counts = y_train_cleaned.value_counts()\n",
    "cleaned_props = y_train_cleaned.value_counts(normalize=True)\n",
    "print(f\"  • SepsisLabel = 0: {cleaned_counts[0]:,} ({cleaned_props[0]:.4f})\")\n",
    "print(f\"  • SepsisLabel = 1: {cleaned_counts[1]:,} ({cleaned_props[1]:.4f})\")\n",
    "\n",
    "# Calcular impacto na proporção\n",
    "prop_change_0 = cleaned_props[0] - original_props[0]\n",
    "prop_change_1 = cleaned_props[1] - original_props[1]\n",
    "print(f\"\\nIMPACTO NA PROPORÇÃO:\")\n",
    "print(f\"  • Mudança SepsisLabel = 0: {prop_change_0:+.4f}\")\n",
    "print(f\"  • Mudança SepsisLabel = 1: {prop_change_1:+.4f}\")\n",
    "\n",
    "print(f\"Dataset limpo final: {X_train_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0084e2b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5cb71fe",
   "metadata": {},
   "source": [
    "### 4.2 Tratamento e Análise de Outliers\n",
    "\n",
    "A ideia é tentar preservar os outliers visto que eles se demonstraram relevantes para a identificação de instâncias com SepsisLabel=1 na Análise Exploratória.\n",
    "Vamos apenas deixar algumas variáveis mais genéricas e conhecidas mais consistentes e fazer uma análise geral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c1182fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECÇÃO E TRATAMENTO DE OUTLIERS:\n",
      "\n",
      "Variáveis numéricas para análise: 16\n",
      "\n",
      "RESUMO DE OUTLIERS DETECTADOS:\n",
      "Variável     Clínicos  IQR      Z-score  N_total  Range Original       Range Tratado       \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hour         0         54,951   27,883   1,209,197 0.00-335.00          0.00-335.00         \n",
      "HR           2         11,203   5,502    1,119,122 20.00-280.00         20.00-250.00        \n",
      "O2Sat        0         19,905   8,912    1,079,707 20.00-100.00         20.00-100.00        \n",
      "Temp         11        5,223    3,392    419,945  23.00-50.00          28.00-42.00         \n",
      "SBP          0         12,748   6,046    1,060,857 20.00-300.00         20.00-300.00        \n",
      "MAP          0         17,543   8,057    1,087,236 20.00-300.00         20.00-300.00        \n",
      "DBP          0         13,033   6,560    852,691  20.00-300.00         20.00-300.00        \n",
      "Resp         0         22,208   10,343   1,051,178 1.00-100.00          1.00-100.00         \n",
      "BUN          0         7,039    2,063    85,439   1.00-268.00          1.00-268.00         \n",
      "WBC          0         2,767    514      79,613   0.10-440.00          0.10-440.00         \n",
      "Platelets    0         2,369    1,070    73,790   2.00-2322.00         2.00-2322.00        \n",
      "Gender       0         0        0        1,209,197 0.00-1.00            0.00-1.00           \n",
      "Unit1        0         0        0        737,114  0.00-1.00            0.00-1.00           \n",
      "Unit2        0         0        0        737,114  0.00-1.00            0.00-1.00           \n",
      "HospAdmTime  1,161,456 161,629  17,170   1,209,191 -5366.86-23.99       0.00-23.99          \n",
      "ICULOS       0         54,672   27,811   1,209,197 1.00-336.00          1.00-336.00         \n",
      "\n",
      "Total de Caps Aplicados: 1,161,469\n"
     ]
    }
   ],
   "source": [
    "# Tratamento de outliers para variáveis numéricas\n",
    "from scipy import stats\n",
    "\n",
    "print(\"DETECÇÃO E TRATAMENTO DE OUTLIERS:\\n\")\n",
    "\n",
    "# Separar variáveis numéricas do dataset limpo\n",
    "numeric_cols = X_train_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Variáveis numéricas para análise: {len(numeric_cols)}\")\n",
    "\n",
    "# Definir limites um pouco mais realistas para algumas variáveis \n",
    "# Considerando registros de outros casos extremos e do próprio dataset\n",
    "# Propósito de deixar os dados mais consistentes\n",
    "clinical_limits = {\n",
    "    'HR': (20, 250),           # Batimentos cardíacos: 20-250 bpm\n",
    "    'Temp': (28, 42),          # Temperatura: 28-42°C\n",
    "    'Hour': (0, 336),          # Horas na UTI: 1-336h (14 dias)\n",
    "    'ICULOS': (0, 336),        # Tempo UTI: 1-336h\n",
    "    'HospAdmTime': (0, 24),   # Tempo hospital: 0 a 24h\n",
    "}\n",
    "\n",
    "outliers_summary = {}\n",
    "X_train_outliers_treated = X_train_cleaned.copy()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    data = X_train_outliers_treated[col].dropna()\n",
    "        \n",
    "    # Cap do Range (quando aplicável)\n",
    "    clinical_outliers = 0\n",
    "    if col in clinical_limits:\n",
    "        min_val, max_val = clinical_limits[col]\n",
    "        clinical_mask = (data < min_val) | (data > max_val)\n",
    "        clinical_outliers = clinical_mask.sum()\n",
    "        \n",
    "        # Aplicar capping\n",
    "        X_train_outliers_treated.loc[X_train_outliers_treated[col] < min_val, col] = min_val\n",
    "        X_train_outliers_treated.loc[X_train_outliers_treated[col] > max_val, col] = max_val\n",
    "    \n",
    "    # Análise do IQR\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    iqr_outliers = ((data < lower_bound) | (data > upper_bound)).sum()\n",
    "    \n",
    "    # Análise do Z-score (outliers > 3 desvios padrão)\n",
    "    z_scores = np.abs(stats.zscore(data))\n",
    "    zscore_outliers = (z_scores > 3).sum()\n",
    "    \n",
    "    outliers_summary[col] = {\n",
    "        'clinical': clinical_outliers,\n",
    "        'iqr': iqr_outliers,\n",
    "        'zscore': zscore_outliers,\n",
    "        'total_values': len(data),\n",
    "        'range_original': (data.min(), data.max()),\n",
    "        'range_treated': (X_train_outliers_treated[col].min(), X_train_outliers_treated[col].max())\n",
    "    }\n",
    "\n",
    "# Exibir resumo dos outliers\n",
    "print(f\"\\nRESUMO DE OUTLIERS DETECTADOS:\")\n",
    "print(f\"{'Variável':<12} {'Clínicos':<9} {'IQR':<8} {'Z-score':<8} {'N_total':<8} {'Range Original':<20} {'Range Tratado':<20}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for col, summary in outliers_summary.items():\n",
    "    clinical = summary['clinical']\n",
    "    iqr = summary['iqr'] \n",
    "    zscore = summary['zscore']\n",
    "    total = summary['total_values']\n",
    "    range_orig = f\"{summary['range_original'][0]:.2f}-{summary['range_original'][1]:.2f}\"\n",
    "    range_treat = f\"{summary['range_treated'][0]:.2f}-{summary['range_treated'][1]:.2f}\"\n",
    "\n",
    "    print(f\"{col:<12} {clinical:<9,} {iqr:<8,} {zscore:<8,} {total:<8,} {range_orig:<20} {range_treat:<20}\")\n",
    "\n",
    "# Estatísticas finais\n",
    "total_clinical_corrections = sum(summary['clinical'] for summary in outliers_summary.values())\n",
    "print(f\"\\nTotal de Caps Aplicados: {total_clinical_corrections:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe2b1d",
   "metadata": {},
   "source": [
    "### 4.3 Estratégias de Imputação\n",
    "\n",
    "Implementação de diferentes técnicas de imputação baseadas no tipo de variável e percentual de missing values.\n",
    "\n",
    "**OBSERVAÇÃO:**\n",
    "As seções `4.3.1` e `4.3.2` precisam ser executadas em ordem e são necessárias para que as demais seções funcionem. Porém `4.3.3`, `4.3.4`, `4.3.5` podem ser executadas em qualquer ordem após executar `4.3.1` e `4.3.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc5442",
   "metadata": {},
   "source": [
    "#### 4.3.1 Imputação para Variáveis com Baixo Missing (<20%)\n",
    "\n",
    "Aplicação de imputação simples usando medidas centrais apropriadas para cada tipo de variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d173ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variáveis para imputação simples:\n",
      "  • HR: 90,075 valores (7.4%)\n",
      "  • O2Sat: 129,490 valores (10.7%)\n",
      "  • SBP: 148,340 valores (12.3%)\n",
      "  • MAP: 121,961 valores (10.1%)\n",
      "  • Resp: 158,019 valores (13.1%)\n",
      "  • HospAdmTime: 6 valores (0.0%)\n",
      "\n",
      "VERIFICAÇÃO PÓS-IMPUTAÇÃO:\n",
      "  • HR: 0 valores missing restantes\n",
      "  • O2Sat: 0 valores missing restantes\n",
      "  • SBP: 0 valores missing restantes\n",
      "  • MAP: 0 valores missing restantes\n",
      "  • Resp: 0 valores missing restantes\n",
      "  • HospAdmTime: 0 valores missing restantes\n",
      "\n",
      "Total de valores imputados (simples): 647,891\n",
      "Missing values restantes: 5,478,673 (28.32%)\n"
     ]
    }
   ],
   "source": [
    "# Imputação simples para variáveis com baixo missing (<20%)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "X_train_simple_imputed = X_train_outliers_treated.copy()\n",
    "\n",
    "# Identificar variáveis com baixo missing (<20%)\n",
    "low_missing_vars = []\n",
    "missing_info = {}\n",
    "\n",
    "for col in X_train_simple_imputed.columns:\n",
    "    missing_pct = (X_train_simple_imputed[col].isnull().sum() / len(X_train_simple_imputed)) * 100\n",
    "    missing_info[col] = missing_pct\n",
    "    \n",
    "    if missing_pct < 20 and missing_pct > 0:\n",
    "        low_missing_vars.append(col)\n",
    "\n",
    "\n",
    "print(f\"\\nVariáveis para imputação simples:\")\n",
    "for var in low_missing_vars:\n",
    "    missing_count = X_train_simple_imputed[var].isnull().sum()\n",
    "    missing_pct = missing_info[var]\n",
    "    print(f\"  • {var}: {missing_count:,} valores ({missing_pct:.1f}%)\")\n",
    "\n",
    "\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "X_train_simple_imputed[low_missing_vars] = numeric_imputer.fit_transform(\n",
    "    X_train_simple_imputed[low_missing_vars]\n",
    ")\n",
    "\n",
    "\n",
    "# Verificar se imputação foi bem-sucedida\n",
    "print(f\"\\nVERIFICAÇÃO PÓS-IMPUTAÇÃO:\")\n",
    "for var in low_missing_vars:\n",
    "    remaining_missing = X_train_simple_imputed[var].isnull().sum()\n",
    "    print(f\"  • {var}: {remaining_missing} valores missing restantes\")\n",
    "\n",
    "total_imputed = sum(missing_info[var] * len(X_train_simple_imputed) / 100 for var in low_missing_vars)\n",
    "print(f\"\\nTotal de valores imputados (simples): {total_imputed:,.0f}\")\n",
    "\n",
    "# Resumo do missing restante\n",
    "remaining_missing = X_train_simple_imputed.isnull().sum().sum()\n",
    "total_values = X_train_simple_imputed.size\n",
    "missing_pct_remaining = (remaining_missing / total_values) * 100\n",
    "\n",
    "print(f\"Missing values restantes: {remaining_missing:,} ({missing_pct_remaining:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a48589",
   "metadata": {},
   "source": [
    "#### 4.3.2 Separando as Demais Variáveis para Imputação Avançada \n",
    "\n",
    "Para uso de técnicas mais sofisticadas como KNN Imputer ou imputação baseada em modelos para variáveis clinicamente importantes, vamos antes definir as variáveis que ainda possuem valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0004175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANÁLISE DINÂMICA DE MISSING VALUES:\n",
      "==================================================\n",
      "\n",
      "CLASSIFICAÇÃO POR ESTRATÉGIA DE IMPUTAÇÃO:\n",
      "Critério: Unit1/Unit2 = Regressão Logística | <40% = Regressão Linear | ≥40% = Híbrida\n",
      "-------------------------------------------------------------------------------------\n",
      "DBP             29.5    % ( 356,506 valores)\n",
      "Unit1           39.0    % ( 472,083 valores)\n",
      "Unit2           39.0    % ( 472,083 valores)\n",
      "Temp            65.3    % ( 789,252 valores)\n",
      "BUN             92.9    % (1,123,758 valores)\n",
      "WBC             93.4    % (1,129,584 valores)\n",
      "Platelets       93.9    % (1,135,407 valores)\n",
      "\n",
      "RESUMO DAS ESTRATÉGIAS:\n",
      "  • Regressão Logística (categóricas Unit1/Unit2): 2 variáveis\n",
      "  • Regressão Linear Simples (<40% missing numéricas): 1 variáveis\n",
      "  • Estratégia Híbrida (≥40% missing numéricas): 4 variáveis\n",
      "\n",
      "Variáveis para Regressão Logística: ['Unit1', 'Unit2']\n",
      "Variáveis para Regressão Linear: ['DBP']\n",
      "Variáveis para Estratégia Híbrida: ['Temp', 'BUN', 'WBC', 'Platelets']\n",
      "\n",
      "Variáveis preditoras selecionadas: ['Hour', 'HR', 'O2Sat', 'SBP', 'MAP', 'Resp', 'Gender', 'HospAdmTime', 'ICULOS']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_advanced_imputed = X_train_simple_imputed.copy()\n",
    "\n",
    "# ESTRATÉGIA DINÂMICA BASEADA EM MISSING ATUAL\n",
    "print(f\"\\nANÁLISE DINÂMICA DE MISSING VALUES:\")\n",
    "print(f\"=\"*50)\n",
    "\n",
    "# Identificar todas as variáveis com missing\n",
    "vars_with_missing = []\n",
    "for col in X_train_advanced_imputed.columns:\n",
    "    missing_count = X_train_advanced_imputed[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(X_train_advanced_imputed)) * 100\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        vars_with_missing.append({\n",
    "            'variavel': col,\n",
    "            'missing_count': missing_count,\n",
    "            'missing_pct': missing_pct,\n",
    "            'tipo': 'categorical' if X_train_advanced_imputed[col].dtype in ['object', 'category'] else 'numeric'\n",
    "        })\n",
    "\n",
    "# Separar por estratégia baseada em 40% de missing e tipo de variável\n",
    "logistic_regression_vars = []  # Unit1, Unit2: Regressão Logística (categóricas)\n",
    "linear_regression_vars = []    # < 40% missing: Regressão Linear simples (numéricas)\n",
    "hybrid_strategy_vars = []      # >= 40% missing: Estratégia Híbrida KNN + Regressão\n",
    "\n",
    "print(f\"\\nCLASSIFICAÇÃO POR ESTRATÉGIA DE IMPUTAÇÃO:\")\n",
    "print(f\"Critério: Unit1/Unit2 = Regressão Logística | <40% = Regressão Linear | ≥40% = Híbrida\")\n",
    "print(f\"-\" * 85)\n",
    "\n",
    "for var_info in sorted(vars_with_missing, key=lambda x: x['missing_pct']):\n",
    "    var = var_info['variavel']\n",
    "    missing_pct = var_info['missing_pct']\n",
    "    missing_count = var_info['missing_count']\n",
    "    \n",
    "    print(f\"{var:<15} {missing_pct:<8.1f}% ({missing_count:>8,} valores)\")\n",
    "    \n",
    "    # Separar Unit1 e Unit2 para regressão logística\n",
    "    if var in ['Unit1', 'Unit2']:\n",
    "        logistic_regression_vars.append(var)\n",
    "    elif missing_pct < 40:\n",
    "        linear_regression_vars.append(var)\n",
    "    else:\n",
    "        hybrid_strategy_vars.append(var)\n",
    "\n",
    "print(f\"\\nRESUMO DAS ESTRATÉGIAS:\")\n",
    "print(f\"  • Regressão Logística (categóricas Unit1/Unit2): {len(logistic_regression_vars)} variáveis\")\n",
    "print(f\"  • Regressão Linear Simples (<40% missing numéricas): {len(linear_regression_vars)} variáveis\")\n",
    "print(f\"  • Estratégia Híbrida (≥40% missing numéricas): {len(hybrid_strategy_vars)} variáveis\")\n",
    "\n",
    "print(f\"\\nVariáveis para Regressão Logística: {logistic_regression_vars}\")\n",
    "print(f\"Variáveis para Regressão Linear: {linear_regression_vars}\")\n",
    "print(f\"Variáveis para Estratégia Híbrida: {hybrid_strategy_vars}\")\n",
    "\n",
    "# Selecionar variáveis preditoras (sem missing ou baixo missing)\n",
    "all_vars_to_impute = logistic_regression_vars + linear_regression_vars + hybrid_strategy_vars\n",
    "predictor_vars = []\n",
    "for col in X_train_advanced_imputed.select_dtypes(include=[np.number]).columns:\n",
    "    missing_pct = (X_train_advanced_imputed[col].isnull().sum() / len(X_train_advanced_imputed)) * 100\n",
    "    if missing_pct == 0 or (missing_pct < 20 and col not in all_vars_to_impute):\n",
    "        predictor_vars.append(col)\n",
    "\n",
    "print(f\"\\nVariáveis preditoras selecionadas: {predictor_vars}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd99f40",
   "metadata": {},
   "source": [
    "#### 4.3.3 Regressão Logística para Categóricas\n",
    "\n",
    "Aqui aplicaremos o modelo de regressão logistica para imputação de Unit1 e Unit2. Mantendo a coerência do Dataset onde é mandatório que Unit1 + Unit2 = 1 para todas as instâncias (Paciente só pode ir para um dos tipos de UTI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29d7854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit1 missing: 472,083 valores\n",
      "Unit2 missing: 472,083 valores\n",
      "Ambas missing: 472,083 valores\n",
      "\n",
      "ETAPA 1: Regressão Logística para Unit1\n",
      "ETAPA 2: Unit2 como complemento de Unit1\n",
      "Unit1 imputado: 472,083 valores\n",
      "Unit2 imputado: 472,083 valores (complemento)\n",
      "Distribuição Unit1 imputada: 0=171,657 | 1=300,426\n",
      "Distribuição Unit2 imputada: 0=300,426 | 1=171,657\n",
      "Verificação Unit1 + Unit2 = 1: ✓ VÁLIDA\n",
      "\n",
      "VERIFICAÇÃO PÓS-IMPUTAÇÃO LOGÍSTICA:\n",
      "  • Unit1: 0 valores missing restantes\n",
      "    Distribuição final: {0.0: np.int64(546415), 1.0: np.int64(662782)}\n",
      "  • Unit2: 0 valores missing restantes\n",
      "    Distribuição final: {0.0: np.int64(662782), 1.0: np.int64(546415)}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ETAPA 1: REGRESSÃO LOGÍSTICA PARA VARIÁVEIS CATEGÓRICAS (Unit1, Unit2)\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Identificar valores missing\n",
    "unit1_missing_mask = X_train_advanced_imputed['Unit1'].isnull()\n",
    "unit2_missing_mask = X_train_advanced_imputed['Unit2'].isnull()\n",
    "both_missing_mask = unit1_missing_mask & unit2_missing_mask\n",
    "\n",
    "unit1_missing_count = unit1_missing_mask.sum()\n",
    "unit2_missing_count = unit2_missing_mask.sum()\n",
    "both_missing_count = both_missing_mask.sum()\n",
    "\n",
    "print(f\"Unit1 missing: {unit1_missing_count:,} valores\")\n",
    "print(f\"Unit2 missing: {unit2_missing_count:,} valores\") \n",
    "print(f\"Ambas missing: {both_missing_count:,} valores\")\n",
    "\n",
    "print(f\"\\nETAPA 1: Regressão Logística para Unit1\")\n",
    "\n",
    "# Preparar dados para treino (onde Unit1 não é missing)\n",
    "complete_mask = ~X_train_advanced_imputed['Unit1'].isnull()\n",
    "training_size = min(100000, complete_mask.sum())\n",
    "\n",
    "training_indices = np.random.choice(\n",
    "    X_train_advanced_imputed[complete_mask].index,\n",
    "    size=training_size,\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "# Features numéricas para predição (excluir Unit1/Unit2)\n",
    "numeric_predictors = [col for col in predictor_vars if col not in logistic_regression_vars]\n",
    "\n",
    "# Treinar modelo logístico para Unit1\n",
    "X_features = X_train_advanced_imputed.loc[training_indices, numeric_predictors]\n",
    "y_target = X_train_advanced_imputed.loc[training_indices, 'Unit1']\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logistic_model.fit(X_features, y_target)\n",
    "\n",
    "# Prever Unit1 para registros missing\n",
    "X_missing_features = X_train_advanced_imputed.loc[both_missing_mask, numeric_predictors]\n",
    "predicted_unit1_proba = logistic_model.predict_proba(X_missing_features)[:, 1]\n",
    "predicted_unit1 = (predicted_unit1_proba > 0.5).astype(int)\n",
    "\n",
    "print(f\"ETAPA 2: Unit2 como complemento de Unit1\")\n",
    "# Unit2 como complemento lógico de Unit1\n",
    "predicted_unit2 = 1 - predicted_unit1\n",
    "\n",
    "# Aplicar imputações\n",
    "X_train_advanced_imputed.loc[both_missing_mask, 'Unit1'] = predicted_unit1\n",
    "X_train_advanced_imputed.loc[both_missing_mask, 'Unit2'] = predicted_unit2\n",
    "\n",
    "print(f\"Unit1 imputado: {both_missing_count:,} valores\")\n",
    "print(f\"Unit2 imputado: {both_missing_count:,} valores (complemento)\")\n",
    "\n",
    "# Estatísticas da imputação\n",
    "unit1_0_count = (predicted_unit1 == 0).sum()\n",
    "unit1_1_count = (predicted_unit1 == 1).sum()\n",
    "print(f\"Distribuição Unit1 imputada: 0={unit1_0_count:,} | 1={unit1_1_count:,}\")\n",
    "print(f\"Distribuição Unit2 imputada: 0={unit1_1_count:,} | 1={unit1_0_count:,}\")\n",
    "\n",
    "# Verificar relação complementar\n",
    "unit1_final = X_train_advanced_imputed.loc[both_missing_mask, 'Unit1']\n",
    "unit2_final = X_train_advanced_imputed.loc[both_missing_mask, 'Unit2']\n",
    "sum_check = (unit1_final + unit2_final == 1).all()\n",
    "print(f\"Verificação Unit1 + Unit2 = 1: {'✓ VÁLIDA' if sum_check else '✗ INVÁLIDA'}\")\n",
    "    \n",
    "# Verificação pós-imputação logística\n",
    "print(f\"\\nVERIFICAÇÃO PÓS-IMPUTAÇÃO LOGÍSTICA:\")\n",
    "for var in logistic_regression_vars:\n",
    "    if var in X_train_advanced_imputed.columns:\n",
    "        remaining_missing = X_train_advanced_imputed[var].isnull().sum()\n",
    "        print(f\"  • {var}: {remaining_missing} valores missing restantes\")\n",
    "        \n",
    "        # Distribuição final\n",
    "        if remaining_missing == 0:\n",
    "            value_counts = X_train_advanced_imputed[var].value_counts().sort_index()\n",
    "            print(f\"    Distribuição final: {dict(value_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8198a75",
   "metadata": {},
   "source": [
    "#### 4.3.4 Regressão Linear para <40% de Missing\n",
    "Aplicar regressão linear para variáveis numéricas com <40% de missing (DBP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f67684c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETAPA 2: REGRESSÃO LINEAR SIMPLES (< 40% missing)\n",
      "\n",
      "\n",
      "---------- IMPUTANDO DBP (Regressão Simples) ----------\n",
      "Missing: 356,506 valores (29.5%)\n",
      "Regressão aplicada: 356,506 valores imputados\n",
      "Valores restantes missing: 0\n",
      "Range imputado: [2.68, 260.61]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ETAPA 2: REGRESSÃO LINEAR SIMPLES (< 40% missing)\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(f\"ETAPA 2: REGRESSÃO LINEAR SIMPLES (< 40% missing)\\n\")\n",
    "\n",
    "for target_var in linear_regression_vars:\n",
    "    print(f\"\\n---------- IMPUTANDO {target_var} (Regressão Simples) ----------\")\n",
    "    \n",
    "    # Identificar valores missing\n",
    "    missing_mask = X_train_advanced_imputed[target_var].isnull()\n",
    "    total_missing = missing_mask.sum()\n",
    "    missing_pct = (total_missing / len(X_train_advanced_imputed)) * 100\n",
    "    print(f\"Missing: {total_missing:,} valores ({missing_pct:.1f}%)\")\n",
    "\n",
    "    # Preparar dados para regressão\n",
    "    complete_mask = ~X_train_advanced_imputed[target_var].isnull()\n",
    "    training_size = min(100000, complete_mask.sum())  # Até 100k para treino\n",
    "    \n",
    "    training_indices = np.random.choice(\n",
    "        X_train_advanced_imputed[complete_mask].index,\n",
    "        size=training_size,\n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    # Features e target para treino\n",
    "    X_features = X_train_advanced_imputed.loc[training_indices, predictor_vars]\n",
    "    y_target = X_train_advanced_imputed.loc[training_indices, target_var]\n",
    "    \n",
    "    # Treinar modelo de regressão\n",
    "    reg_model = LinearRegression()\n",
    "    reg_model.fit(X_features, y_target)\n",
    "    \n",
    "    # Prever todos os valores missing\n",
    "    X_missing_features = X_train_advanced_imputed.loc[missing_mask, predictor_vars]\n",
    "    predicted_values = reg_model.predict(X_missing_features)\n",
    "    \n",
    "    # Aplicar imputação\n",
    "    X_train_advanced_imputed.loc[missing_mask, target_var] = predicted_values\n",
    "    \n",
    "    # Verificar resultado\n",
    "    final_missing = X_train_advanced_imputed[target_var].isnull().sum()\n",
    "    print(f\"Regressão aplicada: {total_missing:,} valores imputados\")\n",
    "    print(f\"Valores restantes missing: {final_missing}\")\n",
    "    print(f\"Range imputado: [{predicted_values.min():.2f}, {predicted_values.max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55862465",
   "metadata": {},
   "source": [
    "#### 4.3.5 KNNImputer + Regressão Linear para >=40% de Missing\n",
    "Aplicação do KNNImputer com 3 vizinhos para amostra de ~5% dos valores da coluna e Regressão Linear para os outros ~95%.\n",
    "\n",
    "Esse código abaixo demora em torno de 5-6min para executar. Tomar ciência disso antes rodar a célula de código abaixo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d936d4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== IMPUTANDO Temp ==========\n",
      "Total de valores missing: 789,252\n",
      "\n",
      "ETAPA 1 - KNN Imputer (39,462 valores - 5%)\n",
      "  KNN aplicado com sucesso: 39,462 valores\n",
      "\n",
      "ETAPA 2 - Regressão Linear (749,790 valores - 95%)\n",
      "  Regressão aplicada com sucesso: 749,790 valores\n",
      "  Valores missing restantes: 0\n",
      "  Imputação híbrida completa para Temp!\n",
      "\n",
      "========== IMPUTANDO BUN ==========\n",
      "Total de valores missing: 1,123,758\n",
      "\n",
      "ETAPA 1 - KNN Imputer (50,000 valores - 5%)\n",
      "  KNN aplicado com sucesso: 50,000 valores\n",
      "\n",
      "ETAPA 2 - Regressão Linear (1,073,758 valores - 95%)\n",
      "  Regressão aplicada com sucesso: 1,073,758 valores\n",
      "  Valores missing restantes: 0\n",
      "  Imputação híbrida completa para BUN!\n",
      "\n",
      "========== IMPUTANDO WBC ==========\n",
      "Total de valores missing: 1,129,584\n",
      "\n",
      "ETAPA 1 - KNN Imputer (50,000 valores - 5%)\n",
      "  KNN aplicado com sucesso: 50,000 valores\n",
      "\n",
      "ETAPA 2 - Regressão Linear (1,085,407 valores - 95%)\n",
      "  Regressão aplicada com sucesso: 1,085,407 valores\n",
      "  Valores missing restantes: 0\n",
      "  Imputação híbrida completa para Platelets!\n",
      "Imputação Concluída!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ETAPA 3: ESTRATÉGIA HÍBRIDA (≥ 40% missing)  \n",
    "# =============================================================================\n",
    "\n",
    "# Imputação avançada híbrida: KNN + Regressão Linear\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Implementar estratégia híbrida para cada variável\n",
    "for target_var in hybrid_strategy_vars:\n",
    "    print(f\"\\n========== IMPUTANDO {target_var} ==========\")\n",
    "    \n",
    "    # Identificar valores missing\n",
    "    missing_mask = X_train_advanced_imputed[target_var].isnull()\n",
    "    total_missing = missing_mask.sum()\n",
    "    print(f\"Total de valores missing: {total_missing:,}\")\n",
    "    \n",
    "    # ETAPA 1: Imputação com KNN (5% dos missing values - otimizado)\n",
    "    knn_sample_size = min(50000, int(total_missing * 0.05))  # Máximo 50k valores\n",
    "    print(f\"\\nETAPA 1 - KNN Imputer ({knn_sample_size:,} valores - 5%)\")\n",
    "    \n",
    "    # Selecionar amostra aleatória dos índices missing para KNN\n",
    "    missing_indices = X_train_advanced_imputed[missing_mask].index\n",
    "    knn_indices = np.random.choice(missing_indices, size=knn_sample_size, replace=False)\n",
    "    \n",
    "    # Preparar subset para KNN (incluir valores não-missing para treino)\n",
    "    mask_not_missing = ~X_train_advanced_imputed[target_var].isnull()\n",
    "    knn_training_size = min(10000, mask_not_missing.sum())  # Reduzir para 10k treino\n",
    "    training_indices = np.random.choice(\n",
    "        X_train_advanced_imputed[mask_not_missing].index, \n",
    "        size=knn_training_size, \n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    # Combinar índices para KNN: treino + amostra para imputação\n",
    "    knn_all_indices = np.concatenate([training_indices, knn_indices])\n",
    "    knn_subset = X_train_advanced_imputed.loc[knn_all_indices, predictor_vars + [target_var]].copy()\n",
    "    \n",
    "    # Aplicar KNN apenas no subset\n",
    "    knn_imputer = KNNImputer(n_neighbors=3, weights='uniform')\n",
    "    knn_imputed = knn_imputer.fit_transform(knn_subset)\n",
    "    \n",
    "    # Extrair valores imputados para a variável alvo\n",
    "    target_col_idx = list(knn_subset.columns).index(target_var)\n",
    "    knn_imputed_values = knn_imputed[-knn_sample_size:, target_col_idx]\n",
    "    \n",
    "    # Atualizar valores no dataset\n",
    "    X_train_advanced_imputed.loc[knn_indices, target_var] = knn_imputed_values\n",
    "    print(f\"  KNN aplicado com sucesso: {knn_sample_size:,} valores\")\n",
    "    \n",
    "    # ETAPA 2: Imputação com Regressão Linear (95% restante)\n",
    "    remaining_missing_mask = X_train_advanced_imputed[target_var].isnull()\n",
    "    remaining_missing_count = remaining_missing_mask.sum()\n",
    "    print(f\"\\nETAPA 2 - Regressão Linear ({remaining_missing_count:,} valores - 95%)\")\n",
    "    \n",
    "    # Preparar dados para regressão (usar todos os dados completos)\n",
    "    complete_mask = ~X_train_advanced_imputed[target_var].isnull()\n",
    "    reg_training_size = min(50000, complete_mask.sum())\n",
    "    \n",
    "    reg_training_indices = np.random.choice(\n",
    "        X_train_advanced_imputed[complete_mask].index,\n",
    "        size=reg_training_size,\n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    # Features e target para treino\n",
    "    X_features = X_train_advanced_imputed.loc[reg_training_indices, predictor_vars]\n",
    "    y_target = X_train_advanced_imputed.loc[reg_training_indices, target_var]\n",
    "    \n",
    "    # Treinar modelo de regressão\n",
    "    reg_model = LinearRegression()\n",
    "    reg_model.fit(X_features, y_target)\n",
    "    \n",
    "    # Prever valores restantes\n",
    "    X_missing_features = X_train_advanced_imputed.loc[remaining_missing_mask, predictor_vars]\n",
    "    predicted_values = reg_model.predict(X_missing_features)\n",
    "    \n",
    "    X_train_advanced_imputed.loc[remaining_missing_mask, target_var] = predicted_values\n",
    "    print(f\"  Regressão aplicada com sucesso: {remaining_missing_count:,} valores\")\n",
    "            \n",
    "    \n",
    "    # Verificar se imputação foi completa\n",
    "    final_missing = X_train_advanced_imputed[target_var].isnull().sum()\n",
    "    print(f\"  Valores missing restantes: {final_missing}\")\n",
    "    print(f\"  Imputação híbrida completa para {target_var}!\")\n",
    "\n",
    "print(\"Imputação Concluída!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f67ae7",
   "metadata": {},
   "source": [
    "#### 4.3.6 Aplicando os Limites Pós-Imputação com 2 modelos\n",
    "\n",
    "Definimos o range de valores segundo a análise feita na seção 4.2 para não permitir que nenhum outlier imputado ultrapasse o intervalo observado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8deb201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "APLICAÇÃO DE CAPS PÓS-IMPUTAÇÃO\n",
      "============================================================\n",
      "  • Temp: Nenhuma correção necessária\n",
      "  • BUN: Nenhuma correção necessária\n",
      "  • WBC: Nenhuma correção necessária\n",
      "  • Platelets: Nenhuma correção necessária\n",
      "  • DBP: Nenhuma correção necessária\n",
      "\n",
      "Total de caps aplicados: 0\n",
      "\n",
      "RANGES APÓS APLICAÇÃO DE CAPS:\n",
      "  • Temp: 28.00 - 42.00\n",
      "  • BUN: 1.00 - 268.00\n",
      "  • WBC: 0.10 - 440.00\n",
      "  • Platelets: 2.00 - 2322.00\n",
      "  • DBP: 20.00 - 300.00\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APLICAÇÃO DE CAPS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"APLICAÇÃO DE CAPS PÓS-IMPUTAÇÃO\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# Definir ranges tratados do outliers_summary (Seção 4.2)\n",
    "caps_ranges = {\n",
    "    'Temp': {'min': 28.00, 'max': 42.00},\n",
    "    'BUN': {'min': 1.00, 'max': 268.00},\n",
    "    'WBC': {'min': 0.10, 'max': 440.00},\n",
    "    'Platelets': {'min': 2.00, 'max': 2322.00},\n",
    "    'DBP': {'min': 20.00, 'max': 300.00}\n",
    "}\n",
    "\n",
    "# Aplicar caps nas variáveis imputadas\n",
    "total_caps_applied = 0\n",
    "\n",
    "for var in caps_ranges.keys():\n",
    "    if var in X_train_advanced_imputed.columns:\n",
    "        min_cap = caps_ranges[var]['min']\n",
    "        max_cap = caps_ranges[var]['max']\n",
    "        \n",
    "        # Contabilizar valores fora dos limites antes da correção\n",
    "        below_min = (X_train_advanced_imputed[var] < min_cap).sum()\n",
    "        above_max = (X_train_advanced_imputed[var] > max_cap).sum()\n",
    "        total_corrections = below_min + above_max\n",
    "        \n",
    "        if total_corrections > 0:\n",
    "            print(f\"  • {var}: {below_min:,} valores < {min_cap}, {above_max:,} valores > {max_cap}\")\n",
    "            \n",
    "            # Aplicar caps\n",
    "            X_train_advanced_imputed[var] = X_train_advanced_imputed[var].clip(\n",
    "                lower=min_cap, \n",
    "                upper=max_cap\n",
    "            )\n",
    "            \n",
    "            total_caps_applied += total_corrections\n",
    "        else:\n",
    "            print(f\"  • {var}: Nenhuma correção necessária\")\n",
    "\n",
    "print(f\"\\nTotal de caps aplicados: {total_caps_applied:,}\")\n",
    "\n",
    "# Verificar ranges após caps\n",
    "print(f\"\\nRANGES APÓS APLICAÇÃO DE CAPS:\")\n",
    "for var in caps_ranges.keys():\n",
    "    if var in X_train_advanced_imputed.columns:\n",
    "        min_val = X_train_advanced_imputed[var].min()\n",
    "        max_val = X_train_advanced_imputed[var].max()\n",
    "        print(f\"  • {var}: {min_val:.2f} - {max_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf38c5",
   "metadata": {},
   "source": [
    "#### 4.3.7 Verificação Final Pós-Imputação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3782dd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFICAÇÃO FINAL PÓS-IMPUTAÇÃO\n",
      "============================================================\n",
      "\n",
      "VALIDAÇÃO DE RANGES PÓS-IMPUTAÇÃO:\n",
      "  • DBP: min=20.00, max=300.00, mean=62.67\n",
      "  • Temp: min=28.00, max=42.00, mean=36.96\n",
      "  • BUN: min=1.00, max=268.00, mean=24.07\n",
      "  • WBC: min=0.10, max=440.00, mean=11.35\n",
      "  • Platelets: min=2.00, max=2322.00, mean=196.31\n",
      "  • Unit1: min=0.00, max=1.00, mean=0.55\n",
      "  • Unit2: min=0.00, max=1.00, mean=0.45\n",
      "\n",
      "VERIFICAÇÃO GERAL DE MISSING:\n",
      "NENHUMA VARIÁVEL COM MISSING RESTANTE!\n",
      "\n",
      "Dataset após imputações: (1209197, 16)\n",
      "Missing values finais: 0 (0.000%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VERIFICAÇÃO FINAL PÓS-IMPUTAÇÃO\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"VERIFICAÇÃO FINAL PÓS-IMPUTAÇÃO\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# Verificar todas as variáveis processadas\n",
    "all_processed_vars = linear_regression_vars + hybrid_strategy_vars + logistic_regression_vars\n",
    "\n",
    "print(f\"\\nVALIDAÇÃO DE RANGES PÓS-IMPUTAÇÃO:\")\n",
    "\n",
    "# Variáveis processadas\n",
    "numeric_processed = linear_regression_vars + hybrid_strategy_vars + logistic_regression_vars\n",
    "for var in numeric_processed:\n",
    "    if var in X_train_advanced_imputed.columns:\n",
    "        min_val = X_train_advanced_imputed[var].min()\n",
    "        max_val = X_train_advanced_imputed[var].max()\n",
    "        mean_val = X_train_advanced_imputed[var].mean()\n",
    "        print(f\"  • {var}: min={min_val:.2f}, max={max_val:.2f}, mean={mean_val:.2f}\")\n",
    "\n",
    "\n",
    "# Verificar se ainda há variáveis com missing\n",
    "print(f\"\\nVERIFICAÇÃO GERAL DE MISSING:\")\n",
    "total_vars_with_missing = 0\n",
    "for col in X_train_advanced_imputed.columns:\n",
    "    missing_count = X_train_advanced_imputed[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        missing_pct = (missing_count / len(X_train_advanced_imputed)) * 100\n",
    "        print(f\"  • {col}: {missing_count:,} valores ({missing_pct:.1f}%)\")\n",
    "        total_vars_with_missing += 1\n",
    "\n",
    "if total_vars_with_missing == 0:\n",
    "    print(\"NENHUMA VARIÁVEL COM MISSING RESTANTE!\")\n",
    "\n",
    "\n",
    "print(f\"\\nDataset após imputações: {X_train_advanced_imputed.shape}\")\n",
    "\n",
    "# Status final do missing\n",
    "final_missing = X_train_advanced_imputed.isnull().sum().sum()\n",
    "total_values = X_train_advanced_imputed.size\n",
    "final_missing_pct = (final_missing / total_values) * 100\n",
    "\n",
    "print(f\"Missing values finais: {final_missing:,} ({final_missing_pct:.3f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc4c04",
   "metadata": {},
   "source": [
    "### 4.4 Validação da Qualidade Pós-Limpeza\n",
    "\n",
    "Aplicação de regressão logística para imputação de variáveis categóricas Unit1 e Unit2, mantendo a relação complementar Unit1 + Unit2 = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ff98118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDAÇÃO DA QUALIDADE PÓS-LIMPEZA:\n",
      "1. VERIFICAÇÃO DE COMPLETUDE:\n",
      "   • Total de valores missing: 0\n",
      "   • Percentual de missing: 0.0000%\n",
      "   • Completude do dataset: 100.0000%\n",
      "\n",
      "3. VERIFICAÇÃO DE DISTRIBUIÇÕES:\n",
      "   • Variáveis numéricas analisadas: 16\n",
      "   • Estatísticas das principais variáveis:\n",
      "     Variável     Mean     Median   Std      Min      Max      Skew  \n",
      "     ------------------------------------------------------------\n",
      "     Hour         25.8     20.0     29.1     0.0      335.0    4.08  \n",
      "     HR           84.5     83.5     16.7     20.0     250.0    0.46  \n",
      "     O2Sat        97.3     98.0     2.8      20.0     100.0    -4.34 \n",
      "     Temp         37.0     36.9     0.5      28.0     42.0     -0.26 \n",
      "     SBP          123.4    121.0    21.8     20.0     300.0    0.64  \n",
      "\n",
      "4. VERIFICAÇÃO DE INTEGRIDADE:\n",
      "   • Shape do X_train: (1209197, 16)\n",
      "   • Shape do y_train: (1209197,)\n",
      "   • Índices alinhados: True\n",
      "   • Estatísticas das principais variáveis:\n",
      "     Variável     Mean     Median   Std      Min      Max      Skew  \n",
      "     ------------------------------------------------------------\n",
      "     Hour         25.8     20.0     29.1     0.0      335.0    4.08  \n",
      "     HR           84.5     83.5     16.7     20.0     250.0    0.46  \n",
      "     O2Sat        97.3     98.0     2.8      20.0     100.0    -4.34 \n",
      "     Temp         37.0     36.9     0.5      28.0     42.0     -0.26 \n",
      "     SBP          123.4    121.0    21.8     20.0     300.0    0.64  \n",
      "\n",
      "4. VERIFICAÇÃO DE INTEGRIDADE:\n",
      "   • Shape do X_train: (1209197, 16)\n",
      "   • Shape do y_train: (1209197,)\n",
      "   • Índices alinhados: True\n"
     ]
    }
   ],
   "source": [
    "# Validação da qualidade dos dados após todas as etapas de limpeza\n",
    "\n",
    "print(\"VALIDAÇÃO DA QUALIDADE PÓS-LIMPEZA:\")\n",
    "\n",
    "X_train_final_cleaned = X_train_advanced_imputed.copy()\n",
    "y_train_final_cleaned = y_train_cleaned.copy()\n",
    "\n",
    "# 1. Verificação de Completude\n",
    "print(\"1. VERIFICAÇÃO DE COMPLETUDE:\")\n",
    "total_missing = X_train_final_cleaned.isnull().sum().sum()\n",
    "total_values = X_train_final_cleaned.size\n",
    "missing_pct = (total_missing / total_values) * 100\n",
    "\n",
    "print(f\"   • Total de valores missing: {total_missing:,}\")\n",
    "print(f\"   • Percentual de missing: {missing_pct:.4f}%\")\n",
    "print(f\"   • Completude do dataset: {100-missing_pct:.4f}%\")\n",
    "\n",
    "if total_missing > 0:\n",
    "    print(\"   • Variáveis com missing restante:\")\n",
    "    for col in X_train_final_cleaned.columns:\n",
    "        missing_count = X_train_final_cleaned[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_pct_col = (missing_count / len(X_train_final_cleaned)) * 100\n",
    "            print(f\"     - {col}: {missing_count:,} ({missing_pct_col:.2f}%)\")\n",
    "\n",
    "\n",
    "# 3. Verificação de Distribuições\n",
    "print(f\"\\n3. VERIFICAÇÃO DE DISTRIBUIÇÕES:\")\n",
    "\n",
    "# Análise estatística básica para variáveis numéricas\n",
    "numeric_cols = X_train_final_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"   • Variáveis numéricas analisadas: {len(numeric_cols)}\")\n",
    "\n",
    "distributions_summary = {}\n",
    "for col in numeric_cols[:5]:  # Primeiras 5 variáveis para exemplo\n",
    "    data = X_train_final_cleaned[col].dropna()\n",
    "    if len(data) > 0:\n",
    "        distributions_summary[col] = {\n",
    "            'mean': data.mean(),\n",
    "            'median': data.median(),  \n",
    "            'std': data.std(),\n",
    "            'min': data.min(),\n",
    "            'max': data.max(),\n",
    "            'skewness': data.skew()\n",
    "        }\n",
    "\n",
    "print(f\"   • Estatísticas das principais variáveis:\")\n",
    "print(f\"     {'Variável':<12} {'Mean':<8} {'Median':<8} {'Std':<8} {'Min':<8} {'Max':<8} {'Skew':<6}\")\n",
    "print(\"     \" + \"-\"*60)\n",
    "\n",
    "for var, stats in distributions_summary.items():\n",
    "    print(f\"     {var:<12} {stats['mean']:<8.1f} {stats['median']:<8.1f} {stats['std']:<8.1f} {stats['min']:<8.1f} {stats['max']:<8.1f} {stats['skewness']:<6.2f}\")\n",
    "\n",
    "# 4. Verificação de Integridade Referencial\n",
    "print(f\"\\n4. VERIFICAÇÃO DE INTEGRIDADE:\")\n",
    "print(f\"   • Shape do X_train: {X_train_final_cleaned.shape}\")\n",
    "print(f\"   • Shape do y_train: {y_train_final_cleaned.shape}\")\n",
    "print(f\"   • Índices alinhados: {X_train_final_cleaned.index.equals(y_train_final_cleaned.index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc3e40",
   "metadata": {},
   "source": [
    "### 4.5 Síntese da Tarefa 2: Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444acdb6",
   "metadata": {},
   "source": [
    "**Resumo completo de todas as decisões e transformações aplicadas na limpeza dos dados:**\n",
    "\n",
    "**1. DETECÇÃO E REMOÇÃO DE DUPLICATAS**\n",
    "\n",
    "**Estratégia Aplicada:**\n",
    "- Remoção de duplicatas exatas baseada em todas as variáveis de features\n",
    "- Método: `drop_duplicates()` padrão (manter primeira ocorrência)\n",
    "\n",
    "**Resultados:**\n",
    "- **Duplicatas removidas**: 32,571 registros (2.6% do dataset)\n",
    "- **Dataset reduzido**: 1,241,768 → 1,209,197 registros\n",
    "- **Proporção de classes**: Mantida praticamente inalterada após remoção\n",
    "\n",
    "**Justificativa:**\n",
    "- Duplicatas exatas em dados temporais de UTI são artefatos de coleta/processamento\n",
    "- Não agregam informação preditiva e podem causar data leakage\n",
    "- Preservação das proporções das classes confirma remoção adequada\n",
    "\n",
    "\n",
    "**2. TRATAMENTO DE OUTLIERS**\n",
    "\n",
    "**Estratégia Aplicada:**\n",
    "- **Capping clínico** baseado em ranges médicos estabelecidos\n",
    "- **Preservação de outliers relevantes** para detecção de sepsis\n",
    "- Aplicado apenas em variáveis com limites fisiológicos claros\n",
    "\n",
    "**Variáveis Tratadas:**\n",
    "- **HR**: 20-250 bpm (batimentos cardíacos)\n",
    "- **Temp**: 28-42°C (temperatura corporal)\n",
    "- **Hour/ICULOS**: 0-336h (14 dias máximo na UTI)\n",
    "- **HospAdmTime**: 0-24h (tempo de admissão hospitalar)\n",
    "\n",
    "**Justificativa:**\n",
    "- Outliers em sepsis são muitas vezes **informativos** (sinais de deterioração)\n",
    "- Limite para variáveis clínicas \n",
    "    - Foi estipulado por uma pesquisa de casos extremos considerando possível imprecisões na coleta e medição\n",
    "    - Ainda mantém variabilidade apesar da redução\n",
    "- Quanto as variáveis de tempo\n",
    "    - O limite inferior foi estipulado para 0, visto que não faz sentido ter valores negativos para essas variáveis\n",
    "    - Limite superior foi mantido o mesmo do dataset original\n",
    "\n",
    "\n",
    "\n",
    "**3. ESTRATÉGIAS DE IMPUTAÇÃO**\n",
    "\n",
    "**3.1 Imputação Simples (Missing <20%)**\n",
    "- **Método**: Mediana para variáveis numéricas\n",
    "- **Variáveis**: 6 variáveis com baixo percentual de missing\n",
    "- **Total imputado**: ~647,891 valores\n",
    "- **Justificativa**: Missing baixo permite imputação simples sem perda significativa de informação\n",
    "\n",
    "**3.2 Imputação por Regressão Logística (Categóricas)**\n",
    "- **Variáveis**: Unit1 e Unit2 (tipos de UTI)  \n",
    "- **Método**: Modelo logístico com features numéricas como preditores\n",
    "- **Restrição**: Manter Unit1 + Unit2 = 1 (paciente em apenas um tipo de UTI)\n",
    "- **Justificativa**: Relação complementar deve ser preservada por coerência lógica\n",
    "\n",
    "**3.3 Imputação por Regressão Linear (<40% Missing)**\n",
    "- **Variáveis**: DBP (pressão diastólica)\n",
    "- **Método**: Regressão linear com amostragem de 100k registros para treino\n",
    "- **Justificativa**: Sinal vital com padrões previsíveis baseados em outras variáveis\n",
    "\n",
    "**3.4 Estratégia Híbrida (≥40% Missing)**\n",
    "- **Variáveis**: Temp, BUN, WBC, Platelets (alta relevância clínica)\n",
    "- **Método**: KNN (5% dos missing) + Regressão Linear (95% restante)\n",
    "- **Justificativa**: Variáveis críticas para sepsis que requerem imputação sofisticada\n",
    "- **Otimização**: Sampling para viabilizar processamento em grande escala\n",
    "\n",
    "\n",
    "\n",
    "**4. CONTROLE DE QUALIDADE PÓS-IMPUTAÇÃO**\n",
    "\n",
    "**Aplicação de Caps:**\n",
    "- Limites baseados nos ranges observados pós-tratamento de outliers\n",
    "- Evita valores imputados fora da realidade clínica\n",
    "- Variáveis: Temp, BUN, WBC, Platelets, DBP\n",
    "\n",
    "**Validação Final:**\n",
    "- **Completude**: 100% (zero missing values restantes)\n",
    "- **Consistência lógica**: Verificação de ranges fisiológicos\n",
    "- **Integridade referencial**: Alinhamento entre X_train e y_train\n",
    "- **Distribuições**: Preservação de características estatísticas essenciais\n",
    "\n",
    "\n",
    "\n",
    "**5. JUSTIFICATIVAS GERAIS DAS DECISÕES**\n",
    "\n",
    "**Orientação Clínica:**\n",
    "- Todas as decisões baseadas em **conhecimento médico** sobre sepsis\n",
    "- Preservação de **variáveis críticas** mesmo com alto missing\n",
    "- Manutenção de **patterns clínicos** relevantes para diagnóstico\n",
    "\n",
    "**Orientação Técnica:**\n",
    "- **Escalabilidade**: Sampling para viabilizar imputação em dataset de 1.2M registros\n",
    "- **Eficiência**: Estratégias diferenciadas por grau de complexidade necessária\n",
    "- **Robustez**: Múltiplas validações de qualidade e integridade\n",
    "\n",
    "**Orientação para Modelagem:**\n",
    "- **Balanceamento**: Preservação cuidadosa das proporções das classes\n",
    "- **Qualidade**: Dataset final sem missing e com consistência lógica\n",
    "- **Preparação**: Dados prontos para feature engineering e normalização\n",
    "\n",
    "\n",
    "**RESULTADO FINAL:**\n",
    "- **Dataset limpo**: 1,209,197 × 16 variáveis (100% completo)\n",
    "- **Qualidade**: Dados consistentes e clinicamente válidos\n",
    "- **Pronto para**: Feature Engineering (Tarefa 3) e Modelagem (Fase 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6547ba43",
   "metadata": {},
   "source": [
    "## 5. TAREFA 3: Construção dos Dados (Feature Engineering)\n",
    "\n",
    "**Objetivo:** Criar novas variáveis ou atributos derivados dos dados existentes que possam melhorar o poder preditivo do modelo.\n",
    "\n",
    "**Estratégias de construção:**\n",
    "* Ratios clínicos baseados em literatura médica\n",
    "* Features temporais derivadas de Hour/ICULOS\n",
    "* Interações entre variáveis relacionadas\n",
    "* Transformações para normalizar distribuições"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b47e5",
   "metadata": {},
   "source": [
    "### 5.1 Criação de Ratios Clínicos\n",
    "\n",
    "Desenvolvimento de índices e ratios clinicamente estabelecidos para detecção de sepsis (ex: razão neutrófilos/linfócitos, índices de choque)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f3254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INICIANDO CRIAÇÃO DE RATIOS CLÍNICOS...\n",
      "Dataset base para Feature Engineering: (1209197, 16)\n",
      "\n",
      "1. RATIOS DE PRESSÃO ARTERIAL:\n",
      "  • Shock_Index (HR/SBP) criado: Range [0.142, 6.741]\n",
      "  • Pulse_Pressure (SBP-DBP) criado: Range [-235.6, 237.5]\n",
      "  • MAP_DBP_Ratio criado: Range [0.216, 8.739]\n",
      "\n",
      "2. RATIOS RESPIRATÓRIOS:\n",
      "  • Resp_O2Sat_Ratio criado: Range [0.0100, 1.7391]\n",
      "\n",
      "3. RATIOS METABÓLICOS:\n",
      "\n",
      "4. RATIOS HEMATOLÓGICOS:\n",
      "  • WBC_Platelets_Ratio criado: Range [0.0003, 22.0000]\n",
      "\n",
      "5. RATIOS DE ELETRÓLITOS:\n",
      "\n",
      "VALIDAÇÃO DOS RATIOS CRIADOS:\n",
      "Total de ratios clínicos criados: 5\n",
      "  ✓ Shock_Index: Sem valores NaN\n",
      "  ✓ Pulse_Pressure: Sem valores NaN\n",
      "  ✓ MAP_DBP_Ratio: Sem valores NaN\n",
      "  ✓ Resp_O2Sat_Ratio: Sem valores NaN\n",
      "  ✓ WBC_Platelets_Ratio: Sem valores NaN\n",
      "\n",
      "Dataset após ratios clínicos: (1209197, 21)\n",
      "Novas features criadas: 5\n"
     ]
    }
   ],
   "source": [
    "# PLACEHOLDER: SEÇÃO REMOVIDA PARA SIMPLIFICAÇÃO\n",
    "# Feature engineering focado apenas em features temporais (Seção 5.2)\n",
    "print(\"Seção 5.1 removida - focando apenas em features temporais de alta qualidade\")\n",
    "\n",
    "# Dataset base para feature engineering\n",
    "X_train_fe = X_train_final_cleaned.copy()\n",
    "y_train_fe = y_train_final_cleaned.copy()\n",
    "\n",
    "print(f\"Dataset base para Feature Engineering: {X_train_fe.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf11848",
   "metadata": {},
   "source": [
    "### 5.2 Features Temporais\n",
    "\n",
    "Criação de variáveis derivadas das informações temporais para capturar padrões de risco ao longo do tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c71a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIAÇÃO DE FEATURES TEMPORAIS COM VALIDAÇÃO POR MÉTRICAS\n",
    "print(\"\\nINICIANDO CRIAÇÃO DE FEATURES TEMPORAIS COM VALIDAÇÃO...\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# =============================================================================\n",
    "# 1. FUNÇÃO DE VALIDAÇÃO POR MÉTRICAS\n",
    "# =============================================================================\n",
    "\n",
    "def validate_feature(X_original, X_with_new_feature, y, feature_name, cv_folds=3):\n",
    "    \"\"\"\n",
    "    Valida se uma nova feature melhora as métricas do modelo\n",
    "    \"\"\"\n",
    "    print(f\"\\nVALIDANDO FEATURE: {feature_name}\")\n",
    "    \n",
    "    # Modelo simples para validação rápida\n",
    "    rf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Métricas com dataset original\n",
    "    precision_orig = cross_val_score(rf, X_original, y, cv=cv, scoring='precision', n_jobs=-1)\n",
    "    recall_orig = cross_val_score(rf, X_original, y, cv=cv, scoring='recall', n_jobs=-1)\n",
    "    f1_orig = cross_val_score(rf, X_original, y, cv=cv, scoring='f1', n_jobs=-1)\n",
    "    \n",
    "    # Métricas com nova feature\n",
    "    precision_new = cross_val_score(rf, X_with_new_feature, y, cv=cv, scoring='precision', n_jobs=-1)\n",
    "    recall_new = cross_val_score(rf, X_with_new_feature, y, cv=cv, scoring='recall', n_jobs=-1)\n",
    "    f1_new = cross_val_score(rf, X_with_new_feature, y, cv=cv, scoring='f1', n_jobs=-1)\n",
    "    \n",
    "    # Calcular melhorias\n",
    "    precision_improvement = precision_new.mean() - precision_orig.mean()\n",
    "    recall_improvement = recall_new.mean() - recall_orig.mean()\n",
    "    f1_improvement = f1_new.mean() - f1_orig.mean()\n",
    "    \n",
    "    print(f\"  Precision: {precision_orig.mean():.4f} → {precision_new.mean():.4f} ({precision_improvement:+.4f})\")\n",
    "    print(f\"  Recall:    {recall_orig.mean():.4f} → {recall_new.mean():.4f} ({recall_improvement:+.4f})\")\n",
    "    print(f\"  F1-Score:  {f1_orig.mean():.4f} → {f1_new.mean():.4f} ({f1_improvement:+.4f})\")\n",
    "    \n",
    "    # Critério de aceitação: melhoria em pelo menos 1 métrica sem deteriorar outras significativamente\n",
    "    improvements = [precision_improvement, recall_improvement, f1_improvement]\n",
    "    positive_improvements = sum(1 for imp in improvements if imp > 0.001)  # Melhoria mínima de 0.1%\n",
    "    negative_improvements = sum(1 for imp in improvements if imp < -0.005)  # Deterioração máxima de 0.5%\n",
    "    \n",
    "    is_good = positive_improvements >= 1 and negative_improvements == 0\n",
    "    \n",
    "    print(f\"  DECISÃO: {'✅ ACEITAR' if is_good else '❌ REJEITAR'}\")\n",
    "    \n",
    "    return is_good, {\n",
    "        'precision_improvement': precision_improvement,\n",
    "        'recall_improvement': recall_improvement,\n",
    "        'f1_improvement': f1_improvement\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# 2. REDUÇÃO DE REDUNDÂNCIA: FUSÃO ICULOS + HOUR\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n2. REDUÇÃO DE REDUNDÂNCIA TEMPORAL:\")\n",
    "\n",
    "if 'Hour' in X_train_fe.columns and 'ICULOS' in X_train_fe.columns:\n",
    "    # Verificar correlação\n",
    "    corr_hour_iculos = X_train_fe['Hour'].corr(X_train_fe['ICULOS'])\n",
    "    print(f\"Correlação Hour vs ICULOS: {corr_hour_iculos:.4f}\")\n",
    "    \n",
    "    # Criar feature fusionada (média ponderada)\n",
    "    X_train_fe['Time_ICU_Unified'] = (X_train_fe['Hour'] + X_train_fe['ICULOS']) / 2\n",
    "    \n",
    "    # Validar se a fusão mantém performance\n",
    "    X_test_unified = X_train_fe.drop(columns=['Hour', 'ICULOS'])\n",
    "    is_good, metrics = validate_feature(\n",
    "        X_train_fe[['Hour', 'ICULOS']], \n",
    "        X_train_fe[['Time_ICU_Unified']], \n",
    "        y_train_fe, \n",
    "        'Time_ICU_Unified (fusão Hour+ICULOS)'\n",
    "    )\n",
    "    \n",
    "    if is_good:\n",
    "        print(\"  ✅ Fusão aceita - removendo Hour e ICULOS originais\")\n",
    "        X_train_fe.drop(columns=['Hour', 'ICULOS'], inplace=True)\n",
    "    else:\n",
    "        print(\"  ❌ Fusão rejeitada - mantendo variáveis originais\")\n",
    "        X_train_fe.drop(columns=['Time_ICU_Unified'], inplace=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. REDUÇÃO DE REDUNDÂNCIA: FUSÃO SBP + MAP  \n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n3. REDUÇÃO DE REDUNDÂNCIA PRESSÃO:\")\n",
    "\n",
    "if 'SBP' in X_train_fe.columns and 'MAP' in X_train_fe.columns:\n",
    "    # Verificar correlação\n",
    "    corr_sbp_map = X_train_fe['SBP'].corr(X_train_fe['MAP'])\n",
    "    print(f\"Correlação SBP vs MAP: {corr_sbp_map:.4f}\")\n",
    "    \n",
    "    # Criar feature fusionada (MAP é mais diretamente relevante clinicamente)\n",
    "    # Usar MAP como base e ajustar com informação de SBP\n",
    "    X_train_fe['Pressure_Unified'] = 0.7 * X_train_fe['MAP'] + 0.3 * X_train_fe['SBP']\n",
    "    \n",
    "    # Validar fusão\n",
    "    is_good, metrics = validate_feature(\n",
    "        X_train_fe[['SBP', 'MAP']], \n",
    "        X_train_fe[['Pressure_Unified']], \n",
    "        y_train_fe, \n",
    "        'Pressure_Unified (fusão SBP+MAP)'\n",
    "    )\n",
    "    \n",
    "    if is_good:\n",
    "        print(\"  ✅ Fusão aceita - removendo SBP e MAP originais\")\n",
    "        X_train_fe.drop(columns=['SBP', 'MAP'], inplace=True)\n",
    "    else:\n",
    "        print(\"  ❌ Fusão rejeitada - mantendo variáveis originais\")\n",
    "        X_train_fe.drop(columns=['Pressure_Unified'], inplace=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. FEATURES TEMPORAIS DE ALTA QUALIDADE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n4. CRIAÇÃO E VALIDAÇÃO DE FEATURES TEMPORAIS:\")\n",
    "\n",
    "# Dataset base atual para comparação\n",
    "X_base = X_train_fe.copy()\n",
    "accepted_features = []\n",
    "rejected_features = []\n",
    "\n",
    "# Feature 1: Janela de Risco Crítico (>100h baseado na EDA)\n",
    "if 'Time_ICU_Unified' in X_train_fe.columns:\n",
    "    time_col = 'Time_ICU_Unified'\n",
    "elif 'Hour' in X_train_fe.columns:\n",
    "    time_col = 'Hour'\n",
    "else:\n",
    "    time_col = None\n",
    "\n",
    "if time_col:\n",
    "    X_test = X_base.copy()\n",
    "    X_test['Critical_Risk_Window'] = (X_test[time_col] > 100).astype(int)\n",
    "    \n",
    "    is_good, metrics = validate_feature(X_base, X_test, y_train_fe, 'Critical_Risk_Window')\n",
    "    \n",
    "    if is_good:\n",
    "        X_train_fe['Critical_Risk_Window'] = X_test['Critical_Risk_Window']\n",
    "        accepted_features.append('Critical_Risk_Window')\n",
    "    else:\n",
    "        rejected_features.append('Critical_Risk_Window')\n",
    "\n",
    "# Feature 2: Categorização de Urgência por Tempo\n",
    "if time_col:\n",
    "    X_test = X_base.copy() \n",
    "    # Criar categorias mais simples: Early, Medium, High Risk\n",
    "    X_test['Time_Category'] = pd.cut(\n",
    "        X_test[time_col],\n",
    "        bins=[-1, 24, 100, float('inf')], \n",
    "        labels=[0, 1, 2],  # Early, Medium, High\n",
    "        include_lowest=True\n",
    "    ).astype(int)\n",
    "    \n",
    "    is_good, metrics = validate_feature(X_base, X_test, y_train_fe, 'Time_Category')\n",
    "    \n",
    "    if is_good:\n",
    "        X_train_fe['Time_Category'] = X_test['Time_Category']\n",
    "        accepted_features.append('Time_Category')\n",
    "    else:\n",
    "        rejected_features.append('Time_Category')\n",
    "\n",
    "# Feature 3: Logaritmo do Tempo (para normalizar distribuição)\n",
    "if time_col:\n",
    "    X_test = X_base.copy()\n",
    "    X_test['Log_Time_ICU'] = np.log1p(X_test[time_col])\n",
    "    \n",
    "    is_good, metrics = validate_feature(X_base, X_test, y_train_fe, 'Log_Time_ICU')\n",
    "    \n",
    "    if is_good:\n",
    "        X_train_fe['Log_Time_ICU'] = X_test['Log_Time_ICU']\n",
    "        accepted_features.append('Log_Time_ICU')\n",
    "    else:\n",
    "        rejected_features.append('Log_Time_ICU')\n",
    "\n",
    "# Feature 4: Admissão Direta UTI\n",
    "if 'HospAdmTime' in X_train_fe.columns:\n",
    "    X_test = X_base.copy()\n",
    "    X_test['Direct_ICU_Admission'] = (X_test['HospAdmTime'] <= 0).astype(int)\n",
    "    \n",
    "    is_good, metrics = validate_feature(X_base, X_test, y_train_fe, 'Direct_ICU_Admission')\n",
    "    \n",
    "    if is_good:\n",
    "        X_train_fe['Direct_ICU_Admission'] = X_test['Direct_ICU_Admission']\n",
    "        accepted_features.append('Direct_ICU_Admission')\n",
    "    else:\n",
    "        rejected_features.append('Direct_ICU_Admission')\n",
    "\n",
    "# =============================================================================\n",
    "# 5. RELATÓRIO FINAL\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RELATÓRIO FINAL - FEATURES TEMPORAIS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\n✅ FEATURES ACEITAS ({len(accepted_features)}):\")\n",
    "for feature in accepted_features:\n",
    "    print(f\"  • {feature}\")\n",
    "\n",
    "print(f\"\\n❌ FEATURES REJEITADAS ({len(rejected_features)}):\")\n",
    "for feature in rejected_features:\n",
    "    print(f\"  • {feature}\")\n",
    "\n",
    "print(f\"\\nDATASET FINAL:\")\n",
    "print(f\"  • Shape original: {X_train_final_cleaned.shape}\")\n",
    "print(f\"  • Shape atual: {X_train_fe.shape}\")\n",
    "print(f\"  • Features úteis adicionadas: {len(accepted_features)}\")\n",
    "print(f\"  • Redução de redundância: Hour+ICULOS e SBP+MAP processadas\")\n",
    "\n",
    "print(f\"\\n📊 PRÓXIMO PASSO: Validação final com modelo completo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63655ae0",
   "metadata": {},
   "source": [
    "### 5.3 Interações entre Variáveis\n",
    "\n",
    "Criação de features que capturam interações sinérgicas entre variáveis clínicas relacionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd20a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER: SEÇÃO 5.3 REMOVIDA\n",
    "# Interações entre variáveis removidas para evitar ruído e redundância\n",
    "print(\"Seção 5.3 (Interações) removida - focando apenas em features validadas por métricas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5fddce",
   "metadata": {},
   "source": [
    "### 5.4 Transformações de Distribuição\n",
    "\n",
    "Aplicação de transformações matemáticas para normalizar distribuições assimétricas identificadas na EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d2b6f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INICIANDO TRANSFORMAÇÕES DE DISTRIBUIÇÃO...\n",
      "\n",
      "1. ANÁLISE DE ASSIMETRIA DAS VARIÁVEIS:\n",
      "Variáveis com assimetria |skew| > 1.0: 10\n",
      "  • O2Sat: skew=-4.341, min=20.00\n",
      "  • MAP: skew=1.131, min=20.00\n",
      "  • DBP: skew=1.141, min=20.00\n",
      "  • Resp: skew=1.096, min=1.00\n",
      "  • BUN: skew=5.448, min=1.00\n",
      "  • WBC: skew=36.228, min=0.10\n",
      "  • Platelets: skew=4.806, min=2.00\n",
      "  • HospAdmTime: skew=47.317, min=0.00\n",
      "  • ICULOS: skew=4.093, min=1.00\n",
      "  • Hour: skew=4.076, min=0.00\n",
      "\n",
      "2. TRANSFORMAÇÕES LOGARÍTMICAS:\n",
      "  • BUN_Log: skew 5.448 → -1.095\n",
      "  • WBC_Log: skew 36.228 → -1.080\n",
      "  • Platelets_Log: skew 4.806 → -3.662\n",
      "  • Hour_Log1p: skew 4.076 → -0.564\n",
      "Variáveis com assimetria |skew| > 1.0: 10\n",
      "  • O2Sat: skew=-4.341, min=20.00\n",
      "  • MAP: skew=1.131, min=20.00\n",
      "  • DBP: skew=1.141, min=20.00\n",
      "  • Resp: skew=1.096, min=1.00\n",
      "  • BUN: skew=5.448, min=1.00\n",
      "  • WBC: skew=36.228, min=0.10\n",
      "  • Platelets: skew=4.806, min=2.00\n",
      "  • HospAdmTime: skew=47.317, min=0.00\n",
      "  • ICULOS: skew=4.093, min=1.00\n",
      "  • Hour: skew=4.076, min=0.00\n",
      "\n",
      "2. TRANSFORMAÇÕES LOGARÍTMICAS:\n",
      "  • BUN_Log: skew 5.448 → -1.095\n",
      "  • WBC_Log: skew 36.228 → -1.080\n",
      "  • Platelets_Log: skew 4.806 → -3.662\n",
      "  • Hour_Log1p: skew 4.076 → -0.564\n",
      "  • ICULOS_Log: skew 4.093 → -0.253\n",
      "  • HospAdmTime_Log1p: skew 47.317 → 19.362\n",
      "\n",
      "3. TRANSFORMAÇÕES DE RAIZ QUADRADA:\n",
      "  • HR_Sqrt: skew 0.461 → 0.119\n",
      "  • Resp_Sqrt: skew 1.096 → 0.242\n",
      "  • WBC_Sqrt: skew 36.228 → 3.814\n",
      "  • ICULOS_Log: skew 4.093 → -0.253\n",
      "  • HospAdmTime_Log1p: skew 47.317 → 19.362\n",
      "\n",
      "3. TRANSFORMAÇÕES DE RAIZ QUADRADA:\n",
      "  • HR_Sqrt: skew 0.461 → 0.119\n",
      "  • Resp_Sqrt: skew 1.096 → 0.242\n",
      "  • WBC_Sqrt: skew 36.228 → 3.814\n",
      "  • Platelets_Sqrt: skew 4.806 → 0.285\n",
      "\n",
      "4. TRANSFORMAÇÕES BOX-COX:\n",
      "  • Platelets_Sqrt: skew 4.806 → 0.285\n",
      "\n",
      "4. TRANSFORMAÇÕES BOX-COX:\n",
      "  • BUN_BoxCox: skew 5.448 → 0.354, λ=0.302\n",
      "\n",
      "5. TRANSFORMAÇÕES YEO-JOHNSON:\n",
      "  • BUN_BoxCox: skew 5.448 → 0.354, λ=0.302\n",
      "\n",
      "5. TRANSFORMAÇÕES YEO-JOHNSON:\n",
      "  • HospAdmTime_YeoJohnson: skew 47.317 → 13.189\n",
      "\n",
      "6. TRANSFORMAÇÕES RECÍPROCAS:\n",
      "  • BUN_Reciprocal: skew 5.448 → 21.095\n",
      "  • WBC_Reciprocal: skew 36.228 → 108.192\n",
      "\n",
      "7. PADRONIZAÇÃO DAS TRANSFORMAÇÕES:\n",
      "Transformações a padronizar: 14\n",
      "  • BUN_Log_Std criada\n",
      "  • HospAdmTime_YeoJohnson: skew 47.317 → 13.189\n",
      "\n",
      "6. TRANSFORMAÇÕES RECÍPROCAS:\n",
      "  • BUN_Reciprocal: skew 5.448 → 21.095\n",
      "  • WBC_Reciprocal: skew 36.228 → 108.192\n",
      "\n",
      "7. PADRONIZAÇÃO DAS TRANSFORMAÇÕES:\n",
      "Transformações a padronizar: 14\n",
      "  • BUN_Log_Std criada\n",
      "  • WBC_Log_Std criada\n",
      "  • Platelets_Log_Std criada\n",
      "  • Hour_Log1p_Std criada\n",
      "  • ICULOS_Log_Std criada\n",
      "\n",
      "VALIDAÇÃO DAS TRANSFORMAÇÕES:\n",
      "Total de transformações criadas: 19\n",
      "  • WBC_Log_Std criada\n",
      "  • Platelets_Log_Std criada\n",
      "  • Hour_Log1p_Std criada\n",
      "  • ICULOS_Log_Std criada\n",
      "\n",
      "VALIDAÇÃO DAS TRANSFORMAÇÕES:\n",
      "Total de transformações criadas: 19\n",
      "\n",
      "Transformações bem-sucedidas: 19\n",
      "Transformações problemáticas: 0\n",
      "\n",
      "Dataset após transformações: (1209197, 40)\n",
      "Total de novas features criadas: 24\n",
      "\n",
      "Transformações bem-sucedidas: 19\n",
      "Transformações problemáticas: 0\n",
      "\n",
      "Dataset após transformações: (1209197, 40)\n",
      "Total de novas features criadas: 24\n"
     ]
    }
   ],
   "source": [
    "# PLACEHOLDER: SEÇÃO 5.4 REMOVIDA  \n",
    "# Transformações de distribuição removidas para evitar complexidade desnecessária\n",
    "print(\"Seção 5.4 (Transformações) removida - focando apenas em features com impacto comprovado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e5a52",
   "metadata": {},
   "source": [
    "### 5.5 Validação e Resumo do Feature Engineering\n",
    "\n",
    "**Síntese completa de todas as features criadas na Tarefa 3 - Construção dos Dados:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d3511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDAÇÃO FINAL E RESUMO DO FEATURE ENGINEERING SIMPLIFICADO\n",
    "print(\"=\"*70)\n",
    "print(\"RESUMO FINAL DO FEATURE ENGINEERING - ABORDAGEM FOCADA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. ANÁLISE QUANTITATIVA DAS FEATURES CRIADAS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n1. ANÁLISE QUANTITATIVA:\")\n",
    "original_features = X_train_final_cleaned.shape[1]\n",
    "final_features = X_train_fe.shape[1]\n",
    "new_features = final_features - original_features\n",
    "\n",
    "print(f\"Features originais (após limpeza): {original_features}\")\n",
    "print(f\"Features finais (após engineering): {final_features}\")\n",
    "print(f\"Novas features criadas: {new_features}\")\n",
    "print(f\"Expansão percentual: {(new_features/original_features)*100:.1f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. LISTA DAS FEATURES VALIDADAS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n2. FEATURES CRIADAS E VALIDADAS:\")\n",
    "\n",
    "all_new_features = []\n",
    "for col in X_train_fe.columns:\n",
    "    if col not in X_train_final_cleaned.columns:\n",
    "        all_new_features.append(col)\n",
    "\n",
    "if all_new_features:\n",
    "    print(\"Features aceitas por validação de métricas:\")\n",
    "    for i, feature in enumerate(all_new_features, 1):\n",
    "        print(f\"  {i:2d}. {feature}\")\n",
    "else:\n",
    "    print(\"Nenhuma feature adicional foi validada como benéfica\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ANÁLISE DE CORRELAÇÃO COM TARGET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n3. ANÁLISE DE CORRELAÇÃO COM TARGET:\")\n",
    "\n",
    "# Calcular correlação das novas features com SepsisLabel\n",
    "correlations = []\n",
    "for col in all_new_features:\n",
    "    if col in X_train_fe.columns:\n",
    "        try:\n",
    "            corr = X_train_fe[col].corr(y_train_fe)\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append((col, abs(corr)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if correlations:\n",
    "    # Ordenar por correlação absoluta\n",
    "    correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Correlações das novas features com SepsisLabel:\")\n",
    "    for i, (feature, corr) in enumerate(correlations):\n",
    "        print(f\"  {i+1:2d}. {feature}: {corr:.4f}\")\n",
    "else:\n",
    "    print(\"Nenhuma nova feature para análise de correlação\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. VALIDAÇÃO FINAL DE QUALIDADE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n4. VALIDAÇÃO FINAL DE QUALIDADE:\")\n",
    "\n",
    "# Verificar se há problemas no dataset final\n",
    "total_missing = X_train_fe.isnull().sum().sum()\n",
    "infinite_values = np.isinf(X_train_fe.select_dtypes(include=[np.number])).sum().sum()\n",
    "\n",
    "print(f\"  • Total de valores missing: {total_missing:,}\")\n",
    "print(f\"  • Total de valores infinitos: {infinite_values:,}\")\n",
    "print(f\"  • Alinhamento com target: {X_train_fe.index.equals(y_train_fe.index)}\")\n",
    "\n",
    "# Distribuição do target\n",
    "target_dist = y_train_fe.value_counts()\n",
    "print(f\"  • Distribuição target: 0={target_dist[0]:,}, 1={target_dist[1]:,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. PRÓXIMOS PASSOS E DATASETS FINAIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n5. DATASETS FINAIS PREPARADOS:\")\n",
    "\n",
    "# Salvar referências para próximas etapas\n",
    "X_train_engineered = X_train_fe.copy()\n",
    "y_train_engineered = y_train_fe.copy()\n",
    "\n",
    "print(f\"  • X_train_engineered: {X_train_engineered.shape}\")\n",
    "print(f\"  • y_train_engineered: {y_train_engineered.shape}\")\n",
    "print(f\"  • Qualidade: 100% completo, sem infinitos\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. RESUMO DA ABORDAGEM APLICADA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"METODOLOGIA APLICADA - FEATURE ENGINEERING FOCADO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "methodology = {\n",
    "    \"Critério 1\": \"Evitar redundância - Fusão de ICULOS+Hour e SBP+MAP\",\n",
    "    \"Critério 2\": \"Evitar ruído - Apenas features com validação por métricas\",\n",
    "    \"Critério 3\": \"Validação rigorosa - Precision, Recall, F1-Score para cada feature\",\n",
    "    \"Resultado\": \"Dataset otimizado com features de alta qualidade comprovada\"\n",
    "}\n",
    "\n",
    "for criterion, description in methodology.items():\n",
    "    print(f\"✓ {criterion}: {description}\")\n",
    "\n",
    "print(f\"\\n🎯 RESULTADO FINAL:\")\n",
    "print(f\"   • Abordagem conservadora e validada\")\n",
    "print(f\"   • Redução de redundância aplicada\")\n",
    "print(f\"   • Apenas features com impacto positivo comprovado\")\n",
    "print(f\"   • Dataset pronto para Formatação dos Dados (Tarefa 5)\")\n",
    "\n",
    "print(f\"\\n📊 PRONTO PARA MODELAGEM OTIMIZADA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da84ed86",
   "metadata": {},
   "source": [
    "## 6. TAREFA 4: Integração dos Dados\n",
    "\n",
    "**Objetivo:** Combinar dados de diferentes fontes em um único conjunto de dados consistente.\n",
    "\n",
    "**Observação:** Esta tarefa não se aplica ao dataset atual, pois trabalhamos com uma única fonte (PhysioNet 2019 Challenge). Esta seção documenta a estrutura para futuras expansões do projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d4688",
   "metadata": {},
   "source": [
    "### 6.1 Documentação da Fonte Única\n",
    "\n",
    "Registro da origem e características do dataset único utilizado no projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ba396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentar características da fonte única\n",
    "dataset_info = {\n",
    "    'source': 'PhysioNet 2019 Challenge',\n",
    "    'description': 'Early Detection of Sepsis from Clinical Data',\n",
    "    'patients': 'Multiple ICU patients with temporal observations',\n",
    "    'timeframe': 'Variable length ICU stays',\n",
    "    'completeness': 'Single comprehensive source'\n",
    "}\n",
    "\n",
    "print(\"INFORMAÇÕES DA FONTE DE DADOS:\")\n",
    "for key, value in dataset_info.items():\n",
    "    print(f\"  • {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34748720",
   "metadata": {},
   "source": [
    "### 6.2 Preparação para Futuras Integrações\n",
    "\n",
    "Estrutura preparatória para possível integração com outras fontes de dados em versões futuras do projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para estrutura de integração futura\n",
    "# Definir schemas, chaves de junção, protocolos de merge\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ab110d",
   "metadata": {},
   "source": [
    "## 7. TAREFA 5: Formatação dos Dados\n",
    "\n",
    "**Objetivo:** Preparar os dados no formato necessário para os algoritmos de modelagem, incluindo normalização, encoding e divisão final dos conjuntos.\n",
    "\n",
    "**Atividades principais:**\n",
    "* Normalização/padronização de variáveis numéricas\n",
    "* Encoding de variáveis categóricas  \n",
    "* Balanceamento de classes\n",
    "* Criação dos datasets finais para modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a1b02",
   "metadata": {},
   "source": [
    "### 7.1 Normalização e Padronização\n",
    "\n",
    "Aplicação de transformações de escala para garantir que todas as variáveis numéricas tenham contribuições equilibradas nos algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20340ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para normalização\n",
    "# StandardScaler, MinMaxScaler baseado na distribuição das variáveis\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdab3d8",
   "metadata": {},
   "source": [
    "### 7.2 Encoding de Variáveis Categóricas\n",
    "\n",
    "Conversão de variáveis categóricas para formato numérico apropriado para algoritmos de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para encoding categórico\n",
    "# One-hot encoding, label encoding baseado na cardinalidade\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78947d7e",
   "metadata": {},
   "source": [
    "### 7.3 Balanceamento de Classes\n",
    "\n",
    "Implementação de técnicas para lidar com o severo desbalanceamento entre classes (98.2% vs 1.8%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd89bef",
   "metadata": {},
   "source": [
    "#### 7.3.1 Análise de Estratégias de Balanceamento\n",
    "\n",
    "Comparação de diferentes abordagens: oversampling, undersampling e métodos combinados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para comparação de estratégias\n",
    "# SMOTE, Random Over/Under Sampling, SMOTETomek\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a7ea5",
   "metadata": {},
   "source": [
    "#### 7.3.2 Implementação da Estratégia Escolhida\n",
    "\n",
    "Aplicação da técnica de balanceamento selecionada com base na análise comparativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para implementação do balanceamento\n",
    "# Aplicar técnica escolhida e validar resultados\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65297c42",
   "metadata": {},
   "source": [
    "### 7.4 Criação dos Datasets Finais\n",
    "\n",
    "Montagem dos conjuntos de dados finais prontos para a fase de modelagem, incluindo validação da integridade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para datasets finais\n",
    "# Criar X_train_final, X_test_final, y_train_final, y_test_final\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f08b9",
   "metadata": {},
   "source": [
    "### 7.5 Validação Final e Export\n",
    "\n",
    "Verificação final da qualidade e consistência dos dados preparados, seguida do salvamento dos datasets processados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6dd45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para validação final\n",
    "# Verificar shapes, tipos, ranges, consistência lógica\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a347d3",
   "metadata": {},
   "source": [
    "## 8. Resumo da Preparação\n",
    "\n",
    "**Síntese das transformações aplicadas:** Documentação completa de todas as modificações realizadas nos dados durante o processo de preparação.\n",
    "\n",
    "**Datasets resultantes:** Características finais dos conjuntos de dados prontos para modelagem.\n",
    "\n",
    "**Próximos passos:** Direcionamento para a fase de modelagem do CRISP-DM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fccc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para resumo final\n",
    "# Documentar todas as transformações e características finais\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023cc45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Próxima Fase:** Modeling (3-modeling.ipynb)\n",
    "\n",
    "**Entregáveis desta fase:**\n",
    "- Datasets limpos e preparados\n",
    "- Features engineered com relevância clínica  \n",
    "- Classes balanceadas adequadamente\n",
    "- Documentação completa das transformações\n",
    "- Validação da qualidade dos dados processados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
