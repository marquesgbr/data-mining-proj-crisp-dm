{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a2846b",
   "metadata": {},
   "source": [
    "# Data Preparation - Dataset de Sepsis\n",
    "## CRISP-DM Fase 3: Preparação dos Dados\n",
    "\n",
    "**Objetivo da Fase:**\n",
    "* Transformar dados brutos em formato adequado para modelagem\n",
    "* Implementar estratégias de limpeza e tratamento baseadas nos insights da EDA\n",
    "* Criar features derivadas com relevância clínica\n",
    "* Preparar datasets finais para algoritmos de machine learning\n",
    "\n",
    "**Baseado nos Insights da EDA:**\n",
    "* 37/41 variáveis apresentam missing values (68.37% do dataset)\n",
    "* 27 variáveis com >80% missing (candidatas à remoção)\n",
    "* Dataset altamente desbalanceado: 98.2% não-sepsis vs 1.8% sepsis\n",
    "* Estrutura temporal importante: risco aumenta após 100h na UTI\n",
    "* Variáveis categóricas bem definidas: Gender, Unit1, Unit2\n",
    "\n",
    "**Tarefas CRISP-DM a serem executadas:**\n",
    "1. **Seleção dos Dados**: Escolher variáveis mais relevantes\n",
    "2. **Limpeza dos Dados**: Tratar inconsistências e valores ausentes  \n",
    "3. **Construção dos Dados**: Criar features derivadas e engenharia\n",
    "4. **Integração dos Dados**: Combinar fontes (não aplicável aqui)\n",
    "5. **Formatação dos Dados**: Preparar formato final para modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21555a66",
   "metadata": {},
   "source": [
    "## Configuração do Ambiente Google Colab\n",
    "\n",
    "Para funcionar no Google Colab, é necessário criar um atalho do diretório MDA no seu próprio Drive e então rodar os dois comandos abaixo e conceder permissão ao seu drive quando rodar a célula logo abaixo.\n",
    "\n",
    "[Link](https://towardsdatascience.com/simplify-file-sharing-44bde79a8a18/) detalhando como funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f059104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modificar para o diretorio que contém os dados de teste e treino\n",
    "%cd /content/drive/MyDrive/MDA/Train\\ and\\ test\\ data\\ -\\ Proj\\ DM/\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fed35e",
   "metadata": {},
   "source": [
    "## 1. Importação das Bibliotecas\n",
    "\n",
    "Importação de todas as bibliotecas necessárias para preparação dos dados, incluindo bibliotecas específicas para pré-processamento, feature engineering e balanceamento de classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca5c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas essenciais para manipulação de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Bibliotecas para pré-processamento\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# Bibliotecas para balanceamento de classes\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Configurações gerais\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68cfe4b",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados e Insights da EDA\n",
    "\n",
    "Carregamento dos datasets de treino e teste, seguido da documentação dos principais insights obtidos na análise exploratória que guiarão as decisões de preparação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945c328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1241768, 41) | y_train: (1241768,)\n",
      "SepsisLabel\n",
      "0.0    0.982015\n",
      "1.0    0.017985\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('dataset_sepsis_train.csv')\n",
    "\n",
    "# Separar features e target\n",
    "X_train = train_df.drop('SepsisLabel', axis=1)\n",
    "y_train = train_df['SepsisLabel']\n",
    "\n",
    "# Forma final\n",
    "print(f\"X_train: {X_train.shape} | y_train: {y_train.shape}\")\n",
    "\n",
    "# Distribuição das classes no treino\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a29a8",
   "metadata": {},
   "source": [
    "## 3. TAREFA 1: Seleção dos Dados\n",
    "\n",
    "**Objetivo:** Escolher as variáveis mais relevantes para o modelo de mineração, removendo features com baixo potencial preditivo ou problemas graves de qualidade.\n",
    "\n",
    "**Critérios de seleção:**\n",
    "* Relevância clínica para detecção de sepsis\n",
    "* Percentual de missing values aceitável\n",
    "* Separabilidade entre classes (baseada na EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90a9f6",
   "metadata": {},
   "source": [
    "### 3.1 Mapeamento de Variáveis com Excesso de Missing Values para Remoção\n",
    "\n",
    "Vamos refazer a análise, mais objetiva e breve, das variáveis com >60% missing values para decidir quais manter, tratar ou remover baseado no critério de separabilidade de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e4c277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMPUTAR_SIMPLES (2 variáveis):\n",
      "  • Temp: Sep=0.393 | Missing=66.2% | n=419,945\n",
      "  • BUN: Sep=0.353 | Missing=93.1% | n=85,440\n",
      "\n",
      "IMPUTAR_AVANCADA (2 variáveis):\n",
      "  • Platelets: Sep=0.203 | Missing=94.1% | n=73,790\n",
      "  • WBC: Sep=0.194 | Missing=93.6% | n=79,613\n",
      "\n",
      "DESCARTAR (24 variáveis):\n",
      "  • Hgb: Sep=0.152 | Missing=92.6% | n=91,759\n",
      "  • Creatinine: Sep=0.151 | Missing=93.9% | n=75,809\n",
      "  • pH: Sep=0.135 | Missing=93.1% | n=86,094\n",
      "  • Fibrinogen: Sep=0.134 | Missing=99.3% | n=8,203\n",
      "  • Hct: Sep=0.091 | Missing=91.1% | n=109,980\n",
      "  • PTT: Sep=0.085 | Missing=97.0% | n=36,690\n",
      "  • Calcium: Sep=0.083 | Missing=94.1% | n=73,269\n",
      "  • Bilirubin_total: Sep=0.077 | Missing=98.5% | n=18,518\n",
      "  • Alkalinephos: Sep=0.071 | Missing=98.4% | n=19,954\n",
      "  • Phosphate: Sep=0.070 | Missing=96.0% | n=50,011\n",
      "  • Bilirubin_direct: Sep=0.062 | Missing=99.8% | n=2,393\n",
      "  • Lactate: Sep=0.048 | Missing=97.3% | n=33,238\n",
      "  • Glucose: Sep=0.039 | Missing=82.9% | n=212,578\n",
      "  • AST: Sep=0.015 | Missing=98.4% | n=20,144\n",
      "  • Potassium: Sep=0.008 | Missing=90.7% | n=115,900\n",
      "  • TroponinI: Sep=0.000 | Missing=99.1% | n=11,743\n",
      "  • EtCO2: Sep=0.000 | Missing=96.3% | n=46,047\n",
      "  • BaseExcess: Sep=0.000 | Missing=94.6% | n=67,324\n",
      "  • HCO3: Sep=0.000 | Missing=95.8% | n=52,334\n",
      "  • FiO2: Sep=0.000 | Missing=91.7% | n=103,618\n",
      "  • PaCO2: Sep=0.000 | Missing=94.4% | n=69,132\n",
      "  • SaO2: Sep=0.000 | Missing=96.6% | n=42,777\n",
      "  • Chloride: Sep=0.000 | Missing=95.4% | n=56,701\n",
      "  • Magnesium: Sep=0.000 | Missing=93.7% | n=78,652\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identificar variáveis com >60% missing\n",
    "high_missing_vars = []\n",
    "for col in X_train.select_dtypes(include=[np.number]).columns:\n",
    "    missing_pct = (X_train[col].isnull().sum() / len(X_train)) * 100\n",
    "    if missing_pct > 60:\n",
    "        high_missing_vars.append({\n",
    "            'variavel': col,\n",
    "            'missing_pct': missing_pct\n",
    "        })\n",
    "\n",
    "# Calcular separabilidade para cada variável\n",
    "separability_results = {\n",
    "    'IMPUTAR_SIMPLES': [],     # Separabilidade > 0.3: Alta discriminação\n",
    "    'IMPUTAR_AVANCADA': [],    # Separabilidade 0.16 - 0.3: Discriminação moderada  \n",
    "    'DESCARTAR': []            # Separabilidade < 0.16: Baixa discriminação\n",
    "}\n",
    "\n",
    "for var_info in high_missing_vars:\n",
    "    col = var_info['variavel']\n",
    "    missing_pct = var_info['missing_pct']\n",
    "    \n",
    "    # Criar DataFrame temporário sem valores faltantes\n",
    "    temp_df = pd.DataFrame({\n",
    "        'feature': X_train[col],\n",
    "        'target': y_train\n",
    "    }).dropna()\n",
    "\n",
    "    # Separar por classe\n",
    "    no_sepsis_data = temp_df[temp_df['target'] == 0]['feature']\n",
    "    sepsis_data = temp_df[temp_df['target'] == 1]['feature']\n",
    "\n",
    "    # Calcular separabilidade (diferença de medianas / desvio padrão)\n",
    "    median_diff = abs(sepsis_data.median() - no_sepsis_data.median())\n",
    "    pooled_std = no_sepsis_data.std() if no_sepsis_data.std() > 0 else 1\n",
    "    separability = median_diff / pooled_std\n",
    "    \n",
    "    # Classificar baseado na separabilidade\n",
    "    var_result = {\n",
    "        'variavel': col,\n",
    "        'missing_pct': missing_pct,\n",
    "        'separabilidade': separability,\n",
    "        'n_amostras': len(temp_df)\n",
    "    }\n",
    "    \n",
    "    if separability > 0.3:\n",
    "        separability_results['IMPUTAR_SIMPLES'].append(var_result)\n",
    "    elif separability >= 0.16:\n",
    "        separability_results['IMPUTAR_AVANCADA'].append(var_result)\n",
    "    else:\n",
    "        separability_results['DESCARTAR'].append(var_result)\n",
    "\n",
    "# Exibir resultados \n",
    "\n",
    "for categoria, vars_list in separability_results.items():\n",
    "    print(f\"\\n{categoria} ({len(vars_list)} variáveis):\")\n",
    "    for var in sorted(vars_list, key=lambda x: x['separabilidade'], reverse=True):\n",
    "        print(f\"  • {var['variavel']}: Sep={var['separabilidade']:.3f} | Missing={var['missing_pct']:.1f}% | n={var['n_amostras']:,}\")\n",
    "\n",
    "variables_to_keep = [var['variavel'] for var in separability_results['IMPUTAR_SIMPLES']]\n",
    "variables_to_treat = [var['variavel'] for var in separability_results['IMPUTAR_AVANCADA']]  \n",
    "variables_to_discard = [var['variavel'] for var in separability_results['DESCARTAR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9bb2c",
   "metadata": {},
   "source": [
    "### 3.2 Análise de Separabilidade Estatística\n",
    "\n",
    "Avaliação da capacidade discriminativa das variáveis que não foram selecionadas para exclusão, a fim de confirmar e justificar as decisões antes de fazer a remoção, usando testes estatísticos e métricas de separação entre classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a8bc6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANÁLISE DE VARIÁVEIS NUMÉRICAS:\n",
      "Variável        Missing%   Separab.   p-value_MW   Mutual Info  N_samples \n",
      "\n",
      "Min Hour: 0\n",
      "Hour            0.0        0.303      0.00000      0.0038413    1,241,768 \n",
      "Min HR: 20.0\n",
      "HR              9.9        0.386      0.00000      0.0010304    1,119,123 \n",
      "Min O2Sat: 20.0\n",
      "O2Sat           13.1       0.000      0.00079      0.0000634    1,079,708 \n",
      "Min Temp: 23.0\n",
      "Temp            66.2       0.326      0.00000      0.0020082    419,945   \n",
      "Min SBP: 20.0\n",
      "SBP             14.6       0.124      0.00000      0.0001972    1,060,857 \n",
      "Min MAP: 20.0\n",
      "MAP             12.4       0.121      0.00000      0.0002270    1,087,236 \n",
      "Min DBP: 20.0\n",
      "DBP             31.3       0.143      0.00000      0.0001955    852,691   \n",
      "Min Resp: 1.0\n",
      "Resp            15.3       0.356      0.00000      0.0010005    1,051,181 \n",
      "Min BUN: 1.0\n",
      "BUN             93.1       0.328      0.00000      0.0018243    85,440    \n",
      "Min WBC: 0.1\n",
      "WBC             93.6       0.166      0.00000      0.0009935    79,613    \n",
      "Min Platelets: 2.0\n",
      "Platelets       94.1       0.189      0.00000      0.0007498    73,790    \n",
      "Min HospAdmTime: -5366.86\n",
      "HospAdmTime     0.0        0.017      0.00000      0.0010616    1,241,762 \n",
      "Min ICULOS: 1.0\n",
      "ICULOS          0.0        0.302      0.00000      0.0040386    1,241,768 \n",
      "\n",
      "ANÁLISE DE VARIÁVEIS CATEGÓRICAS:\n",
      "Variável        Missing%   p-value_Chi2 Mutual Info  N_samples \n",
      "\n",
      "Gender          0.0        0.00000      0.0000457    1,241,768 \n",
      "Unit1           39.5       0.00000      0.0002854    751,787   \n",
      "Unit2           39.5       0.00000      0.0002854    751,787   \n",
      "\n",
      "Separabilidades Numéricas:\n",
      "  • Variáveis com Sep > 0.3: 6\n",
      "  • Variáveis com Sep > 0.16: 8\n",
      "\n",
      "SIGNIFICÂNCIA ESTATÍSTICA (Numéricas):\n",
      "  • p < 0.001 (altamente significativo): 13 variáveis\n",
      "  • 0.001 ≤ p < 0.01 (muito significativo): 0 variáveis\n",
      "  • 0.01 ≤ p < 0.05 (significativo): 0 variáveis\n",
      "  • p ≥ 0.05 (não significativo): 0 variáveis\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "X_train_not_discard = X_train.drop(columns=variables_to_discard)\n",
    "# Separar variáveis numéricas e categóricas\n",
    "categorical_vars = ['Gender', 'Unit1', 'Unit2']  \n",
    "# Numéricas são todas as colunas MENOS as categóricas\n",
    "numeric_vars = [col for col in X_train_not_discard.columns if col not in categorical_vars]\n",
    "\n",
    "\n",
    "# Análise para variáveis numéricas\n",
    "all_separability_results = []\n",
    "\n",
    "print(f\"\\nANÁLISE DE VARIÁVEIS NUMÉRICAS:\")\n",
    "print(f\"{'Variável':<15} {'Missing%':<10} {'Separab.':<10} {'p-value_MW':<12} {'Mutual Info':<12} {'N_samples':<10}\\n\")\n",
    "\n",
    "for var in numeric_vars:\n",
    "    missing_pct = (X_train_not_discard[var].isnull().sum() / len(X_train_not_discard)) * 100\n",
    "    \n",
    "    # Criar DataFrame temporário sem valores faltantes\n",
    "    temp_df = pd.DataFrame({\n",
    "        'feature': X_train_not_discard[var],\n",
    "        'target': y_train\n",
    "    }).dropna()\n",
    "    \n",
    "    # Separar por classe\n",
    "    no_sepsis_data = temp_df[temp_df['target'] == 0]['feature']\n",
    "    sepsis_data = temp_df[temp_df['target'] == 1]['feature']\n",
    "\n",
    "    # Calcular separabilidade (diferença de medianas / desvio padrão)\n",
    "    median_diff = abs(sepsis_data.median() - no_sepsis_data.median())\n",
    "    pooled_std = np.sqrt(((no_sepsis_data.std()**2 + sepsis_data.std()**2) / 2))\n",
    "    separability = median_diff / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    # Teste U de Mann-Whitney (não-paramétrico)\n",
    "    try:\n",
    "        stat, p_value = stats.mannwhitneyu(sepsis_data, no_sepsis_data, alternative='two-sided')\n",
    "        mann_whitney_pval = p_value\n",
    "    except:\n",
    "        mann_whitney_pval = 1.0  # p-value máximo para casos de erro\n",
    "\n",
    "    # Informação mútua\n",
    "    try:\n",
    "        # Discretizar para mutual info (usar quintis)\n",
    "        temp_df['feature_disc'] = pd.qcut(temp_df['feature'], q=5, labels=False, duplicates='drop')\n",
    "        mutual_info = mutual_info_score(temp_df['target'], temp_df['feature_disc'])\n",
    "    except:\n",
    "        mutual_info = 0\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    result = {\n",
    "        'variavel': var,\n",
    "        'missing_pct': missing_pct,\n",
    "        'separabilidade': separability,\n",
    "        'mann_whitney': mann_whitney_pval,\n",
    "        'mutual_info': mutual_info,\n",
    "        'n_amostras': len(temp_df)\n",
    "    }\n",
    "    all_separability_results.append(result)\n",
    "    \n",
    "    # Exibir resultado\n",
    "    print(f\"{var:<15} {missing_pct:<10.1f} {separability:<10.3f} {mann_whitney_pval:<12.5f} {mutual_info:<12.7f} {len(temp_df):<10,}\")\n",
    "\n",
    "\n",
    "# Análise para variáveis categóricas\n",
    "print(f\"\\nANÁLISE DE VARIÁVEIS CATEGÓRICAS:\")\n",
    "print(f\"{'Variável':<15} {'Missing%':<10} {'p-value_Chi2':<12} {'Mutual Info':<12} {'N_samples':<10}\\n\")\n",
    "\n",
    "for var in categorical_vars:\n",
    "    missing_pct = (X_train_not_discard[var].isnull().sum() / len(X_train_not_discard)) * 100\n",
    "    \n",
    "    # Criar DataFrame temporário sem valores faltantes\n",
    "    temp_df = pd.DataFrame({\n",
    "        'feature': X_train_not_discard[var],\n",
    "        'target': y_train\n",
    "    }).dropna()\n",
    "    \n",
    "    # Teste Qui-quadrado\n",
    "    try:\n",
    "        contingency_table = pd.crosstab(temp_df['feature'], temp_df['target'])\n",
    "        chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "        chi2_pval = p_value\n",
    "    except:\n",
    "        chi2_pval = 1.0  # p-value máximo para casos de erro\n",
    "    \n",
    "    # Informação mútua\n",
    "    try:\n",
    "        mutual_info = mutual_info_score(temp_df['target'], temp_df['feature'])\n",
    "    except:\n",
    "        mutual_info = 0\n",
    "    \n",
    "    result = {\n",
    "        'variavel': var,\n",
    "        'missing_pct': missing_pct,\n",
    "        'chi2_sig': chi2_pval,\n",
    "        'mutual_info': mutual_info,\n",
    "        'n_amostras': len(temp_df),\n",
    "        'tipo': 'categorical'\n",
    "    }\n",
    "    all_separability_results.append(result)\n",
    "    \n",
    "    print(f\"{var:<15} {missing_pct:<10.1f} {chi2_pval:<12.5f} {mutual_info:<12.7f} {len(temp_df):<10,}\")\n",
    "\n",
    "\n",
    "# Ranking por separabilidade (variáveis numéricas)\n",
    "numeric_results = [r for r in all_separability_results if 'separabilidade' in r]\n",
    "numeric_results_sorted = sorted(numeric_results, key=lambda x: x['separabilidade'], reverse=True)\n",
    "\n",
    "print(f\"\\nSeparabilidades Numéricas:\")\n",
    "separabilities = [r['separabilidade'] for r in numeric_results]\n",
    "print(f\"  • Variáveis com Sep > 0.3: {sum(1 for s in separabilities if s > 0.3)}\")\n",
    "print(f\"  • Variáveis com Sep > 0.16: {sum(1 for s in separabilities if s > 0.16)}\")\n",
    "\n",
    "# Análise de significância estatística\n",
    "print(f\"\\nSIGNIFICÂNCIA ESTATÍSTICA (Numéricas):\")\n",
    "sig_001 = sum(1 for r in numeric_results if r['mann_whitney'] < 0.001)\n",
    "sig_01 = sum(1 for r in numeric_results if 0.001 <= r['mann_whitney'] < 0.01)\n",
    "sig_05 = sum(1 for r in numeric_results if 0.01 <= r['mann_whitney'] < 0.05)\n",
    "not_sig = sum(1 for r in numeric_results if r['mann_whitney'] >= 0.05)\n",
    "print(f\"  • p < 0.001 (altamente significativo): {sig_001} variáveis\")\n",
    "print(f\"  • 0.001 ≤ p < 0.01 (muito significativo): {sig_01} variáveis\")  \n",
    "print(f\"  • 0.01 ≤ p < 0.05 (significativo): {sig_05} variáveis\")\n",
    "print(f\"  • p ≥ 0.05 (não significativo): {not_sig} variáveis\")\n",
    "\n",
    "# Salvar resultados para uso posterior\n",
    "statistical_analysis_results = {\n",
    "    'numeric_results': numeric_results_sorted,\n",
    "    'all_results': all_separability_results\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ec87c",
   "metadata": {},
   "source": [
    "#### Alteração Após Análise \n",
    "Percebe-se que ainda é possível remover `Age` do escopo de features visto que não há nenhuma métrica que aponte essa variável como algo relevante apesar do que diz a literatura sobre sepsis e o baixo percentual de missing values. Ela possui baixa separabilidade, um Man Whitney não significativo e Mutual Info demonstra zero informação qundo comparado a SepsisLabel\n",
    "\n",
    "| Variável | Missing% | Separabilidade |  p-value_MW | Mutual Info |\n",
    "|----------|----------|----------------|--------------|-------------| \n",
    "| **Age** | 0.0 | 0.000 |  0.4367935 | 0.00000 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977f647",
   "metadata": {},
   "source": [
    "### 3.3 Aplicação das Decisões de Separabilidade\n",
    "\n",
    "Implementação prática da remoção de variáveis com baixa separabilidade e organização das listas para tratamento adiante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39f998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo 25 variáveis com baixa separabilidade...\n",
      "Variáveis removidas:\n",
      "  • EtCO2: 96.3% missing\n",
      "  • BaseExcess: 94.6% missing\n",
      "  • HCO3: 95.8% missing\n",
      "  • FiO2: 91.7% missing\n",
      "  • pH: 93.1% missing\n",
      "  • PaCO2: 94.4% missing\n",
      "  • SaO2: 96.6% missing\n",
      "  • AST: 98.4% missing\n",
      "  • Alkalinephos: 98.4% missing\n",
      "  • Calcium: 94.1% missing\n",
      "  • Chloride: 95.4% missing\n",
      "  • Creatinine: 93.9% missing\n",
      "  • Bilirubin_direct: 99.8% missing\n",
      "  • Glucose: 82.9% missing\n",
      "  • Lactate: 97.3% missing\n",
      "  • Magnesium: 93.7% missing\n",
      "  • Phosphate: 96.0% missing\n",
      "  • Potassium: 90.7% missing\n",
      "  • Bilirubin_total: 98.5% missing\n",
      "  • TroponinI: 99.1% missing\n",
      "  • Hct: 91.1% missing\n",
      "  • Hgb: 92.6% missing\n",
      "  • PTT: 97.0% missing\n",
      "  • Fibrinogen: 99.3% missing\n",
      "  • Age: 0.0% missing (removida por baixa discriminação)\n",
      "\n",
      "Dimensões do dataset:\n",
      "  • Original: (1241768, 41)\n",
      "  • Após seleção: (1241768, 16)\n"
     ]
    }
   ],
   "source": [
    "# Adicionar Age às variáveis a descartar \n",
    "variables_to_discard.append('Age')\n",
    "\n",
    "# Remover variáveis com baixa separabilidade e alto missing do dataset principal\n",
    "total_to_remove = len(variables_to_discard)\n",
    "print(f\"Removendo {total_to_remove} variáveis com baixa separabilidade...\")\n",
    "\n",
    "X_train_selected = X_train.drop(columns=variables_to_discard)\n",
    "\n",
    "print(\"Variáveis removidas:\")\n",
    "for var in variables_to_discard:\n",
    "    missing_pct = (X_train[var].isnull().sum() / len(X_train)) * 100\n",
    "    if var == 'Age':\n",
    "        print(f\"  • {var}: {missing_pct:.1f}% missing (removida por baixa discriminação)\")\n",
    "    else:\n",
    "        print(f\"  • {var}: {missing_pct:.1f}% missing\")\n",
    "\n",
    "print(f\"\\nDimensões do dataset:\")\n",
    "print(f\"  • Original: {X_train.shape}\")\n",
    "print(f\"  • Após seleção: {X_train_selected.shape}\")\n",
    "\n",
    "# Organizar variáveis por estratégia de tratamento\n",
    "high_missing_strategy = {\n",
    "    'imputacao_simples': variables_to_keep,      \n",
    "    'imputacao_avancada': variables_to_treat,    \n",
    "    'removidas': variables_to_discard           \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63353242",
   "metadata": {},
   "source": [
    "### 3.4 Síntese das Decisões de Seleção de Variáveis\n",
    "\n",
    "**Documentação completa das decisões tomadas na Tarefa 1 (Seleção dos Dados) com respectivas justificativas:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759d1bb",
   "metadata": {},
   "source": [
    "\n",
    "RESULTADOS FINAIS DA SELEÇÃO DE VARIÁVEIS\n",
    "\n",
    "CRITÉRIOS DE SELEÇÃO APLICADOS\n",
    "\n",
    "1. **Critério de Missing Values**: Variáveis com >60% de valores ausentes analisadas\n",
    "2. **Critério de Separabilidade**: Capacidade discriminativa entre classes (limite: 0.16)\n",
    "3. **Critério Estatístico**: Significância nos testes Mann-Whitney U e Chi-quadrado\n",
    "\n",
    "IMPACTO FINAL DAS DECISÕES\n",
    "\n",
    "**Redução Dimensional Efetiva:**\n",
    "- **Dataset original**: 1,241,768 × 41 variáveis\n",
    "- **Dataset final**: 1,241,768 × 16 variáveis  \n",
    "- **Redução**: 61% das variáveis removidas (25/41)\n",
    "- **Taxa de compressão**: 2.6:1\n",
    "\n",
    "ESTRATÉGIAS DE TRATAMENTO DEFINIDAS\n",
    "\n",
    "**IMPUTAÇÃO Cuidadosa** (1 variáveis - Separabilidade > 0.3)\n",
    "\n",
    "**Estratégia**: Imputação com medidas robustas (mediana) + validação clínica\n",
    "\n",
    "| Variável | Missing% | Separabilidade | p-value | Justificativa Médica |\n",
    "|----------|----------|----------------|---------|---------------------|\n",
    "| **Temp** | 66.2% | 0.326 | < 0.001 | Temperatura corporal: indicador direto de resposta inflamatória |\n",
    "\n",
    "**IMPUTAÇÃO Específica** (3 variáveis - Separabilidade 0.16-0.3 ou Separabilidade>0.3 e Missing>90%)  \n",
    "\n",
    "**Estratégia**: Técnicas sofisticadas (KNN, regressão) devido à considerável relevância clínica \n",
    "\n",
    "| Variável | Missing% | Separabilidade | p-value | Justificativa Médica |\n",
    "|----------|----------|----------------|---------|---------------------|\n",
    "| **BUN** | 93.1% | 0.328 | < 0.001 | Função renal: biomarcador de disfunção orgânica na sepsis |\n",
    "| **Platelets** | 94.1% | 0.189 | < 0.001 | Coagulação: trombocitopenia marca disfunção hemostática |\n",
    "| **WBC** | 93.6% | 0.166 | < 0.001 | Sistema imune: resposta leucocitária à infecção |\n",
    "\n",
    "**REMOVIDAS** (25 variáveis - Separabilidade < 0.16)\n",
    "\n",
    "**Critério duplo**: Baixa discriminação + Alto missing (>60%)\n",
    "\n",
    "**Destaques das remoções:**\n",
    "- **Age**: 0.0% missing, Sep: 0.000, p-value: 0.437 (única exceção por baixa discriminação)\n",
    "- **24 variáveis** com >80% missing + separabilidade < 0.16\n",
    "- **Maior redução**: TroponinI (99.1% missing), Bilirubin_direct (99.8% missing)\n",
    "\n",
    "VALIDAÇÃO ESTATÍSTICA FINAL\n",
    "\n",
    "**Testes Aplicados:**\n",
    "- **Mann-Whitney U**: Para variáveis numéricas (não-paramétrico)\n",
    "- **Qui-quadrado**: Para variáveis categóricas\n",
    "- **Informação Mútua**: Medida de dependência entre variáveis\n",
    "\n",
    "**Significância dos Testes:**\n",
    "- **Variáveis numéricas significativas**: 13/14 (p < 0.05)\n",
    "- **Variáveis categóricas significativas**: 3/3 (p < 0.001)  \n",
    "- **Taxa de significância geral**: 94.1% (16/17 variáveis)\n",
    "\n",
    "**Estratégias de Tratamento Definidas:**\n",
    "- **Imputação cuidadosa**: 2 variáveis de alta relevância\n",
    "- **Imputação específica**: 6 variáveis de relevância moderada\n",
    "- **Manutenção**: 13 variáveis com baixo missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995bf50",
   "metadata": {},
   "source": [
    "## 4. Limpeza dos Dados\n",
    "\n",
    "**Objetivo:** Corrigir ou remover dados inconsistentes, duplicados ou ausentes através de estratégias específicas para cada tipo de variável.\n",
    "\n",
    "**Estratégias por tipo de missing:**\n",
    "* Missing < 20%: Imputação simples (mediana/moda)\n",
    "* Missing >= 20%: Imputação baseada em modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34539dc5",
   "metadata": {},
   "source": [
    "### 4.1 Detecção e Remoção de Duplicatas\n",
    "\n",
    "Identificação de registros duplicados exatos e tratamento adequado considerando a natureza temporal dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44273b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECÇÃO DE DUPLICATAS:\n",
      "Dataset atual: (1241768, 16)\n",
      "Duplicatas exatas encontradas: 32,571\n",
      "Remover 32,571 duplicatas exatas\n",
      "Dataset após remoção: (1209197, 16)\n",
      "\n",
      "PROPORÇÃO DAS CLASSES APÓS REMOÇÃO:\n",
      "Antes da remoção de duplicatas:\n",
      "  • SepsisLabel = 0: 1,219,435 (0.9820)\n",
      "  • SepsisLabel = 1: 22,333 (0.0180)\n",
      "Após remoção de duplicatas:\n",
      "  • SepsisLabel = 0: 1,187,303 (0.9819)\n",
      "  • SepsisLabel = 1: 21,894 (0.0181)\n",
      "\n",
      "IMPACTO NA PROPORÇÃO:\n",
      "  • Mudança SepsisLabel = 0: -0.0001\n",
      "  • Mudança SepsisLabel = 1: +0.0001\n",
      "Dataset limpo final: (1209197, 16)\n"
     ]
    }
   ],
   "source": [
    "# Verificar duplicatas no dataset selecionado\n",
    "print(\"DETECÇÃO DE DUPLICATAS:\")\n",
    "print(f\"Dataset atual: {X_train_selected.shape}\")\n",
    "\n",
    "# Verificar duplicatas exatas (todas as colunas)\n",
    "duplicatas_exatas = X_train_selected.duplicated().sum()\n",
    "print(f\"Duplicatas exatas encontradas: {duplicatas_exatas:,}\")\n",
    "\n",
    "print(f\"Remover {duplicatas_exatas:,} duplicatas exatas\")\n",
    "X_train_cleaned = X_train_selected.drop_duplicates()\n",
    "y_train_cleaned = y_train.loc[X_train_cleaned.index]\n",
    "print(f\"Dataset após remoção: {X_train_cleaned.shape}\")\n",
    "\n",
    "# Verificar proporção das classes após remoção de duplicatas\n",
    "print(f\"\\nPROPORÇÃO DAS CLASSES APÓS REMOÇÃO:\")\n",
    "print(\"Antes da remoção de duplicatas:\")\n",
    "original_counts = y_train.value_counts()\n",
    "original_props = y_train.value_counts(normalize=True)\n",
    "print(f\"  • SepsisLabel = 0: {original_counts[0]:,} ({original_props[0]:.4f})\")\n",
    "print(f\"  • SepsisLabel = 1: {original_counts[1]:,} ({original_props[1]:.4f})\")\n",
    "\n",
    "print(\"Após remoção de duplicatas:\")\n",
    "cleaned_counts = y_train_cleaned.value_counts()\n",
    "cleaned_props = y_train_cleaned.value_counts(normalize=True)\n",
    "print(f\"  • SepsisLabel = 0: {cleaned_counts[0]:,} ({cleaned_props[0]:.4f})\")\n",
    "print(f\"  • SepsisLabel = 1: {cleaned_counts[1]:,} ({cleaned_props[1]:.4f})\")\n",
    "\n",
    "# Calcular impacto na proporção\n",
    "prop_change_0 = cleaned_props[0] - original_props[0]\n",
    "prop_change_1 = cleaned_props[1] - original_props[1]\n",
    "print(f\"\\nIMPACTO NA PROPORÇÃO:\")\n",
    "print(f\"  • Mudança SepsisLabel = 0: {prop_change_0:+.4f}\")\n",
    "print(f\"  • Mudança SepsisLabel = 1: {prop_change_1:+.4f}\")\n",
    "\n",
    "print(f\"Dataset limpo final: {X_train_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb71fe",
   "metadata": {},
   "source": [
    "### 4.2 Tratamento e Análise de Outliers\n",
    "\n",
    "A ideia é tentar preservar os outliers visto que eles se demonstraram relevantes para a identificação de instâncias com SepsisLabel=1 na Análise Exploratória.\n",
    "Vamos apenas deixar algumas variáveis mais genéricas e conhecidas mais consistentes e fazer uma análise geral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c1182fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OUTLIERS DETECTADOS:\n",
      "Variável     Clínicos  IQR      Z-score  N_total  Range Original       Range Tratado       \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hour         0         54,951   27,883   1,209,197 0.00-335.00          0.00-335.00         \n",
      "HR           0         11,203   5,502    1,119,122 20.00-280.00         20.00-280.00        \n",
      "O2Sat        0         19,905   8,912    1,079,707 20.00-100.00         20.00-100.00        \n",
      "Temp         0         5,223    3,392    419,945  23.00-50.00          23.00-50.00         \n",
      "SBP          0         12,748   6,046    1,060,857 20.00-300.00         20.00-300.00        \n",
      "MAP          0         17,543   8,057    1,087,236 20.00-300.00         20.00-300.00        \n",
      "DBP          0         13,033   6,560    852,691  20.00-300.00         20.00-300.00        \n",
      "Resp         0         22,208   10,343   1,051,178 1.00-100.00          1.00-100.00         \n",
      "BUN          0         7,039    2,063    85,439   1.00-268.00          1.00-268.00         \n",
      "WBC          0         2,767    514      79,613   0.10-440.00          0.10-440.00         \n",
      "Platelets    0         2,369    1,070    73,790   2.00-2322.00         2.00-2322.00        \n",
      "Gender       0         0        0        1,209,197 0.00-1.00            0.00-1.00           \n",
      "Unit1        0         0        0        737,114  0.00-1.00            0.00-1.00           \n",
      "Unit2        0         0        0        737,114  0.00-1.00            0.00-1.00           \n",
      "HospAdmTime  0         161,629  17,170   1,209,191 -5366.86-23.99       -5366.86-23.99      \n",
      "ICULOS       0         54,672   27,811   1,209,197 1.00-336.00          1.00-336.00         \n",
      "\n",
      "Total de Caps Aplicados: 0\n"
     ]
    }
   ],
   "source": [
    "# Tratamento de outliers para variáveis numéricas\n",
    "from scipy import stats\n",
    "\n",
    "# Separar variáveis numéricas do dataset limpo\n",
    "numeric_cols = X_train_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Definir limites um pouco mais realistas para algumas variáveis \n",
    "# Considerando registros de outros casos extremos e do próprio dataset\n",
    "# Propósito de deixar os dados mais consistentes\n",
    "clinical_limits = {\n",
    "    'HR': (20, 250),           # Batimentos cardíacos: 20-250 bpm\n",
    "    'Temp': (28, 42),          # Temperatura: 28-42°C\n",
    "    'Hour': (0, 336),          # Hora na UTI: 1-336h (14 dias)\n",
    "    'ICULOS': (0, 336),        # Tempo UTI: 1-336h\n",
    "    'HospAdmTime': (0, 24),   # Tempo hospital: 0 a 24h\n",
    "}\n",
    "\n",
    "outliers_summary = {}\n",
    "X_train_outliers_treated = X_train_cleaned.copy()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    data = X_train_outliers_treated[col].dropna()\n",
    "        \n",
    "    # Cap do Range (quando aplicável) -- Aplicação dos limites será feita após a primeira iteração na modelagem se necessário\n",
    "    clinical_outliers = 0\n",
    "    # if col in clinical_limits:\n",
    "    #     min_val, max_val = clinical_limits[col]\n",
    "    #     clinical_mask = (data < min_val) | (data > max_val)\n",
    "    #     clinical_outliers = clinical_mask.sum()\n",
    "    #    \n",
    "    #     # Aplicar capping\n",
    "    #     X_train_outliers_treated.loc[X_train_outliers_treated[col] < min_val, col] = min_val\n",
    "    #     X_train_outliers_treated.loc[X_train_outliers_treated[col] > max_val, col] = max_val\n",
    "    \n",
    "    # Análise do IQR\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    iqr_outliers = ((data < lower_bound) | (data > upper_bound)).sum()\n",
    "    \n",
    "    # Análise do Z-score (outliers > 3 desvios padrão)\n",
    "    z_scores = np.abs(stats.zscore(data))\n",
    "    zscore_outliers = (z_scores > 3).sum()\n",
    "    \n",
    "    outliers_summary[col] = {\n",
    "        'clinical': clinical_outliers,\n",
    "        'iqr': iqr_outliers,\n",
    "        'zscore': zscore_outliers,\n",
    "        'total_values': len(data),\n",
    "        'range_original': (data.min(), data.max()),\n",
    "        'range_treated': (X_train_outliers_treated[col].min(), X_train_outliers_treated[col].max())\n",
    "    }\n",
    "\n",
    "# Exibir resumo dos outliers\n",
    "print(f\"\\nOUTLIERS DETECTADOS:\")\n",
    "print(f\"{'Variável':<12} {'Clínicos':<9} {'IQR':<8} {'Z-score':<8} {'N_total':<8} {'Range Original':<20} {'Range Tratado':<20}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for col, summary in outliers_summary.items():\n",
    "    clinical = summary['clinical']\n",
    "    iqr = summary['iqr'] \n",
    "    zscore = summary['zscore']\n",
    "    total = summary['total_values']\n",
    "    range_orig = f\"{summary['range_original'][0]:.2f}-{summary['range_original'][1]:.2f}\"\n",
    "    range_treat = f\"{summary['range_treated'][0]:.2f}-{summary['range_treated'][1]:.2f}\"\n",
    "\n",
    "    print(f\"{col:<12} {clinical:<9,} {iqr:<8,} {zscore:<8,} {total:<8,} {range_orig:<20} {range_treat:<20}\")\n",
    "\n",
    "# Estatísticas finais\n",
    "total_clinical_corrections = sum(summary['clinical'] for summary in outliers_summary.values())\n",
    "print(f\"\\nTotal de Caps Aplicados: {total_clinical_corrections:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe2b1d",
   "metadata": {},
   "source": [
    "### 4.3 Estratégias de Imputação\n",
    "\n",
    "Implementação de diferentes técnicas de imputação baseadas no tipo de variável e percentual de missing values.\n",
    "\n",
    "**OBSERVAÇÃO:**\n",
    "As seções `4.3.1` e `4.3.2` precisam ser executadas em ordem e são necessárias para que as demais seções funcionem. Porém `4.3.3`, `4.3.4`, `4.3.5` podem ser executadas em qualquer ordem após executar `4.3.1` e `4.3.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc5442",
   "metadata": {},
   "source": [
    "#### 4.3.1 Imputação para Variáveis com Baixo Missing (<20%)\n",
    "\n",
    "Aplicação de imputação simples usando medidas centrais apropriadas para cada tipo de variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d173ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variáveis para imputação simples:\n",
      "  • HR: 90,075 valores (7.4%)\n",
      "  • O2Sat: 129,490 valores (10.7%)\n",
      "  • SBP: 148,340 valores (12.3%)\n",
      "  • MAP: 121,961 valores (10.1%)\n",
      "  • Resp: 158,019 valores (13.1%)\n",
      "  • HospAdmTime: 6 valores (0.0%)\n",
      "\n",
      "VERIFICAÇÃO PÓS-IMPUTAÇÃO:\n",
      "  • HR: 0 valores missing restantes\n",
      "  • O2Sat: 0 valores missing restantes\n",
      "  • SBP: 0 valores missing restantes\n",
      "  • MAP: 0 valores missing restantes\n",
      "  • Resp: 0 valores missing restantes\n",
      "  • HospAdmTime: 0 valores missing restantes\n",
      "\n",
      "Total de valores imputados (simples): 647,891\n",
      "Missing values restantes: 5,478,673 (28.32%)\n"
     ]
    }
   ],
   "source": [
    "# Imputação simples para variáveis com baixo missing (<20%)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "X_train_simple_imputed = X_train_outliers_treated.copy()\n",
    "\n",
    "# Identificar variáveis com baixo missing (<20%)\n",
    "low_missing_vars = []\n",
    "missing_info = {}\n",
    "\n",
    "print(f\"\\nVariáveis para imputação simples:\")\n",
    "for col in X_train_simple_imputed.columns:\n",
    "    missing_count = X_train_simple_imputed[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(X_train_simple_imputed)) * 100\n",
    "    missing_info[col] = missing_pct\n",
    "    \n",
    "    if missing_pct < 20 and missing_pct > 0:\n",
    "        low_missing_vars.append(col)\n",
    "        print(f\"  • {col}: {missing_count:,} valores ({missing_pct:.1f}%)\") \n",
    "\n",
    "\n",
    "# for var in low_missing_vars:\n",
    "#     missing_count = X_train_simple_imputed[var].isnull().sum()\n",
    "#     missing_pct = missing_info[var]\n",
    "#     print(f\"  • {var}: {missing_count:,} valores ({missing_pct:.1f}%)\")\n",
    "\n",
    "\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "X_train_simple_imputed[low_missing_vars] = numeric_imputer.fit_transform(\n",
    "    X_train_simple_imputed[low_missing_vars]\n",
    ")\n",
    "\n",
    "\n",
    "# Verificar se imputação foi bem-sucedida\n",
    "print(f\"\\nVERIFICAÇÃO PÓS-IMPUTAÇÃO:\")\n",
    "for var in low_missing_vars:\n",
    "    remaining_missing = X_train_simple_imputed[var].isnull().sum()\n",
    "    print(f\"  • {var}: {remaining_missing} valores missing restantes\")\n",
    "\n",
    "total_imputed = sum(missing_info[var] * len(X_train_simple_imputed) / 100 for var in low_missing_vars)\n",
    "print(f\"\\nTotal de valores imputados (simples): {total_imputed:,.0f}\")\n",
    "\n",
    "# Resumo do missing restante\n",
    "remaining_missing = X_train_simple_imputed.isnull().sum().sum()\n",
    "total_values = X_train_simple_imputed.size\n",
    "missing_pct_remaining = (remaining_missing / total_values) * 100\n",
    "\n",
    "print(f\"Missing values restantes: {remaining_missing:,} ({missing_pct_remaining:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a48589",
   "metadata": {},
   "source": [
    "#### 4.3.2 Separando as Demais Variáveis para Imputação Avançada \n",
    "\n",
    "Para uso de técnicas mais sofisticadas como KNN Imputer ou imputação baseada em modelos para variáveis clinicamente importantes, vamos antes definir as variáveis que ainda possuem valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0004175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANÁLISE DINÂMICA DE MISSING VALUES:\n",
      "==================================================\n",
      "\n",
      "CLASSIFICAÇÃO POR ESTRATÉGIA DE IMPUTAÇÃO:\n",
      "Critério: Unit1/Unit2 = Regressão Logística | <40% = Regressão Linear | ≥40% = Híbrida\n",
      "-------------------------------------------------------------------------------------\n",
      "DBP             29.5    % ( 356,506 valores)\n",
      "Unit1           39.0    % ( 472,083 valores)\n",
      "Unit2           39.0    % ( 472,083 valores)\n",
      "Temp            65.3    % ( 789,252 valores)\n",
      "BUN             92.9    % (1,123,758 valores)\n",
      "WBC             93.4    % (1,129,584 valores)\n",
      "Platelets       93.9    % (1,135,407 valores)\n",
      "\n",
      "RESUMO DAS ESTRATÉGIAS:\n",
      "  • Regressão Logística (categóricas Unit1/Unit2): 2 variáveis\n",
      "  • Regressão Linear Simples (<40% missing numéricas): 1 variáveis\n",
      "  • Estratégia Híbrida (≥40% missing numéricas): 4 variáveis\n",
      "\n",
      "Variáveis para Regressão Logística: ['Unit1', 'Unit2']\n",
      "Variáveis para Regressão Linear: ['DBP']\n",
      "Variáveis para Estratégia Híbrida: ['Temp', 'BUN', 'WBC', 'Platelets']\n",
      "\n",
      "Variáveis preditoras selecionadas: ['Hour', 'HR', 'O2Sat', 'SBP', 'MAP', 'Resp', 'Gender', 'HospAdmTime', 'ICULOS']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_advanced_imputed = X_train_simple_imputed.copy()\n",
    "\n",
    "# ESTRATÉGIA DINÂMICA BASEADA EM MISSING ATUAL\n",
    "print(f\"\\nANÁLISE DINÂMICA DE MISSING VALUES:\")\n",
    "print(f\"=\"*50)\n",
    "\n",
    "# Identificar todas as variáveis com missing\n",
    "vars_with_missing = []\n",
    "for col in X_train_advanced_imputed.columns:\n",
    "    missing_count = X_train_advanced_imputed[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(X_train_advanced_imputed)) * 100\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        vars_with_missing.append({\n",
    "            'variavel': col,\n",
    "            'missing_count': missing_count,\n",
    "            'missing_pct': missing_pct\n",
    "        })\n",
    "\n",
    "# Separar por estratégia baseada em 40% de missing e tipo de variável\n",
    "logistic_regression_vars = []  # Unit1, Unit2: Regressão Logística (categóricas)\n",
    "linear_regression_vars = []    # < 40% missing: Regressão Linear simples (numéricas)\n",
    "hybrid_strategy_vars = []      # >= 40% missing: Estratégia Híbrida KNN + Regressão\n",
    "\n",
    "print(f\"\\nCLASSIFICAÇÃO POR ESTRATÉGIA DE IMPUTAÇÃO:\")\n",
    "print(f\"Critério: Unit1/Unit2 = Regressão Logística | <40% = Regressão Linear | ≥40% = Híbrida\")\n",
    "print(f\"-\" * 85)\n",
    "\n",
    "for var_info in sorted(vars_with_missing, key=lambda x: x['missing_pct']):\n",
    "    var = var_info['variavel']\n",
    "    missing_pct = var_info['missing_pct']\n",
    "    missing_count = var_info['missing_count']\n",
    "    \n",
    "    print(f\"{var:<15} {missing_pct:<8.1f}% ({missing_count:>8,} valores)\")\n",
    "    \n",
    "    # Separar Unit1 e Unit2 para regressão logística\n",
    "    if var in ['Unit1', 'Unit2']:\n",
    "        logistic_regression_vars.append(var)\n",
    "    elif missing_pct < 40:\n",
    "        linear_regression_vars.append(var)\n",
    "    else:\n",
    "        hybrid_strategy_vars.append(var)\n",
    "\n",
    "print(f\"\\nRESUMO DAS ESTRATÉGIAS:\")\n",
    "print(f\"  • Regressão Logística (categóricas Unit1/Unit2): {len(logistic_regression_vars)} variáveis\")\n",
    "print(f\"  • Regressão Linear Simples (<40% missing numéricas): {len(linear_regression_vars)} variáveis\")\n",
    "print(f\"  • Estratégia Híbrida (≥40% missing numéricas): {len(hybrid_strategy_vars)} variáveis\")\n",
    "\n",
    "print(f\"\\nVariáveis para Regressão Logística: {logistic_regression_vars}\")\n",
    "print(f\"Variáveis para Regressão Linear: {linear_regression_vars}\")\n",
    "print(f\"Variáveis para Estratégia Híbrida: {hybrid_strategy_vars}\")\n",
    "\n",
    "# Selecionar variáveis preditoras (sem missing ou baixo missing)\n",
    "all_vars_to_impute = logistic_regression_vars + linear_regression_vars + hybrid_strategy_vars\n",
    "predictor_vars = []\n",
    "for col in X_train_advanced_imputed.select_dtypes(include=[np.number]).columns:\n",
    "    missing_pct = (X_train_advanced_imputed[col].isnull().sum() / len(X_train_advanced_imputed)) * 100\n",
    "    if missing_pct == 0 or (missing_pct < 20 and col not in all_vars_to_impute):\n",
    "        predictor_vars.append(col)\n",
    "\n",
    "print(f\"\\nVariáveis preditoras selecionadas: {predictor_vars}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd99f40",
   "metadata": {},
   "source": [
    "#### 4.3.3 Regressão Logística para Categóricas\n",
    "\n",
    "Aqui aplicaremos o modelo de regressão logistica para imputação de Unit1 e Unit2. Mantendo a coerência do Dataset onde é mandatório que Unit1 + Unit2 = 1 para todas as instâncias (Paciente só pode ir para um dos tipos de UTI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29d7854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit1 missing: 472,083 valores\n",
      "Unit2 missing: 472,083 valores\n",
      "Ambas missing: 472,083 valores\n",
      "\n",
      "ETAPA 1: Regressão Logística para Unit1\n",
      "ETAPA 2: Unit2 como complemento de Unit1\n",
      "Unit1 imputado: 472,083 valores\n",
      "Unit2 imputado: 472,083 valores (complemento)\n",
      "Distribuição Unit1 imputada: 0=169,833 | 1=302,250\n",
      "Distribuição Unit2 imputada: 0=302,250 | 1=169,833\n",
      "Verificação Unit1 + Unit2 = 1: ✓ VÁLIDA\n",
      "\n",
      "VERIFICAÇÃO PÓS-IMPUTAÇÃO LOGÍSTICA:\n",
      "  • Unit1: 0 valores missing restantes\n",
      "    Distribuição final: {0.0: np.int64(544591), 1.0: np.int64(664606)}\n",
      "  • Unit2: 0 valores missing restantes\n",
      "    Distribuição final: {0.0: np.int64(664606), 1.0: np.int64(544591)}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ETAPA 1: REGRESSÃO LOGÍSTICA PARA VARIÁVEIS CATEGÓRICAS (Unit1, Unit2)\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Identificar valores missing\n",
    "unit1_missing_mask = X_train_advanced_imputed['Unit1'].isnull()\n",
    "unit2_missing_mask = X_train_advanced_imputed['Unit2'].isnull()\n",
    "both_missing_mask = unit1_missing_mask & unit2_missing_mask\n",
    "\n",
    "unit1_missing_count = unit1_missing_mask.sum()\n",
    "unit2_missing_count = unit2_missing_mask.sum()\n",
    "both_missing_count = both_missing_mask.sum()\n",
    "\n",
    "print(f\"Unit1 missing: {unit1_missing_count:,} valores\")\n",
    "print(f\"Unit2 missing: {unit2_missing_count:,} valores\") \n",
    "print(f\"Ambas missing: {both_missing_count:,} valores\")\n",
    "\n",
    "print(f\"\\nETAPA 1: Regressão Logística para Unit1\")\n",
    "\n",
    "# Preparar dados para treino (onde Unit1 não é missing)\n",
    "complete_mask = ~X_train_advanced_imputed['Unit1'].isnull()\n",
    "training_size = min(100000, complete_mask.sum())\n",
    "\n",
    "training_indices = np.random.choice(\n",
    "    X_train_advanced_imputed[complete_mask].index,\n",
    "    size=training_size,\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "# Features numéricas para predição (excluir Unit1/Unit2)\n",
    "numeric_predictors = [col for col in predictor_vars if col not in logistic_regression_vars]\n",
    "\n",
    "# Treinar modelo logístico para Unit1\n",
    "X_features = X_train_advanced_imputed.loc[training_indices, numeric_predictors]\n",
    "y_target = X_train_advanced_imputed.loc[training_indices, 'Unit1']\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logistic_model.fit(X_features, y_target)\n",
    "\n",
    "# Prever Unit1 para registros missing\n",
    "X_missing_features = X_train_advanced_imputed.loc[both_missing_mask, numeric_predictors]\n",
    "predicted_unit1_proba = logistic_model.predict_proba(X_missing_features)[:, 1]\n",
    "predicted_unit1 = (predicted_unit1_proba > 0.5).astype(int)\n",
    "\n",
    "print(f\"ETAPA 2: Unit2 como complemento de Unit1\")\n",
    "# Unit2 como complemento lógico de Unit1\n",
    "predicted_unit2 = 1 - predicted_unit1\n",
    "\n",
    "# Aplicar imputações\n",
    "X_train_advanced_imputed.loc[both_missing_mask, 'Unit1'] = predicted_unit1\n",
    "X_train_advanced_imputed.loc[both_missing_mask, 'Unit2'] = predicted_unit2\n",
    "\n",
    "print(f\"Unit1 imputado: {both_missing_count:,} valores\")\n",
    "print(f\"Unit2 imputado: {both_missing_count:,} valores (complemento)\")\n",
    "\n",
    "# Estatísticas da imputação\n",
    "unit1_0_count = (predicted_unit1 == 0).sum()\n",
    "unit1_1_count = (predicted_unit1 == 1).sum()\n",
    "print(f\"Distribuição Unit1 imputada: 0={unit1_0_count:,} | 1={unit1_1_count:,}\")\n",
    "print(f\"Distribuição Unit2 imputada: 0={unit1_1_count:,} | 1={unit1_0_count:,}\")\n",
    "\n",
    "# Verificar relação complementar\n",
    "unit1_final = X_train_advanced_imputed.loc[both_missing_mask, 'Unit1']\n",
    "unit2_final = X_train_advanced_imputed.loc[both_missing_mask, 'Unit2']\n",
    "sum_check = (unit1_final + unit2_final == 1).all()\n",
    "print(f\"Verificação Unit1 + Unit2 = 1: {'✓ VÁLIDA' if sum_check else '✗ INVÁLIDA'}\")\n",
    "    \n",
    "# Verificação pós-imputação logística\n",
    "print(f\"\\nVERIFICAÇÃO PÓS-IMPUTAÇÃO LOGÍSTICA:\")\n",
    "for var in logistic_regression_vars:\n",
    "    if var in X_train_advanced_imputed.columns:\n",
    "        remaining_missing = X_train_advanced_imputed[var].isnull().sum()\n",
    "        print(f\"  • {var}: {remaining_missing} valores missing restantes\")\n",
    "        \n",
    "        # Distribuição final\n",
    "        if remaining_missing == 0:\n",
    "            value_counts = X_train_advanced_imputed[var].value_counts().sort_index()\n",
    "            print(f\"    Distribuição final: {dict(value_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8198a75",
   "metadata": {},
   "source": [
    "#### 4.3.4 Regressão Linear para <40% de Missing\n",
    "Aplicar regressão linear para variáveis numéricas com <40% de missing (DBP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f67684c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETAPA 2: REGRESSÃO LINEAR SIMPLES (< 40% missing)\n",
      "\n",
      "\n",
      "---------- IMPUTANDO DBP (Regressão Simples) ----------\n",
      "Missing: 356,506 valores (29.5%)\n",
      "Regressão aplicada: 356,506 valores imputados\n",
      "Valores restantes missing: 0\n",
      "Range imputado: [3.53, 257.85]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ETAPA 2: REGRESSÃO LINEAR SIMPLES (< 40% missing)\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(f\"ETAPA 2: REGRESSÃO LINEAR SIMPLES (< 40% missing)\\n\")\n",
    "\n",
    "for target_var in linear_regression_vars:\n",
    "    print(f\"\\n---------- IMPUTANDO {target_var} (Regressão Simples) ----------\")\n",
    "    \n",
    "    # Identificar valores missing\n",
    "    missing_mask = X_train_advanced_imputed[target_var].isnull()\n",
    "    total_missing = missing_mask.sum()\n",
    "    missing_pct = (total_missing / len(X_train_advanced_imputed)) * 100\n",
    "    print(f\"Missing: {total_missing:,} valores ({missing_pct:.1f}%)\")\n",
    "\n",
    "    # Preparar dados para regressão\n",
    "    complete_mask = ~X_train_advanced_imputed[target_var].isnull()\n",
    "    training_size = min(100000, complete_mask.sum())  # Até 100k para treino\n",
    "    \n",
    "    training_indices = np.random.choice(\n",
    "        X_train_advanced_imputed[complete_mask].index,\n",
    "        size=training_size,\n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    # Features e target para treino\n",
    "    X_features = X_train_advanced_imputed.loc[training_indices, predictor_vars]\n",
    "    y_target = X_train_advanced_imputed.loc[training_indices, target_var]\n",
    "    \n",
    "    # Treinar modelo de regressão\n",
    "    reg_model = LinearRegression()\n",
    "    reg_model.fit(X_features, y_target)\n",
    "    \n",
    "    # Prever todos os valores missing\n",
    "    X_missing_features = X_train_advanced_imputed.loc[missing_mask, predictor_vars]\n",
    "    predicted_values = reg_model.predict(X_missing_features)\n",
    "    \n",
    "    # Aplicar imputação\n",
    "    X_train_advanced_imputed.loc[missing_mask, target_var] = predicted_values\n",
    "    \n",
    "    # Verificar resultado\n",
    "    final_missing = X_train_advanced_imputed[target_var].isnull().sum()\n",
    "    print(f\"Regressão aplicada: {total_missing:,} valores imputados\")\n",
    "    print(f\"Valores restantes missing: {final_missing}\")\n",
    "    print(f\"Range imputado: [{predicted_values.min():.2f}, {predicted_values.max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55862465",
   "metadata": {},
   "source": [
    "#### 4.3.5 KNNImputer + Regressão Linear para >=40% de Missing\n",
    "Aplicação do KNNImputer com 3 vizinhos para amostra de ~5% dos valores da coluna e Regressão Linear para os outros ~95%.\n",
    "\n",
    "Esse código abaixo demora em torno de 5-6min para executar. Tomar ciência disso antes rodar a célula de código abaixo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d936d4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== IMPUTANDO Temp ==========\n",
      "Total de valores missing: 789,252\n",
      "\n",
      "ETAPA 1 - KNN Imputer (39,462 valores - 5%)\n",
      "  KNN aplicado com sucesso: 39,462 valores\n",
      "\n",
      "ETAPA 2 - Regressão Linear (749,790 valores - 95%)\n",
      "  Regressão aplicada com sucesso: 749,790 valores\n",
      "  Valores missing restantes: 0\n",
      "  Imputação híbrida completa para Temp!\n",
      "\n",
      "========== IMPUTANDO BUN ==========\n",
      "Total de valores missing: 1,123,758\n",
      "\n",
      "ETAPA 1 - KNN Imputer (50,000 valores - 5%)\n",
      "  KNN aplicado com sucesso: 50,000 valores\n",
      "\n",
      "ETAPA 2 - Regressão Linear (1,073,758 valores - 95%)\n",
      "  Regressão aplicada com sucesso: 1,073,758 valores\n",
      "  Valores missing restantes: 0\n",
      "  Imputação híbrida completa para BUN!\n",
      "\n",
      "========== IMPUTANDO WBC ==========\n",
      "Total de valores missing: 1,129,584\n",
      "\n",
      "ETAPA 1 - KNN Imputer (50,000 valores - 5%)\n",
      "  KNN aplicado com sucesso: 50,000 valores\n",
      "\n",
      "ETAPA 2 - Regressão Linear (1,079,584 valores - 95%)\n",
      "  Regressão aplicada com sucesso: 1,079,584 valores\n",
      "  Valores missing restantes: 0\n",
      "  Imputação híbrida completa para WBC!\n",
      "\n",
      "========== IMPUTANDO Platelets ==========\n",
      "Total de valores missing: 1,135,407\n",
      "\n",
      "ETAPA 1 - KNN Imputer (50,000 valores - 5%)\n",
      "  KNN aplicado com sucesso: 50,000 valores\n",
      "\n",
      "ETAPA 2 - Regressão Linear (1,085,407 valores - 95%)\n",
      "  Regressão aplicada com sucesso: 1,085,407 valores\n",
      "  Valores missing restantes: 0\n",
      "  Imputação híbrida completa para Platelets!\n",
      "Imputação Concluída!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ETAPA 3: ESTRATÉGIA HÍBRIDA (≥ 40% missing)  \n",
    "# =============================================================================\n",
    "\n",
    "# Imputação avançada híbrida: KNN + Regressão Linear\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Implementar estratégia híbrida para cada variável\n",
    "for target_var in hybrid_strategy_vars:\n",
    "    print(f\"\\n========== IMPUTANDO {target_var} ==========\")\n",
    "    \n",
    "    # Identificar valores missing\n",
    "    missing_mask = X_train_advanced_imputed[target_var].isnull()\n",
    "    total_missing = missing_mask.sum()\n",
    "    print(f\"Total de valores missing: {total_missing:,}\")\n",
    "    \n",
    "    # ETAPA 1: Imputação com KNN (5% dos missing values - otimizado)\n",
    "    knn_sample_size = min(50000, int(total_missing * 0.05))  # Máximo 50k valores\n",
    "    print(f\"\\nETAPA 1 - KNN Imputer ({knn_sample_size:,} valores - 5%)\")\n",
    "    \n",
    "    # Selecionar amostra aleatória dos índices missing para KNN\n",
    "    missing_indices = X_train_advanced_imputed[missing_mask].index\n",
    "    knn_indices = np.random.choice(missing_indices, size=knn_sample_size, replace=False)\n",
    "    \n",
    "    # Preparar subset para KNN (incluir valores não-missing para treino)\n",
    "    mask_not_missing = ~X_train_advanced_imputed[target_var].isnull()\n",
    "    knn_training_size = min(10000, mask_not_missing.sum())  # Reduzir para 10k treino\n",
    "    training_indices = np.random.choice(\n",
    "        X_train_advanced_imputed[mask_not_missing].index, \n",
    "        size=knn_training_size, \n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    # Combinar índices para KNN: treino + amostra para imputação\n",
    "    knn_all_indices = np.concatenate([training_indices, knn_indices])\n",
    "    knn_subset = X_train_advanced_imputed.loc[knn_all_indices, predictor_vars + [target_var]].copy()\n",
    "    \n",
    "    # Aplicar KNN apenas no subset\n",
    "    knn_imputer = KNNImputer(n_neighbors=3, weights='uniform')\n",
    "    knn_imputed = knn_imputer.fit_transform(knn_subset)\n",
    "    \n",
    "    # Extrair valores imputados para a variável alvo\n",
    "    target_col_idx = list(knn_subset.columns).index(target_var)\n",
    "    knn_imputed_values = knn_imputed[-knn_sample_size:, target_col_idx]\n",
    "    \n",
    "    # Atualizar valores no dataset\n",
    "    X_train_advanced_imputed.loc[knn_indices, target_var] = knn_imputed_values\n",
    "    print(f\"  KNN aplicado com sucesso: {knn_sample_size:,} valores\")\n",
    "    \n",
    "    # ETAPA 2: Imputação com Regressão Linear (95% restante)\n",
    "    remaining_missing_mask = X_train_advanced_imputed[target_var].isnull()\n",
    "    remaining_missing_count = remaining_missing_mask.sum()\n",
    "    print(f\"\\nETAPA 2 - Regressão Linear ({remaining_missing_count:,} valores - 95%)\")\n",
    "    \n",
    "    # Preparar dados para regressão (usar todos os dados completos)\n",
    "    complete_mask = ~X_train_advanced_imputed[target_var].isnull()\n",
    "    reg_training_size = min(50000, complete_mask.sum())\n",
    "    \n",
    "    reg_training_indices = np.random.choice(\n",
    "        X_train_advanced_imputed[complete_mask].index,\n",
    "        size=reg_training_size,\n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    # Features e target para treino\n",
    "    X_features = X_train_advanced_imputed.loc[reg_training_indices, predictor_vars]\n",
    "    y_target = X_train_advanced_imputed.loc[reg_training_indices, target_var]\n",
    "    \n",
    "    # Treinar modelo de regressão\n",
    "    reg_model = LinearRegression()\n",
    "    reg_model.fit(X_features, y_target)\n",
    "    \n",
    "    # Prever valores restantes\n",
    "    X_missing_features = X_train_advanced_imputed.loc[remaining_missing_mask, predictor_vars]\n",
    "    predicted_values = reg_model.predict(X_missing_features)\n",
    "    \n",
    "    X_train_advanced_imputed.loc[remaining_missing_mask, target_var] = predicted_values\n",
    "    print(f\"  Regressão aplicada com sucesso: {remaining_missing_count:,} valores\")\n",
    "            \n",
    "    \n",
    "    # Verificar se imputação foi completa\n",
    "    final_missing = X_train_advanced_imputed[target_var].isnull().sum()\n",
    "    print(f\"  Valores missing restantes: {final_missing}\")\n",
    "    print(f\"  Imputação híbrida completa para {target_var}!\")\n",
    "\n",
    "print(\"Imputação Concluída!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f67ae7",
   "metadata": {},
   "source": [
    "#### 4.3.6 (Não Rodar por agora) Aplicando os Limites Pós-Imputação com 2 modelos\n",
    "\n",
    "Definimos o range de valores segundo a análise feita na seção 4.2 para não permitir que nenhum outlier imputado ultrapasse o intervalo observado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8deb201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APLICAÇÃO DE CAPS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"APLICAÇÃO DE CAPS PÓS-IMPUTAÇÃO\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# Definir ranges tratados do outliers_summary (Seção 4.2)\n",
    "caps_ranges = {\n",
    "    'Temp': {'min': 28.00, 'max': 42.00},\n",
    "    'BUN': {'min': 1.00, 'max': 268.00},\n",
    "    'WBC': {'min': 0.10, 'max': 440.00},\n",
    "    'Platelets': {'min': 2.00, 'max': 2322.00},\n",
    "    'DBP': {'min': 20.00, 'max': 300.00}\n",
    "}\n",
    "\n",
    "# Aplicar caps nas variáveis imputadas\n",
    "total_caps_applied = 0\n",
    "\n",
    "for var in caps_ranges.keys():\n",
    "    if var in X_train_advanced_imputed.columns:\n",
    "        min_cap = caps_ranges[var]['min']\n",
    "        max_cap = caps_ranges[var]['max']\n",
    "        \n",
    "        # Contabilizar valores fora dos limites antes da correção\n",
    "        below_min = (X_train_advanced_imputed[var] < min_cap).sum()\n",
    "        above_max = (X_train_advanced_imputed[var] > max_cap).sum()\n",
    "        total_corrections = below_min + above_max\n",
    "        \n",
    "        if total_corrections > 0:\n",
    "            print(f\"  • {var}: {below_min:,} valores < {min_cap}, {above_max:,} valores > {max_cap}\")\n",
    "            \n",
    "            # Aplicar caps\n",
    "            X_train_advanced_imputed[var] = X_train_advanced_imputed[var].clip(\n",
    "                lower=min_cap, \n",
    "                upper=max_cap\n",
    "            )\n",
    "            \n",
    "            total_caps_applied += total_corrections\n",
    "        else:\n",
    "            print(f\"  • {var}: Nenhuma correção necessária\")\n",
    "\n",
    "print(f\"\\nTotal de caps aplicados: {total_caps_applied:,}\")\n",
    "\n",
    "# Verificar ranges após caps\n",
    "print(f\"\\nRANGES APÓS APLICAÇÃO DE CAPS:\")\n",
    "for var in caps_ranges.keys():\n",
    "    if var in X_train_advanced_imputed.columns:\n",
    "        min_val = X_train_advanced_imputed[var].min()\n",
    "        max_val = X_train_advanced_imputed[var].max()\n",
    "        print(f\"  • {var}: {min_val:.2f} - {max_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf38c5",
   "metadata": {},
   "source": [
    "#### 4.3.7 Verificação Final Pós-Imputação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3782dd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFICAÇÃO FINAL PÓS-IMPUTAÇÃO\n",
      "============================================================\n",
      "\n",
      "VALIDAÇÃO DE RANGES PÓS-IMPUTAÇÃO:\n",
      "  • DBP: min=3.53, max=300.00, mean=62.69\n",
      "  • Temp: min=23.00, max=50.00, mean=36.96\n",
      "  • BUN: min=-6.65, max=268.00, mean=24.17\n",
      "  • WBC: min=0.10, max=440.00, mean=11.31\n",
      "  • Platelets: min=-223.76, max=2322.00, mean=196.36\n",
      "  • Unit1: min=0.00, max=1.00, mean=0.55\n",
      "  • Unit2: min=0.00, max=1.00, mean=0.45\n",
      "\n",
      "VERIFICAÇÃO GERAL DE MISSING:\n",
      "NENHUMA VARIÁVEL COM MISSING RESTANTE!\n",
      "\n",
      "Dataset após imputações: (1209197, 16)\n",
      "Missing values finais: 0 (0.000%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VERIFICAÇÃO FINAL PÓS-IMPUTAÇÃO\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"VERIFICAÇÃO FINAL PÓS-IMPUTAÇÃO\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "print(f\"\\nVALIDAÇÃO DE RANGES PÓS-IMPUTAÇÃO:\")\n",
    "\n",
    "# Variáveis processadas\n",
    "numeric_processed = linear_regression_vars + hybrid_strategy_vars + logistic_regression_vars\n",
    "for var in numeric_processed:\n",
    "    if var in X_train_advanced_imputed.columns:\n",
    "        min_val = X_train_advanced_imputed[var].min()\n",
    "        max_val = X_train_advanced_imputed[var].max()\n",
    "        mean_val = X_train_advanced_imputed[var].mean()\n",
    "        print(f\"  • {var}: min={min_val:.2f}, max={max_val:.2f}, mean={mean_val:.2f}\")\n",
    "\n",
    "\n",
    "# Verificar se ainda há variáveis com missing\n",
    "print(f\"\\nVERIFICAÇÃO GERAL DE MISSING:\")\n",
    "total_vars_with_missing = 0\n",
    "for col in X_train_advanced_imputed.columns:\n",
    "    missing_count = X_train_advanced_imputed[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        missing_pct = (missing_count / len(X_train_advanced_imputed)) * 100\n",
    "        print(f\"  • {col}: {missing_count:,} valores ({missing_pct:.1f}%)\")\n",
    "        total_vars_with_missing += 1\n",
    "\n",
    "if total_vars_with_missing == 0:\n",
    "    print(\"NENHUMA VARIÁVEL COM MISSING RESTANTE!\")\n",
    "\n",
    "\n",
    "print(f\"\\nDataset após imputações: {X_train_advanced_imputed.shape}\")\n",
    "\n",
    "# Status final do missing\n",
    "final_missing = X_train_advanced_imputed.isnull().sum().sum()\n",
    "total_values = X_train_advanced_imputed.size\n",
    "final_missing_pct = (final_missing / total_values) * 100\n",
    "\n",
    "print(f\"Missing values finais: {final_missing:,} ({final_missing_pct:.3f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc4c04",
   "metadata": {},
   "source": [
    "### 4.4 Validação da Qualidade Pós-Limpeza\n",
    "\n",
    "Aplicação de regressão logística para imputação de variáveis categóricas Unit1 e Unit2, mantendo a relação complementar Unit1 + Unit2 = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ff98118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDAÇÃO DA QUALIDADE PÓS-LIMPEZA:\n",
      "1. VERIFICAÇÃO DE COMPLETUDE:\n",
      "   • Total de valores missing: 0\n",
      "   • Percentual de missing: 0.0000%\n",
      "   • Completude do dataset: 100.0000%\n",
      "\n",
      "3. VERIFICAÇÃO DE DISTRIBUIÇÕES:\n",
      "   • Variáveis numéricas analisadas: 16\n",
      "   • Estatísticas das principais variáveis:\n",
      "     Variável     Mean     Median   Std      Min      Max      Skew  \n",
      "     ------------------------------------------------------------\n",
      "     Hour         25.8     20.0     29.1     0.0      335.0    4.08  \n",
      "     HR           84.5     83.5     16.7     20.0     280.0    0.46  \n",
      "     O2Sat        97.3     98.0     2.8      20.0     100.0    -4.34 \n",
      "     Temp         37.0     36.9     0.5      23.0     50.0     -0.30 \n",
      "     SBP          123.4    121.0    21.8     20.0     300.0    0.64  \n",
      "     MAP          82.2     80.0     15.5     20.0     300.0    1.13  \n",
      "     DBP          62.7     61.0     13.0     3.5      300.0    1.14  \n",
      "     Resp         18.6     18.0     4.8      1.0      100.0    1.10  \n",
      "     BUN          24.2     23.6     6.8      -6.7     268.0    5.20  \n",
      "     WBC          11.3     11.2     2.4      0.1      440.0    35.67 \n",
      "     Platelets    196.4    195.8    30.5     -223.8   2322.0   4.52  \n",
      "     Gender       0.6      1.0      0.5      0.0      1.0      -0.24 \n",
      "     Unit1        0.5      1.0      0.5      0.0      1.0      -0.20 \n",
      "     Unit2        0.5      0.0      0.5      0.0      1.0      0.20  \n",
      "     HospAdmTime  -57.6    -6.5     163.4    -5366.9  24.0     -12.03\n",
      "     ICULOS       27.3     21.0     29.3     1.0      336.0    4.09  \n",
      "\n",
      "4. VERIFICAÇÃO DE INTEGRIDADE:\n",
      "   • Shape do X_train: (1209197, 16)\n",
      "   • Shape do y_train: (1209197,)\n",
      "   • Índices alinhados: True\n"
     ]
    }
   ],
   "source": [
    "# Validação da qualidade dos dados após todas as etapas de limpeza\n",
    "\n",
    "print(\"VALIDAÇÃO DA QUALIDADE PÓS-LIMPEZA:\")\n",
    "\n",
    "X_train_final_cleaned = X_train_advanced_imputed.copy()\n",
    "y_train_final_cleaned = y_train_cleaned.copy()\n",
    "\n",
    "# 1. Verificação de Completude\n",
    "print(\"1. VERIFICAÇÃO DE COMPLETUDE:\")\n",
    "total_missing = X_train_final_cleaned.isnull().sum().sum()\n",
    "total_values = X_train_final_cleaned.size\n",
    "missing_pct = (total_missing / total_values) * 100\n",
    "\n",
    "print(f\"   • Total de valores missing: {total_missing:,}\")\n",
    "print(f\"   • Percentual de missing: {missing_pct:.4f}%\")\n",
    "print(f\"   • Completude do dataset: {100-missing_pct:.4f}%\")\n",
    "\n",
    "if total_missing > 0:\n",
    "    print(\"   • Variáveis com missing restante:\")\n",
    "    for col in X_train_final_cleaned.columns:\n",
    "        missing_count = X_train_final_cleaned[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_pct_col = (missing_count / len(X_train_final_cleaned)) * 100\n",
    "            print(f\"     - {col}: {missing_count:,} ({missing_pct_col:.2f}%)\")\n",
    "\n",
    "\n",
    "# 3. Verificação de Distribuições\n",
    "print(f\"\\n3. VERIFICAÇÃO DE DISTRIBUIÇÕES:\")\n",
    "\n",
    "# Análise estatística básica para variáveis numéricas\n",
    "numeric_cols = X_train_final_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"   • Variáveis numéricas analisadas: {len(numeric_cols)}\")\n",
    "\n",
    "distributions_summary = {}\n",
    "for col in numeric_cols:  # Primeiras 5 variáveis para exemplo\n",
    "    data = X_train_final_cleaned[col].dropna()\n",
    "    if len(data) > 0:\n",
    "        distributions_summary[col] = {\n",
    "            'mean': data.mean(),\n",
    "            'median': data.median(),  \n",
    "            'std': data.std(),\n",
    "            'min': data.min(),\n",
    "            'max': data.max(),\n",
    "            'skewness': data.skew()\n",
    "        }\n",
    "\n",
    "print(f\"   • Estatísticas das principais variáveis:\")\n",
    "print(f\"     {'Variável':<12} {'Mean':<8} {'Median':<8} {'Std':<8} {'Min':<8} {'Max':<8} {'Skew':<6}\")\n",
    "print(\"     \" + \"-\"*60)\n",
    "\n",
    "for var, stats in distributions_summary.items():\n",
    "    print(f\"     {var:<12} {stats['mean']:<8.1f} {stats['median']:<8.1f} {stats['std']:<8.1f} {stats['min']:<8.1f} {stats['max']:<8.1f} {stats['skewness']:<6.2f}\")\n",
    "\n",
    "# 4. Verificação de Integridade Referencial\n",
    "print(f\"\\n4. VERIFICAÇÃO DE INTEGRIDADE:\")\n",
    "print(f\"   • Shape do X_train: {X_train_final_cleaned.shape}\")\n",
    "print(f\"   • Shape do y_train: {y_train_final_cleaned.shape}\")\n",
    "print(f\"   • Índices alinhados: {X_train_final_cleaned.index.equals(y_train_final_cleaned.index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6547ba43",
   "metadata": {},
   "source": [
    "## 5. Construção dos Dados (Feature Engineering)\n",
    "\n",
    "**Objetivo:** Criar novas variáveis ou atributos derivados dos dados existentes que possam melhorar o poder preditivo do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b47e5",
   "metadata": {},
   "source": [
    "### 5.1 Criação de função para validação da criação da feature\n",
    "\n",
    "Desenvolvimento de índices e ratios clinicamente estabelecidos para detecção de sepsis (ex: razão neutrófilos/linfócitos, índices de choque).\n",
    "\n",
    "A função demora um pouco a terminar sua execução por causa do modelo utilizado (~1min45s por feature para uma amostra de 30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf5f3254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset base para Feature Engineering: (1209197, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Dataset base para feature engineering\n",
    "X_train_fe = X_train_final_cleaned.copy()\n",
    "y_train_fe = y_train_final_cleaned.copy()\n",
    "\n",
    "print(f\"Dataset base para Feature Engineering: {X_train_fe.shape}\")\n",
    "\n",
    "# 1. FUNÇÃO DE VALIDAÇÃO POR Modelo\n",
    "# Tenta-se fazer previsões, sem normalização/padronização, utilizando o dataset resultante da limpeza e \n",
    "# o que possui as features que se quer criar e compara-se os ganhos/perdas \n",
    "\n",
    "def validate_feature(X_original, X_with_new_feature, y, feature_name, cv_folds=3, \n",
    "                     sample_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Valida se uma nova feature melhora as métricas do modelo\n",
    "    \"\"\"\n",
    "    print(f\"\\nVALIDANDO FEATURE: {feature_name}\")\n",
    "    \n",
    "    _, X_orig_sample, _, X_new_sample, _, y_sample = train_test_split(\n",
    "        X_original, X_with_new_feature, y,\n",
    "        test_size=sample_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Modelo simples para validação rápida\n",
    "    rf = RandomForestClassifier(n_estimators=50, random_state=random_state, n_jobs=-1)\n",
    "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Métricas com dataset original\n",
    "    precision_orig = cross_val_score(rf, X_orig_sample, y_sample, cv=cv, scoring='precision', n_jobs=-1)\n",
    "    recall_orig = cross_val_score(rf, X_orig_sample, y_sample, cv=cv, scoring='recall', n_jobs=-1)\n",
    "    f1_orig = cross_val_score(rf, X_orig_sample, y_sample, cv=cv, scoring='f1', n_jobs=-1)\n",
    "\n",
    "    # Métricas com nova feature\n",
    "    precision_new = cross_val_score(rf, X_new_sample, y_sample, cv=cv, scoring='precision', n_jobs=-1)\n",
    "    recall_new = cross_val_score(rf, X_new_sample, y_sample, cv=cv, scoring='recall', n_jobs=-1)\n",
    "    f1_new = cross_val_score(rf, X_new_sample, y_sample, cv=cv, scoring='f1', n_jobs=-1)\n",
    "\n",
    "    # Calcular melhorias\n",
    "    precision_improvement = precision_new.mean() - precision_orig.mean()\n",
    "    recall_improvement = recall_new.mean() - recall_orig.mean()\n",
    "    f1_improvement = f1_new.mean() - f1_orig.mean()\n",
    "    \n",
    "    print(f\"  Precision: {precision_orig.mean():.4f} → {precision_new.mean():.4f} ({precision_improvement:+.4f})\")\n",
    "    print(f\"  Recall:    {recall_orig.mean():.4f} → {recall_new.mean():.4f} ({recall_improvement:+.4f})\")\n",
    "    print(f\"  F1-Score:  {f1_orig.mean():.4f} → {f1_new.mean():.4f} ({f1_improvement:+.4f})\")\n",
    "    \n",
    "    # Critério de aceitação: melhoria em pelo menos 1 métrica sem deteriorar outras significativamente\n",
    "    improvements = [precision_improvement, recall_improvement, f1_improvement]\n",
    "    positive_improvements = sum(1 for imp in improvements if imp > 0.001)  # Melhoria mínima de 0.1%\n",
    "    negative_improvements = sum(1 for imp in improvements if imp < -0.005)  # Deterioração máxima de 0.5%\n",
    "    \n",
    "    is_good = positive_improvements >= 1 and negative_improvements == 0\n",
    "    \n",
    "    print(f\"  DECISÃO: {'✅ ACEITAR' if is_good else '❌ REJEITAR'}\")\n",
    "    \n",
    "    return is_good, {\n",
    "        'precision_improvement': precision_improvement,\n",
    "        'recall_improvement': recall_improvement,\n",
    "        'f1_improvement': f1_improvement\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf11848",
   "metadata": {},
   "source": [
    "### 5.2 Features Temporais\n",
    "\n",
    "Criação de variáveis derivadas das informações temporais para capturar padrões de risco ao longo do tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6c71a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDANDO FEATURE: Critical_Risk_Window\n",
      "  Precision: 0.5657 → 0.6016 (+0.0359)\n",
      "  Recall:    0.0093 → 0.0104 (+0.0011)\n",
      "  F1-Score:  0.0183 → 0.0203 (+0.0021)\n",
      "  DECISÃO: ✅ ACEITAR\n",
      "\n",
      "VALIDANDO FEATURE: Time_Category\n",
      "  Precision: 0.6016 → 0.6346 (+0.0330)\n",
      "  Recall:    0.0104 → 0.0107 (+0.0003)\n",
      "  F1-Score:  0.0203 → 0.0210 (+0.0006)\n",
      "  DECISÃO: ✅ ACEITAR\n",
      "\n",
      "ACEITAS (2):\n",
      "  • Critical_Risk_Window\n",
      "  • Time_Category\n",
      "\n",
      "REJEITADAS (0):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Feature 1: Janela de Risco Crítico (>100h baseado na EDA)\n",
    "\n",
    "X_test = X_train_fe.copy()\n",
    "X_test['Critical_Risk_Window'] = (X_test['Hour'] > 100).astype(int)\n",
    "\n",
    "is_good, metrics = validate_feature(X_train_fe, X_test, y_train_fe, 'Critical_Risk_Window')\n",
    "\n",
    "if is_good:\n",
    "    X_train_fe['Critical_Risk_Window'] = X_test['Critical_Risk_Window']\n",
    "\n",
    "# Feature 2: Categorização de Urgência por Tempo\n",
    "\n",
    "# Criar categorias mais simples: Early, Medium, High Risk\n",
    "X_test['Time_Category'] = pd.cut(\n",
    "    X_test['Hour'],\n",
    "    bins=[-1, 24, 100, float('inf')], \n",
    "    labels=[0, 1, 2],  # Early, Medium, High\n",
    "    include_lowest=True\n",
    ").astype(int)\n",
    "\n",
    "is_good, metrics = validate_feature(X_train_fe, X_test, y_train_fe, 'Time_Category')\n",
    "\n",
    "if is_good:\n",
    "    X_train_fe['Time_Category'] = X_test['Time_Category']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d933b",
   "metadata": {},
   "source": [
    "**Features Criadas**\n",
    "\n",
    "1. Critical_Risk_Window\n",
    "  - Precision: 0.5657 → 0.6016 (+0.0359)\n",
    "  - Recall:    0.0093 → 0.0104 (+0.0011)\n",
    "  - F1-Score:  0.0183 → 0.0203 (+0.0021)\n",
    "\n",
    "2. Time_Category\n",
    "  - Precision: 0.5657 → 0.6464 (+0.0807)\n",
    "  - Recall:    0.0093 → 0.0105 (+0.0012)\n",
    "  - F1-Score:  0.0183 → 0.0207 (+0.0024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63655ae0",
   "metadata": {},
   "source": [
    "### 5.3 Interações entre as Demais Variáveis \n",
    "\n",
    "Criação de features que capturam interações sinérgicas entre variáveis clínicas relacionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd20a35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas SBP ou MAP não encontradas\n",
      "\n",
      "VALIDANDO FEATURE: Shock_Index\n",
      "  Precision: 0.6346 → 0.5934 (-0.0412)\n",
      "  Recall:    0.0107 → 0.0099 (-0.0008)\n",
      "  F1-Score:  0.0210 → 0.0195 (-0.0015)\n",
      "  DECISÃO: ❌ REJEITAR\n"
     ]
    }
   ],
   "source": [
    "# REDUÇÃO DE REDUNDÂNCIA: FUSÃO SBP + MAP  (variáveis com alta correlação: >0.7)\n",
    "\n",
    "# Verificar correlação\n",
    "if 'SBP' not in X_train_fe.columns or 'MAP' not in X_train_fe.columns:\n",
    "    print('Colunas SBP ou MAP não encontradas')\n",
    "else:\n",
    "    corr_sbp_map = X_train_fe['SBP'].corr(X_train_fe['MAP'])\n",
    "    print(f\"Correlação SBP vs MAP: {corr_sbp_map:.4f}\")\n",
    "\n",
    "    # Criar feature fusionada (MAP é mais diretamente relevante clinicamente)\n",
    "    # Usar MAP como base e ajustar com informação de SBP\n",
    "    X_train_fe['Pressure_Unified'] = 0.7 * X_train_fe['MAP'] + 0.3 * X_train_fe['SBP']\n",
    "\n",
    "    # Validar fusão\n",
    "    is_good, metrics = validate_feature(\n",
    "        X_train_fe[['SBP', 'MAP']], \n",
    "        X_train_fe[['Pressure_Unified']], \n",
    "        y_train_fe, \n",
    "        'Pressure_Unified'\n",
    "    )\n",
    "\n",
    "    if is_good:\n",
    "        print(\"Fusão aceita - removendo SBP e MAP originais\")\n",
    "        X_train_fe.drop(columns=['SBP', 'MAP'], inplace=True)\n",
    "    else:\n",
    "        print(\"Fusão rejeitada - mantendo variáveis originais\")\n",
    "        X_train_fe.drop(columns=['Pressure_Unified'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d5e133",
   "metadata": {},
   "source": [
    "**Features Criadas**\n",
    "\n",
    "1. Pressure_Unified (média ponderada de SBP+MAP):\n",
    "\n",
    "  - Precision: 0.0231 → 0.0616 (+0.0385)\n",
    "  - Recall:    0.0009 → 0.0009 (+0.0000)\n",
    "  - F1-Score:  0.0018 → 0.0018 (+0.0000)\n",
    "\n",
    "O resultado foi remoção de SBP e MAP e a criação de Pressure_Unified\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e5a52",
   "metadata": {},
   "source": [
    "### 5.4 Análise do Resultado do Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d56d3511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. ANÁLISE QUANTITATIVA:\n",
      "Features originais (após limpeza): 16\n",
      "Features finais (após engineering): 17\n",
      "Saldo features criadas: 1\n",
      "\n",
      "2. FEATURES CRIADAS E VALIDADAS:\n",
      "Features aceitas por validação de métricas:\n",
      "   1. Pressure_Unified\n",
      "   2. Critical_Risk_Window\n",
      "   3. Time_Category\n",
      "\n",
      "3. ANÁLISE DE CORRELAÇÃO COM TARGET:\n",
      "Correlações das novas features com SepsisLabel:\n",
      "   1. Critical_Risk_Window: 0.1233\n",
      "   2. Time_Category: 0.0818\n",
      "   3. Pressure_Unified: 0.0164\n",
      "\n",
      "4. VALIDAÇÃO FINAL DE QUALIDADE:\n",
      "  • Total de valores missing: 0\n",
      "  • Alinhamento com target: True\n",
      "  • Distribuição target: 0=1,187,303, 1=21,894\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. ANÁLISE QUANTITATIVA DAS FEATURES CRIADAS==\n",
    "\n",
    "print(\"\\n1. ANÁLISE QUANTITATIVA:\")\n",
    "original_features = X_train_final_cleaned.shape[1]\n",
    "final_features = X_train_fe.shape[1]\n",
    "new_features = final_features - original_features\n",
    "\n",
    "print(f\"Features originais (após limpeza): {original_features}\")\n",
    "print(f\"Features finais (após engineering): {final_features}\")\n",
    "print(f\"Saldo features criadas: {new_features}\")\n",
    "\n",
    "# 2. LISTA DAS FEATURES VALIDADAS\n",
    "\n",
    "print(\"\\n2. FEATURES CRIADAS E VALIDADAS:\")\n",
    "\n",
    "all_new_features = []\n",
    "for col in X_train_fe.columns:\n",
    "    if col not in X_train_final_cleaned.columns:\n",
    "        all_new_features.append(col)\n",
    "\n",
    "if all_new_features:\n",
    "    print(\"Features aceitas por validação de métricas:\")\n",
    "    for i, feature in enumerate(all_new_features, 1):\n",
    "        print(f\"  {i:2d}. {feature}\")\n",
    "else:\n",
    "    print(\"Nenhuma feature adicional foi validada como benéfica\")\n",
    "\n",
    "# 3. ANÁLISE DE CORRELAÇÃO COM TARGET\n",
    "\n",
    "print(\"\\n3. ANÁLISE DE CORRELAÇÃO COM TARGET:\")\n",
    "\n",
    "# Calcular correlação das novas features com SepsisLabel\n",
    "correlations = []\n",
    "for col in all_new_features:\n",
    "    if col in X_train_fe.columns:\n",
    "        try:\n",
    "            corr = X_train_fe[col].corr(y_train_fe)\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append((col, abs(corr)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if correlations:\n",
    "    # Ordenar por correlação absoluta\n",
    "    correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Correlações das novas features com SepsisLabel:\")\n",
    "    for i, (feature, corr) in enumerate(correlations):\n",
    "        print(f\"  {i+1:2d}. {feature}: {corr:.4f}\")\n",
    "else:\n",
    "    print(\"Nenhuma nova feature para análise de correlação\")\n",
    "\n",
    "# 4. VALIDAÇÃO FINAL DE QUALIDADE\n",
    "\n",
    "print(\"\\n4. VALIDAÇÃO FINAL DE QUALIDADE:\")\n",
    "\n",
    "# Verificar se há problemas no dataset final\n",
    "total_missing = X_train_fe.isnull().sum().sum()\n",
    "infinite_values = np.isinf(X_train_fe.select_dtypes(include=[np.number])).sum().sum()\n",
    "\n",
    "print(f\"  • Total de valores missing: {total_missing:,}\")\n",
    "print(f\"  • Alinhamento com target: {X_train_fe.index.equals(y_train_fe.index)}\")\n",
    "\n",
    "# Distribuição do target\n",
    "target_dist = y_train_fe.value_counts()\n",
    "print(f\"  • Distribuição target: 0={target_dist[0]:,}, 1={target_dist[1]:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ba396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentar características da fonte única\n",
    "dataset_info = {\n",
    "    'source': 'PhysioNet 2019 Challenge',\n",
    "    'description': 'Early Detection of Sepsis from Clinical Data',\n",
    "    'patients': 'Multiple ICU patients with temporal observations',\n",
    "    'timeframe': 'Variable length ICU stays',\n",
    "    'completeness': 'Single comprehensive source'\n",
    "}\n",
    "\n",
    "print(\"INFORMAÇÕES DA FONTE DE DADOS:\")\n",
    "for key, value in dataset_info.items():\n",
    "    print(f\"  • {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ab110d",
   "metadata": {},
   "source": [
    "## 6. Formatação dos Dados\n",
    "\n",
    "**Objetivo:** Preparar os dados no formato necessário para os algoritmos de modelagem, incluindo normalização, encoding e divisão final dos conjuntos.\n",
    "\n",
    "**Atividades principais:**\n",
    "* Normalização/padronização de variáveis numéricas\n",
    "* Encoding de variáveis categóricas  \n",
    "* Balanceamento de classes\n",
    "* Criação dos datasets finais para modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a1b02",
   "metadata": {},
   "source": [
    "### 6.1 Normalização e Padronização\n",
    "\n",
    "Aplicação de transformações de escala para garantir que todas as variáveis numéricas tenham contribuições equilibradas nos algoritmos.\n",
    "\n",
    "Aplicação de transformações matemáticas para normalizar distribuições assimétricas identificadas na EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20340ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para normalização\n",
    "# StandardScaler, MinMaxScaler baseado na distribuição das variáveis\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdab3d8",
   "metadata": {},
   "source": [
    "### 6.2 Encoding de Variáveis Categóricas\n",
    "\n",
    "Conversão de variáveis categóricas para formato numérico apropriado para algoritmos de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para encoding categórico\n",
    "# One-hot encoding, label encoding baseado na cardinalidade\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78947d7e",
   "metadata": {},
   "source": [
    "### 6.3 Balanceamento de Classes\n",
    "\n",
    "Implementação de técnicas para lidar com o severo desbalanceamento entre classes (98.2% vs 1.8%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd89bef",
   "metadata": {},
   "source": [
    "#### 6.3.1 Análise de Estratégias de Balanceamento\n",
    "\n",
    "Comparação de diferentes abordagens: oversampling, undersampling e métodos combinados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para comparação de estratégias\n",
    "# SMOTE, Random Over/Under Sampling, SMOTETomek\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a7ea5",
   "metadata": {},
   "source": [
    "#### 6.3.2 Implementação da Estratégia Escolhida\n",
    "\n",
    "Aplicação da técnica de balanceamento selecionada com base na análise comparativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para implementação do balanceamento\n",
    "# Aplicar técnica escolhida e validar resultados\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65297c42",
   "metadata": {},
   "source": [
    "### 6.4 Criação dos Datasets Finais\n",
    "\n",
    "Montagem dos conjuntos de dados finais prontos para a fase de modelagem, incluindo validação da integridade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para datasets finais\n",
    "# Criar X_train_final, X_test_final, y_train_final, y_test_final\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f08b9",
   "metadata": {},
   "source": [
    "## 7. Validação Final e Export\n",
    "\n",
    "Verificação final da qualidade e consistência dos dados preparados, seguida do salvamento dos datasets processados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6dd45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para validação final\n",
    "# Verificar shapes, tipos, ranges, consistência lógica\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
