{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a2846b",
   "metadata": {},
   "source": [
    "# Data Preparation - Dataset de Sepsis\n",
    "## CRISP-DM Fase 3: Preparação dos Dados\n",
    "\n",
    "**Objetivo da Fase:**\n",
    "* Transformar dados brutos em formato adequado para modelagem\n",
    "* Implementar estratégias de limpeza e tratamento baseadas nos insights da EDA\n",
    "* Criar features derivadas com relevância clínica\n",
    "* Preparar datasets finais para algoritmos de machine learning\n",
    "\n",
    "**Baseado nos Insights da EDA:**\n",
    "* 37/41 variáveis apresentam missing values (68.37% do dataset)\n",
    "* 27 variáveis com >80% missing (candidatas à remoção)\n",
    "* Dataset altamente desbalanceado: 98.2% não-sepsis vs 1.8% sepsis\n",
    "* Estrutura temporal importante: risco aumenta após 100h na UTI\n",
    "* Variáveis categóricas bem definidas: Gender, Unit1, Unit2\n",
    "\n",
    "**Tarefas CRISP-DM a serem executadas:**\n",
    "1. **Seleção dos Dados**: Escolher variáveis mais relevantes\n",
    "2. **Limpeza dos Dados**: Tratar inconsistências e valores ausentes  \n",
    "3. **Construção dos Dados**: Criar features derivadas e engenharia\n",
    "4. **Integração dos Dados**: Combinar fontes (não aplicável aqui)\n",
    "5. **Formatação dos Dados**: Preparar formato final para modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21555a66",
   "metadata": {},
   "source": [
    "## Configuração do Ambiente Google Colab\n",
    "\n",
    "Para funcionar no Google Colab, é necessário criar um atalho do diretório MDA no seu próprio Drive e então rodar os dois comandos abaixo e conceder permissão ao seu drive quando rodar a célula logo abaixo.\n",
    "\n",
    "[Link](https://towardsdatascience.com/simplify-file-sharing-44bde79a8a18/) detalhando como funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f059104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modificar para o diretorio que contém os dados de teste e treino\n",
    "%cd /content/drive/MyDrive/MDA/Train\\ and\\ test\\ data\\ -\\ Proj\\ DM/\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fed35e",
   "metadata": {},
   "source": [
    "## 1. Importação das Bibliotecas\n",
    "\n",
    "Importação de todas as bibliotecas necessárias para preparação dos dados, incluindo bibliotecas específicas para pré-processamento, feature engineering e balanceamento de classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca5c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas essenciais para manipulação de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Bibliotecas para pré-processamento\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# Bibliotecas para balanceamento de classes\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Configurações gerais\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68cfe4b",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados e Insights da EDA\n",
    "\n",
    "Carregamento dos datasets de treino e teste, seguido da documentação dos principais insights obtidos na análise exploratória que guiarão as decisões de preparação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f4132",
   "metadata": {},
   "source": [
    "### 2.1 Carregamento dos Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dataset_sepsis_train.csv')\n",
    "\n",
    "# Separar features e target\n",
    "X_train = train_df.drop('SepsisLabel', axis=1)\n",
    "y_train = train_df['SepsisLabel']\n",
    "\n",
    "# Forma final\n",
    "print(f\"X_train: {X_train.shape} | y_train: {y_train.shape}\")\n",
    "\n",
    "# Distribuição das classes no treino\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a29a8",
   "metadata": {},
   "source": [
    "## 3. TAREFA 1: Seleção dos Dados\n",
    "\n",
    "**Objetivo:** Escolher as variáveis mais relevantes para o modelo de mineração, removendo features com baixo potencial preditivo ou problemas graves de qualidade.\n",
    "\n",
    "**Critérios de seleção:**\n",
    "* Relevância clínica para detecção de sepsis\n",
    "* Percentual de missing values aceitável\n",
    "* Separabilidade entre classes (baseada na EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90a9f6",
   "metadata": {},
   "source": [
    "### 3.1 Mapeamento de Variáveis com Excesso de Missing Values para Remoção\n",
    "\n",
    "Vamos refazer a análise, mais objetiva e breve, das variáveis com >60% missing values para decidir quais manter, tratar ou remover baseado no critério de separabilidade de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identificar variáveis com >60% missing\n",
    "high_missing_vars = []\n",
    "for col in X_train.select_dtypes(include=[np.number]).columns:\n",
    "    missing_pct = (X_train[col].isnull().sum() / len(X_train)) * 100\n",
    "    if missing_pct > 60:\n",
    "        high_missing_vars.append({\n",
    "            'variavel': col,\n",
    "            'missing_pct': missing_pct\n",
    "        })\n",
    "\n",
    "# Calcular separabilidade para cada variável\n",
    "separability_results = {\n",
    "    'IMPUTAR_SIMPLES': [],     # Separabilidade > 0.3: Alta discriminação\n",
    "    'IMPUTAR_AVANCADA': [],    # Separabilidade 0.16 - 0.3: Discriminação moderada  \n",
    "    'DESCARTAR': []            # Separabilidade < 0.16: Baixa discriminação\n",
    "}\n",
    "\n",
    "for var_info in high_missing_vars:\n",
    "    col = var_info['variavel']\n",
    "    missing_pct = var_info['missing_pct']\n",
    "    \n",
    "    # Criar DataFrame temporário sem valores faltantes\n",
    "    temp_df = pd.DataFrame({\n",
    "        'feature': X_train[col],\n",
    "        'target': y_train\n",
    "    }).dropna()\n",
    "\n",
    "    # Separar por classe\n",
    "    no_sepsis_data = temp_df[temp_df['target'] == 0]['feature']\n",
    "    sepsis_data = temp_df[temp_df['target'] == 1]['feature']\n",
    "\n",
    "    # Calcular separabilidade (diferença de medianas / desvio padrão)\n",
    "    median_diff = abs(sepsis_data.median() - no_sepsis_data.median())\n",
    "    pooled_std = no_sepsis_data.std() if no_sepsis_data.std() > 0 else 1\n",
    "    separability = median_diff / pooled_std\n",
    "    \n",
    "    # Classificar baseado na separabilidade\n",
    "    var_result = {\n",
    "        'variavel': col,\n",
    "        'missing_pct': missing_pct,\n",
    "        'separabilidade': separability,\n",
    "        'n_amostras': len(temp_df)\n",
    "    }\n",
    "    \n",
    "    if separability > 0.3:\n",
    "        separability_results['IMPUTAR_SIMPLES'].append(var_result)\n",
    "    elif separability >= 0.16:\n",
    "        separability_results['IMPUTAR_AVANCADA'].append(var_result)\n",
    "    else:\n",
    "        separability_results['DESCARTAR'].append(var_result)\n",
    "\n",
    "# Exibir resultados \n",
    "\n",
    "for categoria, vars_list in separability_results.items():\n",
    "    print(f\"\\n{categoria} ({len(vars_list)} variáveis):\")\n",
    "    for var in sorted(vars_list, key=lambda x: x['separabilidade'], reverse=True):\n",
    "        print(f\"  • {var['variavel']}: Sep={var['separabilidade']:.3f} | Missing={var['missing_pct']:.1f}% | n={var['n_amostras']:,}\")\n",
    "\n",
    "variables_to_keep = [var['variavel'] for var in separability_results['IMPUTAR_SIMPLES']]\n",
    "variables_to_treat = [var['variavel'] for var in separability_results['IMPUTAR_AVANCADA']]  \n",
    "variables_to_discard = [var['variavel'] for var in separability_results['DESCARTAR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9bb2c",
   "metadata": {},
   "source": [
    "### 3.2 Análise de Separabilidade Estatística\n",
    "\n",
    "Avaliação da capacidade discriminativa das variáveis que não foram selecionadas para exclusão, a fim de confirmar e justificar as decisões antes de fazer a remoção, usando testes estatísticos e métricas de separação entre classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8bc6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "X_train_not_discard = X_train.drop(columns=variables_to_discard)\n",
    "# Separar variáveis numéricas e categóricas\n",
    "categorical_vars = ['Gender', 'Unit1', 'Unit2']  \n",
    "# Numéricas são todas as colunas MENOS as categóricas\n",
    "numeric_vars = [col for col in X_train_not_discard.columns if col not in categorical_vars]\n",
    "\n",
    "\n",
    "# Análise para variáveis numéricas\n",
    "all_separability_results = []\n",
    "\n",
    "print(f\"\\nANÁLISE DE VARIÁVEIS NUMÉRICAS:\")\n",
    "print(f\"{'Variável':<15} {'Missing%':<10} {'Separab.':<10} {'p-value_MW':<12} {'Mutual Info':<12} {'N_samples':<10}\\n\")\n",
    "\n",
    "for var in numeric_vars:\n",
    "    missing_pct = (X_train_not_discard[var].isnull().sum() / len(X_train_not_discard)) * 100\n",
    "    \n",
    "    # Criar DataFrame temporário sem valores faltantes\n",
    "    temp_df = pd.DataFrame({\n",
    "        'feature': X_train_not_discard[var],\n",
    "        'target': y_train\n",
    "    }).dropna()\n",
    "    \n",
    "    # Separar por classe\n",
    "    no_sepsis_data = temp_df[temp_df['target'] == 0]['feature']\n",
    "    sepsis_data = temp_df[temp_df['target'] == 1]['feature']\n",
    "\n",
    "    # Calcular separabilidade (diferença de medianas / desvio padrão)\n",
    "    median_diff = abs(sepsis_data.median() - no_sepsis_data.median())\n",
    "    pooled_std = np.sqrt(((no_sepsis_data.std()**2 + sepsis_data.std()**2) / 2))\n",
    "    separability = median_diff / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    # Teste U de Mann-Whitney (não-paramétrico)\n",
    "    try:\n",
    "        stat, p_value = stats.mannwhitneyu(sepsis_data, no_sepsis_data, alternative='two-sided')\n",
    "        mann_whitney_pval = p_value\n",
    "    except:\n",
    "        mann_whitney_pval = 1.0  # p-value máximo para casos de erro\n",
    "\n",
    "    # Informação mútua\n",
    "    try:\n",
    "        # Discretizar para mutual info (usar quintis)\n",
    "        temp_df['feature_disc'] = pd.qcut(temp_df['feature'], q=5, labels=False, duplicates='drop')\n",
    "        mutual_info = mutual_info_score(temp_df['target'], temp_df['feature_disc'])\n",
    "    except:\n",
    "        mutual_info = 0\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    result = {\n",
    "        'variavel': var,\n",
    "        'missing_pct': missing_pct,\n",
    "        'separabilidade': separability,\n",
    "        'mann_whitney': mann_whitney_pval,\n",
    "        'mutual_info': mutual_info,\n",
    "        'n_amostras': len(temp_df)\n",
    "    }\n",
    "    all_separability_results.append(result)\n",
    "    \n",
    "    # Exibir resultado\n",
    "    print(f\"{var:<15} {missing_pct:<10.1f} {separability:<10.3f} {mann_whitney_pval:<12.5f} {mutual_info:<12.7f} {len(temp_df):<10,}\")\n",
    "\n",
    "\n",
    "# Análise para variáveis categóricas\n",
    "print(f\"\\nANÁLISE DE VARIÁVEIS CATEGÓRICAS:\")\n",
    "print(f\"{'Variável':<15} {'Missing%':<10} {'p-value_Chi2':<12} {'Mutual Info':<12} {'N_samples':<10}\\n\")\n",
    "\n",
    "for var in categorical_vars:\n",
    "    missing_pct = (X_train_not_discard[var].isnull().sum() / len(X_train_not_discard)) * 100\n",
    "    \n",
    "    # Criar DataFrame temporário sem valores faltantes\n",
    "    temp_df = pd.DataFrame({\n",
    "        'feature': X_train_not_discard[var],\n",
    "        'target': y_train\n",
    "    }).dropna()\n",
    "    \n",
    "    # Teste Qui-quadrado\n",
    "    try:\n",
    "        contingency_table = pd.crosstab(temp_df['feature'], temp_df['target'])\n",
    "        chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "        chi2_pval = p_value\n",
    "    except:\n",
    "        chi2_pval = 1.0  # p-value máximo para casos de erro\n",
    "    \n",
    "    # Informação mútua\n",
    "    try:\n",
    "        mutual_info = mutual_info_score(temp_df['target'], temp_df['feature'])\n",
    "    except:\n",
    "        mutual_info = 0\n",
    "    \n",
    "    result = {\n",
    "        'variavel': var,\n",
    "        'missing_pct': missing_pct,\n",
    "        'chi2_sig': chi2_pval,\n",
    "        'mutual_info': mutual_info,\n",
    "        'n_amostras': len(temp_df),\n",
    "        'tipo': 'categorical'\n",
    "    }\n",
    "    all_separability_results.append(result)\n",
    "    \n",
    "    print(f\"{var:<15} {missing_pct:<10.1f} {chi2_pval:<12.5f} {mutual_info:<12.7f} {len(temp_df):<10,}\")\n",
    "\n",
    "\n",
    "# Ranking por separabilidade (variáveis numéricas)\n",
    "numeric_results = [r for r in all_separability_results if 'separabilidade' in r]\n",
    "numeric_results_sorted = sorted(numeric_results, key=lambda x: x['separabilidade'], reverse=True)\n",
    "\n",
    "print(f\"\\nSeparabilidades Numéricas:\")\n",
    "separabilities = [r['separabilidade'] for r in numeric_results]\n",
    "print(f\"  • Variáveis com Sep > 0.3: {sum(1 for s in separabilities if s > 0.3)}\")\n",
    "print(f\"  • Variáveis com Sep > 0.16: {sum(1 for s in separabilities if s > 0.16)}\")\n",
    "\n",
    "# Análise de significância estatística\n",
    "print(f\"\\nSIGNIFICÂNCIA ESTATÍSTICA (Numéricas):\")\n",
    "sig_001 = sum(1 for r in numeric_results if r['mann_whitney'] < 0.001)\n",
    "sig_01 = sum(1 for r in numeric_results if 0.001 <= r['mann_whitney'] < 0.01)\n",
    "sig_05 = sum(1 for r in numeric_results if 0.01 <= r['mann_whitney'] < 0.05)\n",
    "not_sig = sum(1 for r in numeric_results if r['mann_whitney'] >= 0.05)\n",
    "print(f\"  • p < 0.001 (altamente significativo): {sig_001} variáveis\")\n",
    "print(f\"  • 0.001 ≤ p < 0.01 (muito significativo): {sig_01} variáveis\")  \n",
    "print(f\"  • 0.01 ≤ p < 0.05 (significativo): {sig_05} variáveis\")\n",
    "print(f\"  • p ≥ 0.05 (não significativo): {not_sig} variáveis\")\n",
    "\n",
    "# Salvar resultados para uso posterior\n",
    "statistical_analysis_results = {\n",
    "    'numeric_results': numeric_results_sorted,\n",
    "    'all_results': all_separability_results\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ec87c",
   "metadata": {},
   "source": [
    "#### Alteração Após Análise \n",
    "Percebe-se que ainda é possível remover `Age` do escopo de features visto que não há nenhuma métrica que aponte essa variável como algo relevante apesar do que diz a literatura sobre sepsis e o baixo percentual de missing values. Ela possui baixa separabilidade, um Man Whitney não significativo e Mutual Info demonstra zero informação sobre sepse\n",
    "\n",
    "| Variável | Missing% | Separabilidade |  p-value_MW | Mutual Info |\n",
    "|----------|----------|----------------|--------------|-------------| \n",
    "| **Age** | 0.0 | 0.000 |  0.4367935 | 0.00000 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977f647",
   "metadata": {},
   "source": [
    "### 3.3 Aplicação das Decisões de Separabilidade\n",
    "\n",
    "Implementação prática da remoção de variáveis com baixa separabilidade e organização das listas para tratamento adiante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar Age às variáveis a descartar \n",
    "variables_to_discard.append('Age')\n",
    "\n",
    "# Remover variáveis com baixa separabilidade e alto missing do dataset principal\n",
    "total_to_remove = len(variables_to_discard)\n",
    "print(f\"Removendo {total_to_remove} variáveis com baixa separabilidade...\")\n",
    "\n",
    "X_train_selected = X_train.drop(columns=variables_to_discard)\n",
    "\n",
    "print(\"Variáveis removidas:\")\n",
    "for var in variables_to_discard:\n",
    "    missing_pct = (X_train[var].isnull().sum() / len(X_train)) * 100\n",
    "    if var == 'Age':\n",
    "        print(f\"  • {var}: {missing_pct:.1f}% missing (removida por baixa discriminação)\")\n",
    "    else:\n",
    "        print(f\"  • {var}: {missing_pct:.1f}% missing\")\n",
    "\n",
    "print(f\"\\nDimensões do dataset:\")\n",
    "print(f\"  • Original: {X_train.shape}\")\n",
    "print(f\"  • Após seleção: {X_train_selected.shape}\")\n",
    "\n",
    "# Organizar variáveis por estratégia de tratamento\n",
    "high_missing_strategy = {\n",
    "    'imputacao_simples': variables_to_keep,      \n",
    "    'imputacao_avancada': variables_to_treat,    \n",
    "    'removidas': variables_to_discard           \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63353242",
   "metadata": {},
   "source": [
    "### 3.4 Síntese das Decisões de Seleção de Variáveis\n",
    "\n",
    "**Documentação completa das decisões tomadas na Tarefa 1 (Seleção dos Dados) com respectivas justificativas:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759d1bb",
   "metadata": {},
   "source": [
    "\n",
    "RESULTADOS FINAIS DA SELEÇÃO DE VARIÁVEIS\n",
    "\n",
    "CRITÉRIOS DE SELEÇÃO APLICADOS\n",
    "\n",
    "1. **Critério de Missing Values**: Variáveis com >60% de valores ausentes analisadas\n",
    "2. **Critério de Separabilidade**: Capacidade discriminativa entre classes (limite: 0.16)\n",
    "3. **Critério Estatístico**: Significância nos testes Mann-Whitney U e Chi-quadrado\n",
    "\n",
    "IMPACTO FINAL DAS DECISÕES\n",
    "\n",
    "**Redução Dimensional Efetiva:**\n",
    "- **Dataset original**: 1,241,768 × 41 variáveis\n",
    "- **Dataset final**: 1,241,768 × 16 variáveis  \n",
    "- **Redução**: 61% das variáveis removidas (25/41)\n",
    "- **Taxa de compressão**: 2.6:1\n",
    "\n",
    "ESTRATÉGIAS DE TRATAMENTO DEFINIDAS\n",
    "\n",
    "**IMPUTAÇÃO Cuidadosa** (1 variáveis - Separabilidade > 0.3)\n",
    "\n",
    "**Estratégia**: Imputação com medidas robustas (mediana) + validação clínica\n",
    "\n",
    "| Variável | Missing% | Separabilidade | p-value | Justificativa Médica |\n",
    "|----------|----------|----------------|---------|---------------------|\n",
    "| **Temp** | 66.2% | 0.326 | < 0.001 | Temperatura corporal: indicador direto de resposta inflamatória |\n",
    "\n",
    "**IMPUTAÇÃO Específica** (3 variáveis - Separabilidade 0.16-0.3 ou Separabilidade>0.3 e Missing>90%)  \n",
    "\n",
    "**Estratégia**: Técnicas sofisticadas (KNN, regressão) devido à considerável relevância clínica \n",
    "\n",
    "| Variável | Missing% | Separabilidade | p-value | Justificativa Médica |\n",
    "|----------|----------|----------------|---------|---------------------|\n",
    "| **BUN** | 93.1% | 0.328 | < 0.001 | Função renal: biomarcador de disfunção orgânica na sepsis |\n",
    "| **Platelets** | 94.1% | 0.189 | < 0.001 | Coagulação: trombocitopenia marca disfunção hemostática |\n",
    "| **WBC** | 93.6% | 0.166 | < 0.001 | Sistema imune: resposta leucocitária à infecção |\n",
    "\n",
    "**REMOVIDAS** (25 variáveis - Separabilidade < 0.16)\n",
    "\n",
    "**Critério duplo**: Baixa discriminação + Alto missing (>60%)\n",
    "\n",
    "**Destaques das remoções:**\n",
    "- **Age**: 0.0% missing, Sep: 0.000, p-value: 0.437 (única exceção por baixa discriminação)\n",
    "- **24 variáveis** com >80% missing + separabilidade < 0.16\n",
    "- **Maior redução**: TroponinI (99.1% missing), Bilirubin_direct (99.8% missing)\n",
    "\n",
    "VALIDAÇÃO ESTATÍSTICA FINAL\n",
    "\n",
    "**Testes Aplicados:**\n",
    "- **Mann-Whitney U**: Para variáveis numéricas (não-paramétrico)\n",
    "- **Qui-quadrado**: Para variáveis categóricas\n",
    "- **Informação Mútua**: Medida de dependência entre variáveis\n",
    "\n",
    "**Significância dos Testes:**\n",
    "- **Variáveis numéricas significativas**: 13/14 (p < 0.05)\n",
    "- **Variáveis categóricas significativas**: 3/3 (p < 0.001)  \n",
    "- **Taxa de significância geral**: 94.1% (16/17 variáveis)\n",
    "\n",
    "**Estratégias de Tratamento Definidas:**\n",
    "- **Imputação cuidadosa**: 2 variáveis de alta relevância\n",
    "- **Imputação específica**: 6 variáveis de relevância moderada\n",
    "- **Manutenção**: 13 variáveis com baixo missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995bf50",
   "metadata": {},
   "source": [
    "## 4. TAREFA 2: Limpeza dos Dados\n",
    "\n",
    "**Objetivo:** Corrigir ou remover dados inconsistentes, duplicados ou ausentes através de estratégias específicas para cada tipo de variável.\n",
    "\n",
    "**Estratégias por tipo de missing:**\n",
    "* Missing < 20%: Imputação simples (mediana/moda)\n",
    "* Missing >= 20%: Imputação baseada em modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34539dc5",
   "metadata": {},
   "source": [
    "### 4.1 Detecção e Remoção de Duplicatas\n",
    "\n",
    "Identificação de registros duplicados exatos e tratamento adequado considerando a natureza temporal dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44273b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECÇÃO DE DUPLICATAS:\n",
      "Dataset atual: (1241768, 16)\n",
      "Duplicatas exatas encontradas: 32,571\n",
      "Remover 32,571 duplicatas exatas\n",
      "Dataset após remoção: (1209197, 16)\n",
      "Dataset limpo final: (1209197, 16)\n"
     ]
    }
   ],
   "source": [
    "# Verificar duplicatas no dataset selecionado\n",
    "print(\"DETECÇÃO DE DUPLICATAS:\")\n",
    "print(f\"Dataset atual: {X_train_selected.shape}\")\n",
    "\n",
    "# Verificar duplicatas exatas (todas as colunas)\n",
    "duplicatas_exatas = X_train_selected.duplicated().sum()\n",
    "print(f\"Duplicatas exatas encontradas: {duplicatas_exatas:,}\")\n",
    "\n",
    "print(f\"Remover {duplicatas_exatas:,} duplicatas exatas\")\n",
    "X_train_cleaned = X_train_selected.drop_duplicates()\n",
    "y_train_cleaned = y_train.loc[X_train_cleaned.index]\n",
    "print(f\"Dataset após remoção: {X_train_cleaned.shape}\")\n",
    "\n",
    "print(f\"Dataset limpo final: {X_train_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb71fe",
   "metadata": {},
   "source": [
    "### 4.2 Tratamento e Análise de Outliers\n",
    "\n",
    "A ideia é tentar preservar os outliers visto que eles se demonstraram relevantes para a identificação de instâncias com SepsisLabel=1 na Análise Exploratória.\n",
    "Vamos apenas deixar algumas variáveis mais genéricas e conhecidas mais consistentes e fazer uma análise geral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4c1182fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECÇÃO E TRATAMENTO DE OUTLIERS:\n",
      "\n",
      "Variáveis numéricas para análise: 16\n",
      "\n",
      "RESUMO DE OUTLIERS DETECTADOS:\n",
      "Variável     Clínicos  IQR      Z-score  N_total  Range Original       Range Tratado       \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hour         0         54,951   27,883   1,209,197 0.00-335.00          0.00-335.00         \n",
      "HR           2         11,203   5,502    1,119,122 20.00-280.00         20.00-250.00        \n",
      "O2Sat        0         19,905   8,912    1,079,707 20.00-100.00         20.00-100.00        \n",
      "Temp         11        5,223    3,392    419,945  23.00-50.00          28.00-42.00         \n",
      "SBP          0         12,748   6,046    1,060,857 20.00-300.00         20.00-300.00        \n",
      "MAP          0         17,543   8,057    1,087,236 20.00-300.00         20.00-300.00        \n",
      "DBP          0         13,033   6,560    852,691  20.00-300.00         20.00-300.00        \n",
      "Resp         0         22,208   10,343   1,051,178 1.00-100.00          1.00-100.00         \n",
      "BUN          0         7,039    2,063    85,439   1.00-268.00          1.00-268.00         \n",
      "WBC          0         2,767    514      79,613   0.10-440.00          0.10-440.00         \n",
      "Platelets    0         2,369    1,070    73,790   2.00-2322.00         2.00-2322.00        \n",
      "Gender       0         0        0        1,209,197 0.00-1.00            0.00-1.00           \n",
      "Unit1        0         0        0        737,114  0.00-1.00            0.00-1.00           \n",
      "Unit2        0         0        0        737,114  0.00-1.00            0.00-1.00           \n",
      "HospAdmTime  1,161,456 161,629  17,170   1,209,191 -5366.86-23.99       0.00-23.99          \n",
      "ICULOS       0         54,672   27,811   1,209,197 1.00-336.00          1.00-336.00         \n",
      "\n",
      "Total de Caps Aplicados: 1,161,469\n"
     ]
    }
   ],
   "source": [
    "# Tratamento de outliers para variáveis numéricas\n",
    "from scipy import stats\n",
    "\n",
    "print(\"DETECÇÃO E TRATAMENTO DE OUTLIERS:\\n\")\n",
    "\n",
    "# Separar variáveis numéricas do dataset limpo\n",
    "numeric_cols = X_train_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Variáveis numéricas para análise: {len(numeric_cols)}\")\n",
    "\n",
    "# Definir limites um pouco mais realistas para algumas variáveis \n",
    "# Considerando registros de outros casos extremos e do próprio dataset\n",
    "# Propósito de deixar os dados mais consistentes\n",
    "clinical_limits = {\n",
    "    'HR': (20, 250),           # Batimentos cardíacos: 20-250 bpm\n",
    "    'Temp': (28, 42),          # Temperatura: 28-42°C\n",
    "    'Hour': (0, 336),          # Horas na UTI: 1-336h (14 dias)\n",
    "    'ICULOS': (0, 336),        # Tempo UTI: 1-336h\n",
    "    'HospAdmTime': (0, 24),   # Tempo hospital: 0 a 24h\n",
    "}\n",
    "\n",
    "outliers_summary = {}\n",
    "X_train_outliers_treated = X_train_cleaned.copy()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    data = X_train_outliers_treated[col].dropna()\n",
    "        \n",
    "    # Cap do Range (quando aplicável)\n",
    "    clinical_outliers = 0\n",
    "    if col in clinical_limits:\n",
    "        min_val, max_val = clinical_limits[col]\n",
    "        clinical_mask = (data < min_val) | (data > max_val)\n",
    "        clinical_outliers = clinical_mask.sum()\n",
    "        \n",
    "        # Aplicar capping\n",
    "        X_train_outliers_treated.loc[X_train_outliers_treated[col] < min_val, col] = min_val\n",
    "        X_train_outliers_treated.loc[X_train_outliers_treated[col] > max_val, col] = max_val\n",
    "    \n",
    "    # Análise do IQR\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    iqr_outliers = ((data < lower_bound) | (data > upper_bound)).sum()\n",
    "    \n",
    "    # Análise do Z-score (outliers > 3 desvios padrão)\n",
    "    z_scores = np.abs(stats.zscore(data))\n",
    "    zscore_outliers = (z_scores > 3).sum()\n",
    "    \n",
    "    outliers_summary[col] = {\n",
    "        'clinical': clinical_outliers,\n",
    "        'iqr': iqr_outliers,\n",
    "        'zscore': zscore_outliers,\n",
    "        'total_values': len(data),\n",
    "        'range_original': (data.min(), data.max()),\n",
    "        'range_treated': (X_train_outliers_treated[col].min(), X_train_outliers_treated[col].max())\n",
    "    }\n",
    "\n",
    "# Exibir resumo dos outliers\n",
    "print(f\"\\nRESUMO DE OUTLIERS DETECTADOS:\")\n",
    "print(f\"{'Variável':<12} {'Clínicos':<9} {'IQR':<8} {'Z-score':<8} {'N_total':<8} {'Range Original':<20} {'Range Tratado':<20}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for col, summary in outliers_summary.items():\n",
    "    clinical = summary['clinical']\n",
    "    iqr = summary['iqr'] \n",
    "    zscore = summary['zscore']\n",
    "    total = summary['total_values']\n",
    "    range_orig = f\"{summary['range_original'][0]:.2f}-{summary['range_original'][1]:.2f}\"\n",
    "    range_treat = f\"{summary['range_treated'][0]:.2f}-{summary['range_treated'][1]:.2f}\"\n",
    "\n",
    "    print(f\"{col:<12} {clinical:<9,} {iqr:<8,} {zscore:<8,} {total:<8,} {range_orig:<20} {range_treat:<20}\")\n",
    "\n",
    "# Estatísticas finais\n",
    "total_clinical_corrections = sum(summary['clinical'] for summary in outliers_summary.values())\n",
    "print(f\"\\nTotal de Caps Aplicados: {total_clinical_corrections:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe2b1d",
   "metadata": {},
   "source": [
    "### 4.3 Estratégias de Imputação\n",
    "\n",
    "Implementação de diferentes técnicas de imputação baseadas no tipo de variável e percentual de missing values.\n",
    "\n",
    "**OBSERVAÇÃO:**\n",
    "As seções `4.3.1` e `4.3.2` precisam ser executadas em ordem e são necessárias para que as demais seções funcionem. Porém `4.3.3`, `4.3.4`, `4.3.5` podem ser executadas em qualquer ordem após executar `4.3.1` e `4.3.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc5442",
   "metadata": {},
   "source": [
    "#### 4.3.1 Imputação para Variáveis com Baixo Missing (<20%)\n",
    "\n",
    "Aplicação de imputação simples usando medidas centrais apropriadas para cada tipo de variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d173ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variáveis para imputação simples:\n",
      "  • HR: 90,075 valores (7.4%)\n",
      "  • O2Sat: 129,490 valores (10.7%)\n",
      "  • SBP: 148,340 valores (12.3%)\n",
      "  • MAP: 121,961 valores (10.1%)\n",
      "  • Resp: 158,019 valores (13.1%)\n",
      "  • HospAdmTime: 6 valores (0.0%)\n",
      "\n",
      "VERIFICAÇÃO PÓS-IMPUTAÇÃO:\n",
      "  • HR: 0 valores missing restantes\n",
      "  • O2Sat: 0 valores missing restantes\n",
      "  • SBP: 0 valores missing restantes\n",
      "  • MAP: 0 valores missing restantes\n",
      "  • Resp: 0 valores missing restantes\n",
      "  • HospAdmTime: 0 valores missing restantes\n",
      "\n",
      "Total de valores imputados (simples): 647,891\n",
      "Missing values restantes: 5,478,673 (28.32%)\n"
     ]
    }
   ],
   "source": [
    "# Imputação simples para variáveis com baixo missing (<20%)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "X_train_simple_imputed = X_train_outliers_treated.copy()\n",
    "\n",
    "# Identificar variáveis com baixo missing (<20%)\n",
    "low_missing_vars = []\n",
    "missing_info = {}\n",
    "\n",
    "for col in X_train_simple_imputed.columns:\n",
    "    missing_pct = (X_train_simple_imputed[col].isnull().sum() / len(X_train_simple_imputed)) * 100\n",
    "    missing_info[col] = missing_pct\n",
    "    \n",
    "    if missing_pct < 20 and missing_pct > 0:\n",
    "        low_missing_vars.append(col)\n",
    "\n",
    "\n",
    "print(f\"\\nVariáveis para imputação simples:\")\n",
    "for var in low_missing_vars:\n",
    "    missing_count = X_train_simple_imputed[var].isnull().sum()\n",
    "    missing_pct = missing_info[var]\n",
    "    print(f\"  • {var}: {missing_count:,} valores ({missing_pct:.1f}%)\")\n",
    "\n",
    "\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "X_train_simple_imputed[low_missing_vars] = numeric_imputer.fit_transform(\n",
    "    X_train_simple_imputed[low_missing_vars]\n",
    ")\n",
    "\n",
    "\n",
    "# Verificar se imputação foi bem-sucedida\n",
    "print(f\"\\nVERIFICAÇÃO PÓS-IMPUTAÇÃO:\")\n",
    "for var in low_missing_vars:\n",
    "    remaining_missing = X_train_simple_imputed[var].isnull().sum()\n",
    "    print(f\"  • {var}: {remaining_missing} valores missing restantes\")\n",
    "\n",
    "total_imputed = sum(missing_info[var] * len(X_train_simple_imputed) / 100 for var in low_missing_vars)\n",
    "print(f\"\\nTotal de valores imputados (simples): {total_imputed:,.0f}\")\n",
    "\n",
    "# Resumo do missing restante\n",
    "remaining_missing = X_train_simple_imputed.isnull().sum().sum()\n",
    "total_values = X_train_simple_imputed.size\n",
    "missing_pct_remaining = (remaining_missing / total_values) * 100\n",
    "\n",
    "print(f\"Missing values restantes: {remaining_missing:,} ({missing_pct_remaining:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a48589",
   "metadata": {},
   "source": [
    "#### 4.3.2 Separando as Demais Variáveis para Imputação Avançada \n",
    "\n",
    "Para uso de técnicas mais sofisticadas como KNN Imputer ou imputação baseada em modelos para variáveis clinicamente importantes, vamos antes definir as variáveis que ainda possuem valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d0004175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANÁLISE DINÂMICA DE MISSING VALUES:\n",
      "==================================================\n",
      "\n",
      "CLASSIFICAÇÃO POR ESTRATÉGIA DE IMPUTAÇÃO:\n",
      "Critério: Unit1/Unit2 = Regressão Logística | <40% = Regressão Linear | ≥40% = Híbrida\n",
      "-------------------------------------------------------------------------------------\n",
      "DBP             29.5    % ( 356,506 valores)\n",
      "Unit1           39.0    % ( 472,083 valores)\n",
      "Unit2           39.0    % ( 472,083 valores)\n",
      "Temp            65.3    % ( 789,252 valores)\n",
      "BUN             92.9    % (1,123,758 valores)\n",
      "WBC             93.4    % (1,129,584 valores)\n",
      "Platelets       93.9    % (1,135,407 valores)\n",
      "\n",
      "RESUMO DAS ESTRATÉGIAS:\n",
      "  • Regressão Logística (categóricas Unit1/Unit2): 2 variáveis\n",
      "  • Regressão Linear Simples (<40% missing numéricas): 1 variáveis\n",
      "  • Estratégia Híbrida (≥40% missing numéricas): 4 variáveis\n",
      "\n",
      "Variáveis para Regressão Logística: ['Unit1', 'Unit2']\n",
      "Variáveis para Regressão Linear: ['DBP']\n",
      "Variáveis para Estratégia Híbrida: ['Temp', 'BUN', 'WBC', 'Platelets']\n",
      "\n",
      "Variáveis preditoras selecionadas: ['Hour', 'HR', 'O2Sat', 'SBP', 'MAP', 'Resp', 'Gender', 'HospAdmTime', 'ICULOS']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_advanced_imputed = X_train_simple_imputed.copy()\n",
    "\n",
    "# ESTRATÉGIA DINÂMICA BASEADA EM MISSING ATUAL\n",
    "print(f\"\\nANÁLISE DINÂMICA DE MISSING VALUES:\")\n",
    "print(f\"=\"*50)\n",
    "\n",
    "# Identificar todas as variáveis com missing\n",
    "vars_with_missing = []\n",
    "for col in X_train_advanced_imputed.columns:\n",
    "    missing_count = X_train_advanced_imputed[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(X_train_advanced_imputed)) * 100\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        vars_with_missing.append({\n",
    "            'variavel': col,\n",
    "            'missing_count': missing_count,\n",
    "            'missing_pct': missing_pct,\n",
    "            'tipo': 'categorical' if X_train_advanced_imputed[col].dtype in ['object', 'category'] else 'numeric'\n",
    "        })\n",
    "\n",
    "# Separar por estratégia baseada em 40% de missing e tipo de variável\n",
    "logistic_regression_vars = []  # Unit1, Unit2: Regressão Logística (categóricas)\n",
    "linear_regression_vars = []    # < 40% missing: Regressão Linear simples (numéricas)\n",
    "hybrid_strategy_vars = []      # >= 40% missing: Estratégia Híbrida KNN + Regressão\n",
    "\n",
    "print(f\"\\nCLASSIFICAÇÃO POR ESTRATÉGIA DE IMPUTAÇÃO:\")\n",
    "print(f\"Critério: Unit1/Unit2 = Regressão Logística | <40% = Regressão Linear | ≥40% = Híbrida\")\n",
    "print(f\"-\" * 85)\n",
    "\n",
    "for var_info in sorted(vars_with_missing, key=lambda x: x['missing_pct']):\n",
    "    var = var_info['variavel']\n",
    "    missing_pct = var_info['missing_pct']\n",
    "    missing_count = var_info['missing_count']\n",
    "    \n",
    "    print(f\"{var:<15} {missing_pct:<8.1f}% ({missing_count:>8,} valores)\")\n",
    "    \n",
    "    # Separar Unit1 e Unit2 para regressão logística\n",
    "    if var in ['Unit1', 'Unit2']:\n",
    "        logistic_regression_vars.append(var)\n",
    "    elif missing_pct < 40:\n",
    "        linear_regression_vars.append(var)\n",
    "    else:\n",
    "        hybrid_strategy_vars.append(var)\n",
    "\n",
    "print(f\"\\nRESUMO DAS ESTRATÉGIAS:\")\n",
    "print(f\"  • Regressão Logística (categóricas Unit1/Unit2): {len(logistic_regression_vars)} variáveis\")\n",
    "print(f\"  • Regressão Linear Simples (<40% missing numéricas): {len(linear_regression_vars)} variáveis\")\n",
    "print(f\"  • Estratégia Híbrida (≥40% missing numéricas): {len(hybrid_strategy_vars)} variáveis\")\n",
    "\n",
    "print(f\"\\nVariáveis para Regressão Logística: {logistic_regression_vars}\")\n",
    "print(f\"Variáveis para Regressão Linear: {linear_regression_vars}\")\n",
    "print(f\"Variáveis para Estratégia Híbrida: {hybrid_strategy_vars}\")\n",
    "\n",
    "# Selecionar variáveis preditoras (sem missing ou baixo missing)\n",
    "all_vars_to_impute = logistic_regression_vars + linear_regression_vars + hybrid_strategy_vars\n",
    "predictor_vars = []\n",
    "for col in X_train_advanced_imputed.select_dtypes(include=[np.number]).columns:\n",
    "    missing_pct = (X_train_advanced_imputed[col].isnull().sum() / len(X_train_advanced_imputed)) * 100\n",
    "    if missing_pct == 0 or (missing_pct < 20 and col not in all_vars_to_impute):\n",
    "        predictor_vars.append(col)\n",
    "\n",
    "print(f\"\\nVariáveis preditoras selecionadas: {predictor_vars}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd99f40",
   "metadata": {},
   "source": [
    "#### 4.3.3 Regressão Logística para Categóricas\n",
    "\n",
    "Aqui aplicaremos o modelo de regressão logistica para imputação de Unit1 e Unit2. Mantendo a coerência do Dataset onde é mandatório que Unit1 + Unit2 = 1 para todas as instâncias (Paciente só pode ir para um dos tipos de UTI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "29d7854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit1 missing: 472,083 valores\n",
      "Unit2 missing: 472,083 valores\n",
      "Ambas missing: 472,083 valores\n",
      "\n",
      "ETAPA 1: Regressão Logística para Unit1\n",
      "ETAPA 2: Unit2 como complemento de Unit1\n",
      "Unit1 imputado: 472,083 valores\n",
      "Unit2 imputado: 472,083 valores (complemento)\n",
      "Distribuição Unit1 imputada: 0=178,891 | 1=293,192\n",
      "Distribuição Unit2 imputada: 0=293,192 | 1=178,891\n",
      "Verificação Unit1 + Unit2 = 1: ✓ VÁLIDA\n",
      "\n",
      "VERIFICAÇÃO PÓS-IMPUTAÇÃO LOGÍSTICA:\n",
      "  • Unit1: 0 valores missing restantes\n",
      "    Distribuição final: {0.0: np.int64(553649), 1.0: np.int64(655548)}\n",
      "  • Unit2: 0 valores missing restantes\n",
      "    Distribuição final: {0.0: np.int64(655548), 1.0: np.int64(553649)}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ETAPA 1: REGRESSÃO LOGÍSTICA PARA VARIÁVEIS CATEGÓRICAS (Unit1, Unit2)\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Identificar valores missing\n",
    "unit1_missing_mask = X_train_advanced_imputed['Unit1'].isnull()\n",
    "unit2_missing_mask = X_train_advanced_imputed['Unit2'].isnull()\n",
    "both_missing_mask = unit1_missing_mask & unit2_missing_mask\n",
    "\n",
    "unit1_missing_count = unit1_missing_mask.sum()\n",
    "unit2_missing_count = unit2_missing_mask.sum()\n",
    "both_missing_count = both_missing_mask.sum()\n",
    "\n",
    "print(f\"Unit1 missing: {unit1_missing_count:,} valores\")\n",
    "print(f\"Unit2 missing: {unit2_missing_count:,} valores\") \n",
    "print(f\"Ambas missing: {both_missing_count:,} valores\")\n",
    "\n",
    "print(f\"\\nETAPA 1: Regressão Logística para Unit1\")\n",
    "\n",
    "# Preparar dados para treino (onde Unit1 não é missing)\n",
    "complete_mask = ~X_train_advanced_imputed['Unit1'].isnull()\n",
    "training_size = min(100000, complete_mask.sum())\n",
    "\n",
    "training_indices = np.random.choice(\n",
    "    X_train_advanced_imputed[complete_mask].index,\n",
    "    size=training_size,\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "# Features numéricas para predição (excluir Unit1/Unit2)\n",
    "numeric_predictors = [col for col in predictor_vars if col not in logistic_regression_vars]\n",
    "\n",
    "# Treinar modelo logístico para Unit1\n",
    "X_features = X_train_advanced_imputed.loc[training_indices, numeric_predictors]\n",
    "y_target = X_train_advanced_imputed.loc[training_indices, 'Unit1']\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logistic_model.fit(X_features, y_target)\n",
    "\n",
    "# Prever Unit1 para registros missing\n",
    "X_missing_features = X_train_advanced_imputed.loc[both_missing_mask, numeric_predictors]\n",
    "predicted_unit1_proba = logistic_model.predict_proba(X_missing_features)[:, 1]\n",
    "predicted_unit1 = (predicted_unit1_proba > 0.5).astype(int)\n",
    "\n",
    "print(f\"ETAPA 2: Unit2 como complemento de Unit1\")\n",
    "# Unit2 como complemento lógico de Unit1\n",
    "predicted_unit2 = 1 - predicted_unit1\n",
    "\n",
    "# Aplicar imputações\n",
    "X_train_advanced_imputed.loc[both_missing_mask, 'Unit1'] = predicted_unit1\n",
    "X_train_advanced_imputed.loc[both_missing_mask, 'Unit2'] = predicted_unit2\n",
    "\n",
    "print(f\"Unit1 imputado: {both_missing_count:,} valores\")\n",
    "print(f\"Unit2 imputado: {both_missing_count:,} valores (complemento)\")\n",
    "\n",
    "# Estatísticas da imputação\n",
    "unit1_0_count = (predicted_unit1 == 0).sum()\n",
    "unit1_1_count = (predicted_unit1 == 1).sum()\n",
    "print(f\"Distribuição Unit1 imputada: 0={unit1_0_count:,} | 1={unit1_1_count:,}\")\n",
    "print(f\"Distribuição Unit2 imputada: 0={unit1_1_count:,} | 1={unit1_0_count:,}\")\n",
    "\n",
    "# Verificar relação complementar\n",
    "unit1_final = X_train_advanced_imputed.loc[both_missing_mask, 'Unit1']\n",
    "unit2_final = X_train_advanced_imputed.loc[both_missing_mask, 'Unit2']\n",
    "sum_check = (unit1_final + unit2_final == 1).all()\n",
    "print(f\"Verificação Unit1 + Unit2 = 1: {'✓ VÁLIDA' if sum_check else '✗ INVÁLIDA'}\")\n",
    "    \n",
    "# Verificação pós-imputação logística\n",
    "print(f\"\\nVERIFICAÇÃO PÓS-IMPUTAÇÃO LOGÍSTICA:\")\n",
    "for var in logistic_regression_vars:\n",
    "    if var in X_train_advanced_imputed.columns:\n",
    "        remaining_missing = X_train_advanced_imputed[var].isnull().sum()\n",
    "        print(f\"  • {var}: {remaining_missing} valores missing restantes\")\n",
    "        \n",
    "        # Distribuição final\n",
    "        if remaining_missing == 0:\n",
    "            value_counts = X_train_advanced_imputed[var].value_counts().sort_index()\n",
    "            print(f\"    Distribuição final: {dict(value_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8198a75",
   "metadata": {},
   "source": [
    "#### 4.3.4 Regressão Linear para <40% de Missing\n",
    "Aplicar regressão linear para variáveis numéricas com <40% de missing (DBP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67684c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETAPA 2: REGRESSÃO LINEAR SIMPLES (< 40% missing)\n",
      "\n",
      "\n",
      "---------- IMPUTANDO DBP (Regressão Simples) ----------\n",
      "Missing: 356,506 valores (29.5%)\n",
      "Regressão aplicada: 356,506 valores imputados\n",
      "Valores restantes missing: 0\n",
      "Range imputado: [2.99, 259.58]\n",
      "Range atual da variável: [2.99, 300.00]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ETAPA 2: REGRESSÃO LINEAR SIMPLES (< 40% missing)\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(f\"ETAPA 2: REGRESSÃO LINEAR SIMPLES (< 40% missing)\\n\")\n",
    "\n",
    "for target_var in linear_regression_vars:\n",
    "    print(f\"\\n---------- IMPUTANDO {target_var} (Regressão Simples) ----------\")\n",
    "    \n",
    "    # Identificar valores missing\n",
    "    missing_mask = X_train_advanced_imputed[target_var].isnull()\n",
    "    total_missing = missing_mask.sum()\n",
    "    missing_pct = (total_missing / len(X_train_advanced_imputed)) * 100\n",
    "    print(f\"Missing: {total_missing:,} valores ({missing_pct:.1f}%)\")\n",
    "\n",
    "    # Preparar dados para regressão\n",
    "    complete_mask = ~X_train_advanced_imputed[target_var].isnull()\n",
    "    training_size = min(100000, complete_mask.sum())  # Até 100k para treino\n",
    "    \n",
    "    training_indices = np.random.choice(\n",
    "        X_train_advanced_imputed[complete_mask].index,\n",
    "        size=training_size,\n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    # Features e target para treino\n",
    "    X_features = X_train_advanced_imputed.loc[training_indices, predictor_vars]\n",
    "    y_target = X_train_advanced_imputed.loc[training_indices, target_var]\n",
    "    \n",
    "    # Treinar modelo de regressão\n",
    "    reg_model = LinearRegression()\n",
    "    reg_model.fit(X_features, y_target)\n",
    "    \n",
    "    # Prever todos os valores missing\n",
    "    X_missing_features = X_train_advanced_imputed.loc[missing_mask, predictor_vars]\n",
    "    predicted_values = reg_model.predict(X_missing_features)\n",
    "    \n",
    "    # Aplicar imputação\n",
    "    X_train_advanced_imputed.loc[missing_mask, target_var] = predicted_values\n",
    "    \n",
    "    # Verificar resultado\n",
    "    final_missing = X_train_advanced_imputed[target_var].isnull().sum()\n",
    "    print(f\"Regressão aplicada: {total_missing:,} valores imputados\")\n",
    "    print(f\"Valores restantes missing: {final_missing}\")\n",
    "    print(f\"Range imputado: [{predicted_values.min():.2f}, {predicted_values.max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55862465",
   "metadata": {},
   "source": [
    "#### 4.3.5 KNNImputer + Regressão Linear para >=40% de Missing\n",
    "Aplicação do KNNImputer com 3 vizinhos para amostra de ~5% dos valores da coluna e Regressão Linear para os outros ~95%.\n",
    "\n",
    "Esse código abaixo demora em torno de 5-6min para executar. Tomar ciência disso antes rodar a célula de código abaixo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936d4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== IMPUTANDO Temp ==========\n",
      "Total de valores missing: 789,252\n",
      "\n",
      "ETAPA 1 - KNN Imputer (39,462 valores - 5%)\n",
      "  KNN aplicado com sucesso: 39,462 valores\n",
      "\n",
      "ETAPA 2 - Regressão Linear (749,790 valores - 95%)\n",
      "  Regressão aplicada com sucesso: 749,790 valores\n",
      "  Valores missing restantes: 0\n",
      "  Imputação híbrida completa para Temp!\n",
      "\n",
      "========== IMPUTANDO BUN ==========\n",
      "Total de valores missing: 1,123,758\n",
      "\n",
      "ETAPA 1 - KNN Imputer (50,000 valores - 5%)\n",
      "  KNN aplicado com sucesso: 50,000 valores\n",
      "\n",
      "ETAPA 2 - Regressão Linear (1,073,758 valores - 95%)\n",
      "  Regressão aplicada com sucesso: 1,073,758 valores\n",
      "  Valores missing restantes: 0\n",
      "  Imputação híbrida completa para BUN!\n",
      "\n",
      "========== IMPUTANDO WBC ==========\n",
      "Total de valores missing: 1,129,584\n",
      "\n",
      "ETAPA 1 - KNN Imputer (50,000 valores - 5%)\n",
      "  KNN aplicado com sucesso: 50,000 valores\n",
      "\n",
      "ETAPA 2 - Regressão Linear (1,079,584 valores - 95%)\n",
      "  Regressão aplicada com sucesso: 1,079,584 valores\n",
      "  Valores missing restantes: 0\n",
      "  Imputação híbrida completa para WBC!\n",
      "\n",
      "========== IMPUTANDO Platelets ==========\n",
      "Total de valores missing: 1,135,407\n",
      "\n",
      "ETAPA 1 - KNN Imputer (50,000 valores - 5%)\n",
      "  KNN aplicado com sucesso: 50,000 valores\n",
      "\n",
      "ETAPA 2 - Regressão Linear (1,085,407 valores - 95%)\n",
      "  Regressão aplicada com sucesso: 1,085,407 valores\n",
      "  Valores missing restantes: 0\n",
      "  Imputação híbrida completa para Platelets!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ETAPA 3: ESTRATÉGIA HÍBRIDA (≥ 40% missing)  \n",
    "# =============================================================================\n",
    "\n",
    "# Imputação avançada híbrida: KNN + Regressão Linear\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Implementar estratégia híbrida para cada variável\n",
    "for target_var in hybrid_strategy_vars:\n",
    "    print(f\"\\n========== IMPUTANDO {target_var} ==========\")\n",
    "    \n",
    "    # Identificar valores missing\n",
    "    missing_mask = X_train_advanced_imputed[target_var].isnull()\n",
    "    total_missing = missing_mask.sum()\n",
    "    print(f\"Total de valores missing: {total_missing:,}\")\n",
    "    \n",
    "    # ETAPA 1: Imputação com KNN (5% dos missing values - otimizado)\n",
    "    knn_sample_size = min(50000, int(total_missing * 0.05))  # Máximo 50k valores\n",
    "    print(f\"\\nETAPA 1 - KNN Imputer ({knn_sample_size:,} valores - 5%)\")\n",
    "    \n",
    "    # Selecionar amostra aleatória dos índices missing para KNN\n",
    "    missing_indices = X_train_advanced_imputed[missing_mask].index\n",
    "    knn_indices = np.random.choice(missing_indices, size=knn_sample_size, replace=False)\n",
    "    \n",
    "    # Preparar subset para KNN (incluir valores não-missing para treino)\n",
    "    mask_not_missing = ~X_train_advanced_imputed[target_var].isnull()\n",
    "    knn_training_size = min(10000, mask_not_missing.sum())  # Reduzir para 10k treino\n",
    "    training_indices = np.random.choice(\n",
    "        X_train_advanced_imputed[mask_not_missing].index, \n",
    "        size=knn_training_size, \n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    # Combinar índices para KNN: treino + amostra para imputação\n",
    "    knn_all_indices = np.concatenate([training_indices, knn_indices])\n",
    "    knn_subset = X_train_advanced_imputed.loc[knn_all_indices, predictor_vars + [target_var]].copy()\n",
    "    \n",
    "    # Aplicar KNN apenas no subset\n",
    "    knn_imputer = KNNImputer(n_neighbors=3, weights='uniform')\n",
    "    knn_imputed = knn_imputer.fit_transform(knn_subset)\n",
    "    \n",
    "    # Extrair valores imputados para a variável alvo\n",
    "    target_col_idx = list(knn_subset.columns).index(target_var)\n",
    "    knn_imputed_values = knn_imputed[-knn_sample_size:, target_col_idx]\n",
    "    \n",
    "    # Atualizar valores no dataset\n",
    "    X_train_advanced_imputed.loc[knn_indices, target_var] = knn_imputed_values\n",
    "    print(f\"  KNN aplicado com sucesso: {knn_sample_size:,} valores\")\n",
    "    \n",
    "    # ETAPA 2: Imputação com Regressão Linear (95% restante)\n",
    "    remaining_missing_mask = X_train_advanced_imputed[target_var].isnull()\n",
    "    remaining_missing_count = remaining_missing_mask.sum()\n",
    "    print(f\"\\nETAPA 2 - Regressão Linear ({remaining_missing_count:,} valores - 95%)\")\n",
    "    \n",
    "    # Preparar dados para regressão (usar todos os dados completos)\n",
    "    complete_mask = ~X_train_advanced_imputed[target_var].isnull()\n",
    "    reg_training_size = min(50000, complete_mask.sum())\n",
    "    \n",
    "    reg_training_indices = np.random.choice(\n",
    "        X_train_advanced_imputed[complete_mask].index,\n",
    "        size=reg_training_size,\n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    # Features e target para treino\n",
    "    X_features = X_train_advanced_imputed.loc[reg_training_indices, predictor_vars]\n",
    "    y_target = X_train_advanced_imputed.loc[reg_training_indices, target_var]\n",
    "    \n",
    "    # Treinar modelo de regressão\n",
    "    reg_model = LinearRegression()\n",
    "    reg_model.fit(X_features, y_target)\n",
    "    \n",
    "    # Prever valores restantes\n",
    "    X_missing_features = X_train_advanced_imputed.loc[remaining_missing_mask, predictor_vars]\n",
    "    predicted_values = reg_model.predict(X_missing_features)\n",
    "    \n",
    "    X_train_advanced_imputed.loc[remaining_missing_mask, target_var] = predicted_values\n",
    "    print(f\"  Regressão aplicada com sucesso: {remaining_missing_count:,} valores\")\n",
    "            \n",
    "    \n",
    "    # Verificar se imputação foi completa\n",
    "    final_missing = X_train_advanced_imputed[target_var].isnull().sum()\n",
    "    print(f\"  Valores missing restantes: {final_missing}\")\n",
    "    print(f\"  Imputação híbrida completa para {target_var}!\")\n",
    "\n",
    "print(\"Imputação Concluída!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f67ae7",
   "metadata": {},
   "source": [
    "#### 4.3.6 Aplicando os Limites Pós-Imputação\n",
    "\n",
    "Definimos o range de valores segundo a análise feita na seção 4.2 para não permitir que nenhum outlier imputado ultrapasse o intervalo observado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c8deb201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "APLICAÇÃO DE CAPS PÓS-IMPUTAÇÃO\n",
      "============================================================\n",
      "  • Temp: Nenhuma correção necessária\n",
      "  • BUN: Nenhuma correção necessária\n",
      "  • WBC: Nenhuma correção necessária\n",
      "  • Platelets: Nenhuma correção necessária\n",
      "  • DBP: Nenhuma correção necessária\n",
      "\n",
      "Total de caps aplicados: 0\n",
      "\n",
      "RANGES APÓS APLICAÇÃO DE CAPS:\n",
      "  • Temp: 28.00 - 42.00\n",
      "  • BUN: 1.00 - 268.00\n",
      "  • WBC: 0.10 - 440.00\n",
      "  • Platelets: 2.00 - 2322.00\n",
      "  • DBP: 20.00 - 300.00\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APLICAÇÃO DE CAPS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"APLICAÇÃO DE CAPS PÓS-IMPUTAÇÃO\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# Definir ranges tratados do outliers_summary (Seção 4.2)\n",
    "caps_ranges = {\n",
    "    'Temp': {'min': 28.00, 'max': 42.00},\n",
    "    'BUN': {'min': 1.00, 'max': 268.00},\n",
    "    'WBC': {'min': 0.10, 'max': 440.00},\n",
    "    'Platelets': {'min': 2.00, 'max': 2322.00},\n",
    "    'DBP': {'min': 20.00, 'max': 300.00}\n",
    "}\n",
    "\n",
    "# Aplicar caps nas variáveis imputadas\n",
    "total_caps_applied = 0\n",
    "\n",
    "for var in caps_ranges.keys():\n",
    "    if var in X_train_advanced_imputed.columns:\n",
    "        min_cap = caps_ranges[var]['min']\n",
    "        max_cap = caps_ranges[var]['max']\n",
    "        \n",
    "        # Contabilizar valores fora dos limites antes da correção\n",
    "        below_min = (X_train_advanced_imputed[var] < min_cap).sum()\n",
    "        above_max = (X_train_advanced_imputed[var] > max_cap).sum()\n",
    "        total_corrections = below_min + above_max\n",
    "        \n",
    "        if total_corrections > 0:\n",
    "            print(f\"  • {var}: {below_min:,} valores < {min_cap}, {above_max:,} valores > {max_cap}\")\n",
    "            \n",
    "            # Aplicar caps\n",
    "            X_train_advanced_imputed[var] = X_train_advanced_imputed[var].clip(\n",
    "                lower=min_cap, \n",
    "                upper=max_cap\n",
    "            )\n",
    "            \n",
    "            total_caps_applied += total_corrections\n",
    "        else:\n",
    "            print(f\"  • {var}: Nenhuma correção necessária\")\n",
    "\n",
    "print(f\"\\nTotal de caps aplicados: {total_caps_applied:,}\")\n",
    "\n",
    "# Verificar ranges após caps\n",
    "print(f\"\\nRANGES APÓS APLICAÇÃO DE CAPS:\")\n",
    "for var in caps_ranges.keys():\n",
    "    if var in X_train_advanced_imputed.columns:\n",
    "        min_val = X_train_advanced_imputed[var].min()\n",
    "        max_val = X_train_advanced_imputed[var].max()\n",
    "        print(f\"  • {var}: {min_val:.2f} - {max_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf38c5",
   "metadata": {},
   "source": [
    "#### 4.3.7 Verificação Final Pós-Imputação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782dd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFICAÇÃO FINAL PÓS-IMPUTAÇÃO\n",
      "============================================================\n",
      "\n",
      "VALIDAÇÃO DE RANGES PÓS-IMPUTAÇÃO:\n",
      "  • DBP: min=20.00, max=300.00, mean=62.67\n",
      "  • Temp: min=28.00, max=42.00, mean=36.96\n",
      "  • BUN: min=1.00, max=268.00, mean=24.28\n",
      "  • WBC: min=0.10, max=440.00, mean=11.37\n",
      "  • Platelets: min=2.00, max=2322.00, mean=196.22\n",
      "  • Unit1: min=0.00, max=1.00, mean=0.54\n",
      "  • Unit2: min=0.00, max=1.00, mean=0.46\n",
      "\n",
      "VERIFICAÇÃO GERAL DE MISSING:\n",
      "  ✓ NENHUMA VARIÁVEL COM MISSING RESTANTE!\n",
      "\n",
      "Dataset após imputação avançada: (1209197, 16)\n",
      "Missing values finais: 0 (0.000%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VERIFICAÇÃO FINAL PÓS-IMPUTAÇÃO\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"VERIFICAÇÃO FINAL PÓS-IMPUTAÇÃO\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# Verificar todas as variáveis processadas\n",
    "all_processed_vars = linear_regression_vars + hybrid_strategy_vars + logistic_regression_vars\n",
    "\n",
    "print(f\"\\nVALIDAÇÃO DE RANGES PÓS-IMPUTAÇÃO:\")\n",
    "\n",
    "# Variáveis processadas\n",
    "numeric_processed = linear_regression_vars + hybrid_strategy_vars + logistic_regression_vars\n",
    "for var in numeric_processed:\n",
    "    if var in X_train_advanced_imputed.columns:\n",
    "        min_val = X_train_advanced_imputed[var].min()\n",
    "        max_val = X_train_advanced_imputed[var].max()\n",
    "        mean_val = X_train_advanced_imputed[var].mean()\n",
    "        print(f\"  • {var}: min={min_val:.2f}, max={max_val:.2f}, mean={mean_val:.2f}\")\n",
    "\n",
    "\n",
    "# Verificar se ainda há variáveis com missing\n",
    "print(f\"\\nVERIFICAÇÃO GERAL DE MISSING:\")\n",
    "total_vars_with_missing = 0\n",
    "for col in X_train_advanced_imputed.columns:\n",
    "    missing_count = X_train_advanced_imputed[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        missing_pct = (missing_count / len(X_train_advanced_imputed)) * 100\n",
    "        print(f\"  • {col}: {missing_count:,} valores ({missing_pct:.1f}%)\")\n",
    "        total_vars_with_missing += 1\n",
    "\n",
    "if total_vars_with_missing == 0:\n",
    "    print(\"NENHUMA VARIÁVEL COM MISSING RESTANTE!\")\n",
    "\n",
    "\n",
    "print(f\"\\nDataset após imputações: {X_train_advanced_imputed.shape}\")\n",
    "\n",
    "# Status final do missing\n",
    "final_missing = X_train_advanced_imputed.isnull().sum().sum()\n",
    "total_values = X_train_advanced_imputed.size\n",
    "final_missing_pct = (final_missing / total_values) * 100\n",
    "\n",
    "print(f\"Missing values finais: {final_missing:,} ({final_missing_pct:.3f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc4c04",
   "metadata": {},
   "source": [
    "### 4.4 Validação da Qualidade Pós-Limpeza\n",
    "\n",
    "Aplicação de regressão logística para imputação de variáveis categóricas Unit1 e Unit2, mantendo a relação complementar Unit1 + Unit2 = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2ff98118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDAÇÃO DA QUALIDADE PÓS-LIMPEZA:\n",
      "=============================================\n",
      "1. VERIFICAÇÃO DE COMPLETUDE:\n",
      "   • Total de valores missing: 0\n",
      "   • Percentual de missing: 0.0000%\n",
      "   • Completude do dataset: 100.0000%\n",
      "\n",
      "2. VERIFICAÇÃO DE CONSISTÊNCIA LÓGICA:\n",
      "   • Inconsistências SBP < DBP: 293\n",
      "   • O2Sat fora do range 0-100%: 0\n",
      "   • Temperatura fora do range 25-45°C: 0\n",
      "   • Total de inconsistências detectadas: 293\n",
      "\n",
      "3. VERIFICAÇÃO DE DISTRIBUIÇÕES:\n",
      "   • Variáveis numéricas analisadas: 16\n",
      "   • Estatísticas das principais variáveis:\n",
      "     Variável     Mean     Median   Std      Min      Max      Skew  \n",
      "     ------------------------------------------------------------\n",
      "     Hour         25.8     20.0     29.1     0.0      335.0    4.08  \n",
      "     HR           84.5     83.5     16.7     20.0     250.0    0.46  \n",
      "     O2Sat        97.3     98.0     2.8      20.0     100.0    -4.34 \n",
      "     Temp         37.0     36.9     0.5      28.0     42.0     -0.27 \n",
      "     SBP          123.4    121.0    21.8     20.0     300.0    0.64  \n",
      "\n",
      "4. VERIFICAÇÃO DE INTEGRIDADE:\n",
      "   • Shape do X_train: (1209197, 16)\n",
      "   • Shape do y_train: (1209197,)\n",
      "   • Índices alinhados: True\n",
      "\n",
      "5. RESUMO FINAL DA QUALIDADE:\n",
      "   • Dataset original: (1241768, 41)\n",
      "   • Dataset final limpo: (1209197, 16)\n",
      "   • Variáveis removidas: 25\n",
      "   • Registros removidos: 32,571\n",
      "   • Completude final: 100.0000%\n",
      "   • Inconsistências restantes: 293\n",
      "\n",
      "Datasets finais prontos para próximas etapas:\n",
      "   • X_train_final_cleaned: (1209197, 16)\n",
      "   • y_train_final_cleaned: (1209197,)\n"
     ]
    }
   ],
   "source": [
    "# Validação da qualidade dos dados após todas as etapas de limpeza\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"VALIDAÇÃO DA QUALIDADE PÓS-LIMPEZA:\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "X_train_final_cleaned = X_train_advanced_imputed.copy()\n",
    "y_train_final_cleaned = y_train_cleaned.copy()\n",
    "\n",
    "# 1. Verificação de Completude\n",
    "print(\"1. VERIFICAÇÃO DE COMPLETUDE:\")\n",
    "total_missing = X_train_final_cleaned.isnull().sum().sum()\n",
    "total_values = X_train_final_cleaned.size\n",
    "missing_pct = (total_missing / total_values) * 100\n",
    "\n",
    "print(f\"   • Total de valores missing: {total_missing:,}\")\n",
    "print(f\"   • Percentual de missing: {missing_pct:.4f}%\")\n",
    "print(f\"   • Completude do dataset: {100-missing_pct:.4f}%\")\n",
    "\n",
    "if total_missing > 0:\n",
    "    print(\"   • Variáveis com missing restante:\")\n",
    "    for col in X_train_final_cleaned.columns:\n",
    "        missing_count = X_train_final_cleaned[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_pct_col = (missing_count / len(X_train_final_cleaned)) * 100\n",
    "            print(f\"     - {col}: {missing_count:,} ({missing_pct_col:.2f}%)\")\n",
    "\n",
    "# 2. Verificação de Consistência Lógica\n",
    "print(f\"\\n2. VERIFICAÇÃO DE CONSISTÊNCIA LÓGICA:\")\n",
    "\n",
    "# Verificar ranges fisicamente possíveis\n",
    "consistency_issues = 0\n",
    "\n",
    "# Pressão arterial: SBP >= DBP\n",
    "if 'SBP' in X_train_final_cleaned.columns and 'DBP' in X_train_final_cleaned.columns:\n",
    "    pressure_inconsistent = (X_train_final_cleaned['SBP'] < X_train_final_cleaned['DBP']).sum()\n",
    "    print(f\"   • Inconsistências SBP < DBP: {pressure_inconsistent:,}\")\n",
    "    consistency_issues += pressure_inconsistent\n",
    "\n",
    "# Saturação de oxigênio: 0-100%\n",
    "if 'O2Sat' in X_train_final_cleaned.columns:\n",
    "    o2sat_invalid = ((X_train_final_cleaned['O2Sat'] < 0) | (X_train_final_cleaned['O2Sat'] > 100)).sum()\n",
    "    print(f\"   • O2Sat fora do range 0-100%: {o2sat_invalid:,}\")\n",
    "    consistency_issues += o2sat_invalid\n",
    "\n",
    "# Temperatura: range humano aceitável\n",
    "if 'Temp' in X_train_final_cleaned.columns:\n",
    "    temp_invalid = ((X_train_final_cleaned['Temp'] < 25) | (X_train_final_cleaned['Temp'] > 45)).sum()\n",
    "    print(f\"   • Temperatura fora do range 25-45°C: {temp_invalid:,}\")\n",
    "    consistency_issues += temp_invalid\n",
    "\n",
    "print(f\"   • Total de inconsistências detectadas: {consistency_issues:,}\")\n",
    "\n",
    "# 3. Verificação de Distribuições\n",
    "print(f\"\\n3. VERIFICAÇÃO DE DISTRIBUIÇÕES:\")\n",
    "\n",
    "# Análise estatística básica para variáveis numéricas\n",
    "numeric_cols = X_train_final_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"   • Variáveis numéricas analisadas: {len(numeric_cols)}\")\n",
    "\n",
    "distributions_summary = {}\n",
    "for col in numeric_cols[:5]:  # Primeiras 5 variáveis para exemplo\n",
    "    data = X_train_final_cleaned[col].dropna()\n",
    "    if len(data) > 0:\n",
    "        distributions_summary[col] = {\n",
    "            'mean': data.mean(),\n",
    "            'median': data.median(),  \n",
    "            'std': data.std(),\n",
    "            'min': data.min(),\n",
    "            'max': data.max(),\n",
    "            'skewness': data.skew()\n",
    "        }\n",
    "\n",
    "print(f\"   • Estatísticas das principais variáveis:\")\n",
    "print(f\"     {'Variável':<12} {'Mean':<8} {'Median':<8} {'Std':<8} {'Min':<8} {'Max':<8} {'Skew':<6}\")\n",
    "print(\"     \" + \"-\"*60)\n",
    "\n",
    "for var, stats in distributions_summary.items():\n",
    "    print(f\"     {var:<12} {stats['mean']:<8.1f} {stats['median']:<8.1f} {stats['std']:<8.1f} {stats['min']:<8.1f} {stats['max']:<8.1f} {stats['skewness']:<6.2f}\")\n",
    "\n",
    "# 4. Verificação de Integridade Referencial\n",
    "print(f\"\\n4. VERIFICAÇÃO DE INTEGRIDADE:\")\n",
    "print(f\"   • Shape do X_train: {X_train_final_cleaned.shape}\")\n",
    "print(f\"   • Shape do y_train: {y_train_final_cleaned.shape}\")\n",
    "print(f\"   • Índices alinhados: {X_train_final_cleaned.index.equals(y_train_final_cleaned.index)}\")\n",
    "\n",
    "# 5. Resumo Final da Qualidade\n",
    "print(f\"\\n5. RESUMO FINAL DA QUALIDADE:\")\n",
    "print(f\"   • Dataset original: {X_train.shape}\")\n",
    "print(f\"   • Dataset final limpo: {X_train_final_cleaned.shape}\")\n",
    "print(f\"   • Variáveis removidas: {X_train.shape[1] - X_train_final_cleaned.shape[1]}\")\n",
    "print(f\"   • Registros removidos: {X_train.shape[0] - X_train_final_cleaned.shape[0]:,}\")\n",
    "print(f\"   • Completude final: {100-missing_pct:.4f}%\")\n",
    "print(f\"   • Inconsistências restantes: {consistency_issues:,}\")\n",
    "\n",
    "# Salvar datasets finais limpos\n",
    "print(f\"\\nDatasets finais prontos para próximas etapas:\")\n",
    "print(f\"   • X_train_final_cleaned: {X_train_final_cleaned.shape}\")\n",
    "print(f\"   • y_train_final_cleaned: {y_train_final_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea04b5f",
   "metadata": {},
   "source": [
    "## Resumo da Tarefa 2: Limpeza dos Dados\n",
    "\n",
    "### Resultados Alcançados\n",
    "\n",
    "**1. Detecção e Remoção de Duplicatas**\n",
    "- Duplicatas exatas removidas: 32,571 registros\n",
    "- Dataset reduzido: 1,241,768 → 1,209,197 registros (-2.6%)\n",
    "- Duplicatas em sinais vitais detectadas: 131,810 (mantidas por serem clinicamente válidas)\n",
    "\n",
    "**2. Tratamento de Outliers**\n",
    "- Estratégia: Capping baseado em limites clínicos\n",
    "- Correções aplicadas em variáveis como HR, Temp, SBP, etc.\n",
    "- Preservação dos dados com ajuste aos ranges médicos aceitáveis\n",
    "\n",
    "**3. Imputação de Missing Values**\n",
    "- **Imputação Simples (missing <20%)**: 6 variáveis processadas\n",
    "  - Total imputado: 647,891 valores\n",
    "  - Método: Mediana para variáveis numéricas\n",
    "- **Imputação Avançada (variáveis estratégicas)**: 2 variáveis\n",
    "  - WBC e Platelets: Regressão linear com sampling otimizado\n",
    "  - Total imputado: 2,264,991 valores\n",
    "\n",
    "**4. Qualidade Final**\n",
    "- Dataset final: 1,209,197 × 16 variáveis\n",
    "- Missing values restantes: 16.61% (principalmente Temp e BUN)\n",
    "- Completude geral: 83.39%\n",
    "- Status de qualidade: ACEITÁVEL\n",
    "- Inconsistências lógicas: Controladas dentro de limites clínicos\n",
    "\n",
    "### Variáveis com Missing Restante\n",
    "- **Temp**: 66.2% missing (mantida por alta separabilidade)\n",
    "- **BUN**: 93.1% missing (mantida por alta separabilidade)\n",
    "- **DBP**: 29.7% missing (sinal vital secundário)\n",
    "\n",
    "### Próximos Passos\n",
    "Dataset limpo e pronto para:\n",
    "- Feature Engineering (Tarefa 3)\n",
    "- Formatação final para modelagem\n",
    "- Aplicação das mesmas transformações no conjunto de teste  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6547ba43",
   "metadata": {},
   "source": [
    "## 5. TAREFA 3: Construção dos Dados (Feature Engineering)\n",
    "\n",
    "**Objetivo:** Criar novas variáveis ou atributos derivados dos dados existentes que possam melhorar o poder preditivo do modelo.\n",
    "\n",
    "**Estratégias de construção:**\n",
    "* Ratios clínicos baseados em literatura médica\n",
    "* Features temporais derivadas de Hour/ICULOS\n",
    "* Interações entre variáveis relacionadas\n",
    "* Transformações para normalizar distribuições"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b47e5",
   "metadata": {},
   "source": [
    "### 5.1 Criação de Ratios Clínicos\n",
    "\n",
    "Desenvolvimento de índices e ratios clinicamente estabelecidos para detecção de sepsis (ex: razão neutrófilos/linfócitos, índices de choque)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f3254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para criação de ratios clínicos\n",
    "# Implementar ratios estabelecidos na literatura médica\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf11848",
   "metadata": {},
   "source": [
    "### 5.2 Features Temporais\n",
    "\n",
    "Criação de variáveis derivadas das informações temporais para capturar padrões de risco ao longo do tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c71a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para features temporais\n",
    "# Categorização por janelas de risco, tendências temporais\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63655ae0",
   "metadata": {},
   "source": [
    "### 5.3 Interações entre Variáveis\n",
    "\n",
    "Criação de features que capturam interações sinérgicas entre variáveis clínicas relacionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd20a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para features de interação\n",
    "# Produtos, somas ponderadas, interações não-lineares\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5fddce",
   "metadata": {},
   "source": [
    "### 5.4 Transformações de Distribuição\n",
    "\n",
    "Aplicação de transformações matemáticas para normalizar distribuições assimétricas identificadas na EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para transformações\n",
    "# Log, Box-Cox, transformações de potência\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da84ed86",
   "metadata": {},
   "source": [
    "## 6. TAREFA 4: Integração dos Dados\n",
    "\n",
    "**Objetivo:** Combinar dados de diferentes fontes em um único conjunto de dados consistente.\n",
    "\n",
    "**Observação:** Esta tarefa não se aplica ao dataset atual, pois trabalhamos com uma única fonte (PhysioNet 2019 Challenge). Esta seção documenta a estrutura para futuras expansões do projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d4688",
   "metadata": {},
   "source": [
    "### 6.1 Documentação da Fonte Única\n",
    "\n",
    "Registro da origem e características do dataset único utilizado no projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ba396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentar características da fonte única\n",
    "dataset_info = {\n",
    "    'source': 'PhysioNet 2019 Challenge',\n",
    "    'description': 'Early Detection of Sepsis from Clinical Data',\n",
    "    'patients': 'Multiple ICU patients with temporal observations',\n",
    "    'timeframe': 'Variable length ICU stays',\n",
    "    'completeness': 'Single comprehensive source'\n",
    "}\n",
    "\n",
    "print(\"INFORMAÇÕES DA FONTE DE DADOS:\")\n",
    "for key, value in dataset_info.items():\n",
    "    print(f\"  • {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34748720",
   "metadata": {},
   "source": [
    "### 6.2 Preparação para Futuras Integrações\n",
    "\n",
    "Estrutura preparatória para possível integração com outras fontes de dados em versões futuras do projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para estrutura de integração futura\n",
    "# Definir schemas, chaves de junção, protocolos de merge\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ab110d",
   "metadata": {},
   "source": [
    "## 7. TAREFA 5: Formatação dos Dados\n",
    "\n",
    "**Objetivo:** Preparar os dados no formato necessário para os algoritmos de modelagem, incluindo normalização, encoding e divisão final dos conjuntos.\n",
    "\n",
    "**Atividades principais:**\n",
    "* Normalização/padronização de variáveis numéricas\n",
    "* Encoding de variáveis categóricas  \n",
    "* Balanceamento de classes\n",
    "* Criação dos datasets finais para modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a1b02",
   "metadata": {},
   "source": [
    "### 7.1 Normalização e Padronização\n",
    "\n",
    "Aplicação de transformações de escala para garantir que todas as variáveis numéricas tenham contribuições equilibradas nos algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20340ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para normalização\n",
    "# StandardScaler, MinMaxScaler baseado na distribuição das variáveis\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdab3d8",
   "metadata": {},
   "source": [
    "### 7.2 Encoding de Variáveis Categóricas\n",
    "\n",
    "Conversão de variáveis categóricas para formato numérico apropriado para algoritmos de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para encoding categórico\n",
    "# One-hot encoding, label encoding baseado na cardinalidade\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78947d7e",
   "metadata": {},
   "source": [
    "### 7.3 Balanceamento de Classes\n",
    "\n",
    "Implementação de técnicas para lidar com o severo desbalanceamento entre classes (98.2% vs 1.8%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd89bef",
   "metadata": {},
   "source": [
    "#### 7.3.1 Análise de Estratégias de Balanceamento\n",
    "\n",
    "Comparação de diferentes abordagens: oversampling, undersampling e métodos combinados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para comparação de estratégias\n",
    "# SMOTE, Random Over/Under Sampling, SMOTETomek\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a7ea5",
   "metadata": {},
   "source": [
    "#### 7.3.2 Implementação da Estratégia Escolhida\n",
    "\n",
    "Aplicação da técnica de balanceamento selecionada com base na análise comparativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para implementação do balanceamento\n",
    "# Aplicar técnica escolhida e validar resultados\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65297c42",
   "metadata": {},
   "source": [
    "### 7.4 Criação dos Datasets Finais\n",
    "\n",
    "Montagem dos conjuntos de dados finais prontos para a fase de modelagem, incluindo validação da integridade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para datasets finais\n",
    "# Criar X_train_final, X_test_final, y_train_final, y_test_final\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f08b9",
   "metadata": {},
   "source": [
    "### 7.5 Validação Final e Export\n",
    "\n",
    "Verificação final da qualidade e consistência dos dados preparados, seguida do salvamento dos datasets processados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6dd45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para validação final\n",
    "# Verificar shapes, tipos, ranges, consistência lógica\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a347d3",
   "metadata": {},
   "source": [
    "## 8. Resumo da Preparação\n",
    "\n",
    "**Síntese das transformações aplicadas:** Documentação completa de todas as modificações realizadas nos dados durante o processo de preparação.\n",
    "\n",
    "**Datasets resultantes:** Características finais dos conjuntos de dados prontos para modelagem.\n",
    "\n",
    "**Próximos passos:** Direcionamento para a fase de modelagem do CRISP-DM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fccc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder para resumo final\n",
    "# Documentar todas as transformações e características finais\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023cc45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Próxima Fase:** Modeling (3-modeling.ipynb)\n",
    "\n",
    "**Entregáveis desta fase:**\n",
    "- Datasets limpos e preparados\n",
    "- Features engineered com relevância clínica  \n",
    "- Classes balanceadas adequadamente\n",
    "- Documentação completa das transformações\n",
    "- Validação da qualidade dos dados processados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
