{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3671cc",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o de Bibliotecas e Configura√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o das bibliotecas necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import ast\n",
    "from joblib import load\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de plotagem\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e4c11",
   "metadata": {},
   "source": [
    "## 2. Recupera√ß√£o dos Resultados dos Modelos Treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b39365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_results(results_folder='modeling/results'):\n",
    "    \"\"\"Carrega todos os resultados de avalia√ß√£o salvos dos notebooks individuais\"\"\"\n",
    "    all_results = {}\n",
    "\n",
    "    if not os.path.exists(results_folder):\n",
    "        print(f\"Pasta {results_folder} n√£o encontrada!\")\n",
    "        print(\"Execute os notebooks individuais dos modelos primeiro.\")\n",
    "        return all_results\n",
    "\n",
    "    # Mapear arquivos para modelos\n",
    "    # Comentar as linhas dos modelos que voc√™ n√£o deseja carregar\n",
    "    result_files = {\n",
    "        'knn_results.json': 'KNN',\n",
    "        'lvq_results.json': 'LVQ', \n",
    "        'svm_results.json': 'SVM',\n",
    "        'rf_results.json': 'Random Forest',\n",
    "        'dt_results.json': 'Decision Tree'\n",
    "    }\n",
    "\n",
    "    for filename, model_name in result_files.items():\n",
    "        filepath = os.path.join(results_folder, filename)\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            try:\n",
    "                with open(filepath, 'r') as f:\n",
    "                    result_data = json.load(f)\n",
    "                all_results[model_name] = result_data\n",
    "                print(f\"{model_name}: Carregado com sucesso\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao carregar {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"Arquivo n√£o encontrado: {filename}\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = load_model_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea2990",
   "metadata": {},
   "source": [
    "## 3. Prepara√ß√£o dos Dados para Plotagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame com m√©tricas dos modelos para plotagem\n",
    "def create_metrics_dataframe(final_results):\n",
    "    \"\"\"Cria DataFrame com as m√©tricas de todos os modelos para facilitar plotagem\"\"\"\n",
    "    metrics_data = []\n",
    "    \n",
    "    for model_name, results in final_results.items():\n",
    "        train_metrics = results['train_metrics']\n",
    "        test_metrics = results['test_metrics']\n",
    "        \n",
    "        metrics_data.append({\n",
    "            'Modelo': model_name,\n",
    "            'Acur√°cia_Teste': test_metrics['accuracy_score'],\n",
    "            'F1_CV': results.get('best_cv_score', 0.0),  # Score do CV (melhor configura√ß√£o)\n",
    "            'F1_Treino': train_metrics['f1'],\n",
    "            'F1_Teste': test_metrics['f1'],\n",
    "            'Precis√£o_Teste': test_metrics['precision'],\n",
    "            'Recall_Teste': test_metrics['recall'],\n",
    "            'G-Mean_Teste': test_metrics['gmean']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(metrics_data)\n",
    "\n",
    "# Criar o DataFrame de m√©tricas\n",
    "if final_results:\n",
    "    metrics_df = create_metrics_dataframe(final_results)\n",
    "    print(f\"Modelos dispon√≠veis: {list(final_results.keys())}\")\n",
    "    print(f\"\\nPrimeiras linhas do DataFrame:\")\n",
    "    print(metrics_df.head())\n",
    "else:\n",
    "    print(\"Nenhum resultado encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b49d2",
   "metadata": {},
   "source": [
    "## 4. Gr√°ficos de Compara√ß√£o de M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Gr√°ficos de compara√ß√£o\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Gr√°fico 1: Compara√ß√£o de F1-Score\n",
    "ax1 = axes[0]\n",
    "x_pos = np.arange(len(metrics_df))\n",
    "ax1.bar(x_pos - 0.2, metrics_df['F1_CV'], 0.4, label='F1-Score CV', alpha=0.8)\n",
    "ax1.bar(x_pos + 0.2, metrics_df['F1_Teste'], 0.4, label='F1-Score Teste', alpha=0.8)\n",
    "ax1.set_xlabel('Modelos')\n",
    "ax1.set_ylabel('F1-Score')\n",
    "ax1.set_title('Compara√ß√£o F1-Score: CV vs Teste')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(metrics_df['Modelo'], rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico 2: Compara√ß√£o de m√∫ltiplas m√©tricas\n",
    "ax2 = axes[1]\n",
    "metrics_to_plot = ['Acur√°cia_Teste', 'Precis√£o_Teste', 'Recall_Teste']\n",
    "x_pos = np.arange(len(metrics_df))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    ax2.bar(x_pos + (i-1.5)*width, metrics_df[metric], width, \n",
    "            label=metric.replace('_Teste', ''), alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Modelos')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Compara√ß√£o de M√∫ltiplas M√©tricas')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(metrics_df['Modelo'], rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico 3: Boxplot para verificar overfitting\n",
    "ax3 = axes[2]\n",
    "overfitting_data = []\n",
    "models_names = []\n",
    "for model_name, results in final_results.items():\n",
    "    f1_train = results['train_metrics']['f1']\n",
    "    f1_test = results['test_metrics']['f1']\n",
    "    overfitting_data.append([f1_train, f1_test])\n",
    "    models_names.append(model_name)\n",
    "\n",
    "overfitting_df = pd.DataFrame(overfitting_data, \n",
    "                             columns=['Treino', 'Teste'], \n",
    "                             index=models_names)\n",
    "\n",
    "overfitting_df.plot(kind='bar', ax=ax3, alpha=0.8)\n",
    "ax3.set_title('Overfitting: F1-Score Treino vs Teste')\n",
    "ax3.set_ylabel('F1-Score')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.tick_params(axis='x', rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2099ab47",
   "metadata": {},
   "source": [
    "## 5. Matrizes de Confus√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(final_results):\n",
    "    \"\"\"Plota matrizes de confus√£o individuais para cada modelo - adaptado para SVHN (10 classes)\"\"\"\n",
    "    \n",
    "    if not final_results:\n",
    "        print(\"‚ùå Nenhum resultado dispon√≠vel para plotar matrizes de confus√£o.\")\n",
    "        return\n",
    "    \n",
    "    # Obter os labels verdadeiros da primeira predi√ß√£o dispon√≠vel\n",
    "    first_model_name = next(iter(final_results.keys()))\n",
    "    y_test = final_results[first_model_name]['test_labels']\n",
    "    \n",
    "    print(f\"üìä qPlotando matrizes de confus√£o para {len(final_results)} modelo(s)\")\n",
    "    print(f\"üéØ Dataset SVHN: 10 classes (d√≠gitos 0-9)\")\n",
    "    print(f\"üìà Total de amostras de teste: {len(y_test):,}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Plotar matrizes de confus√£o - uma por modelo\n",
    "    for model_name, results in final_results.items():\n",
    "        y_pred = results['predictions']\n",
    "        \n",
    "        # Matriz de confus√£o\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Criar figura individual para cada matriz\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "        \n",
    "        plt.title(f'Matriz de Confus√£o - {model_name}', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xlabel('Classe Predita', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('Classe Real', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Labels das classes SVHN (d√≠gitos 0-9)\n",
    "        class_labels = [f'D√≠gito {i}' for i in range(10)]\n",
    "        plt.xticks(np.arange(10) + 0.5, range(10), rotation=0)\n",
    "        plt.yticks(np.arange(10) + 0.5, range(10), rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Estat√≠sticas detalhadas da matriz de confus√£o\n",
    "        accuracy = np.trace(cm) / np.sum(cm)\n",
    "        total_samples = np.sum(cm)\n",
    "        \n",
    "        print(f\"\\\\nüìä {model_name} - Detalhes da Matriz de Confus√£o:\")\n",
    "        print(f\"  ‚úÖ Predi√ß√µes Corretas: {np.trace(cm):,}\")\n",
    "        print(f\"  ‚ùå Predi√ß√µes Incorretas: {total_samples - np.trace(cm):,}\")\n",
    "        print(f\"  üìà Taxa de Acerto: {accuracy:.1%}\")\n",
    "        print(f\"  üìä Total de amostras: {total_samples:,}\")\n",
    "        \n",
    "        # Mostrar classes mais confundidas\n",
    "        np.fill_diagonal(cm, 0)  # Remover diagonal para ver apenas erros\n",
    "        max_confusion_idx = np.unravel_index(np.argmax(cm), cm.shape)\n",
    "        max_confusion_value = cm[max_confusion_idx]\n",
    "        \n",
    "        if max_confusion_value > 0:\n",
    "            print(f\"  üîÑ Maior confus√£o: D√≠gito {max_confusion_idx[0]} ‚Üí D√≠gito {max_confusion_idx[1]} ({max_confusion_value} casos)\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# Plotar matrizes se temos dados\n",
    "if final_results:\n",
    "    plot_confusion_matrices(final_results)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Aguardando resultados dos modelos para plotar matrizes de confus√£o.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7b19f",
   "metadata": {},
   "source": [
    "## 6. Visualiza√ß√£o Compacta de Todas as Matrizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_confusion_matrices_grid(final_results):\n",
    "    \"\"\"Plota todas as matrizes de confus√£o em uma √∫nica figura (estilo grid)\"\"\"\n",
    "    \n",
    "    if not final_results:\n",
    "        print(\"‚ùå Nenhum resultado dispon√≠vel.\")\n",
    "        return\n",
    "    \n",
    "    n_models = len(final_results)\n",
    "    \n",
    "    # Calcular layout do grid\n",
    "    if n_models <= 2:\n",
    "        rows, cols = 1, n_models\n",
    "        figsize = (6*n_models, 6)\n",
    "    elif n_models <= 4:\n",
    "        rows, cols = 2, 2\n",
    "        figsize = (12, 12)\n",
    "    else:\n",
    "        rows = (n_models + 2) // 3\n",
    "        cols = 3\n",
    "        figsize = (18, 6*rows)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    fig.suptitle('Matrizes de Confus√£o - Todos os Modelos (SVHN)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Se h√° apenas um modelo, axes n√£o √© uma lista\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes if hasattr(axes, '__iter__') else [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Obter labels verdadeiros\n",
    "    first_model_name = next(iter(final_results.keys()))\n",
    "    y_test = final_results[first_model_name]['test_labels']\n",
    "    \n",
    "    model_idx = 0\n",
    "    for model_name, results in final_results.items():\n",
    "        y_pred = results['predictions']\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        ax = axes[model_idx]\n",
    "        \n",
    "        # Plot da matriz\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   square=True, linewidths=0.5, \n",
    "                   cbar_kws={\"shrink\": .8}, ax=ax)\n",
    "        \n",
    "        ax.set_title(f'{model_name}', fontweight='bold', fontsize=12)\n",
    "        ax.set_xlabel('Predito', fontsize=10)\n",
    "        ax.set_ylabel('Real', fontsize=10)\n",
    "        \n",
    "        # Labels menores para economizar espa√ßo\n",
    "        ax.set_xticklabels(range(10), fontsize=8)\n",
    "        ax.set_yticklabels(range(10), fontsize=8, rotation=0)\n",
    "        \n",
    "        model_idx += 1\n",
    "    \n",
    "    # Esconder axes extras se houver\n",
    "    for idx in range(model_idx, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Resumo comparativo\n",
    "    print(\"\\\\nüìä Resumo Comparativo - Acur√°cia por Classe:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Criar uma tabela de acur√°cia por classe\n",
    "    class_accuracies = {}\n",
    "    \n",
    "    for model_name, results in final_results.items():\n",
    "        y_pred = results['predictions']\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Calcular acur√°cia por classe (diagonal / soma da linha)\n",
    "        class_acc = []\n",
    "        for i in range(10):\n",
    "            if cm[i].sum() > 0:\n",
    "                acc = cm[i, i] / cm[i].sum()\n",
    "                class_acc.append(acc)\n",
    "            else:\n",
    "                class_acc.append(0.0)\n",
    "        \n",
    "        class_accuracies[model_name] = class_acc\n",
    "    \n",
    "    # Converter para DataFrame para visualiza√ß√£o\n",
    "    acc_df = pd.DataFrame(class_accuracies, index=[f'D√≠gito {i}' for i in range(10)])\n",
    "    print(acc_df.round(3))\n",
    "\n",
    "# Plotar grid compacto se temos dados\n",
    "if final_results:\n",
    "    plot_all_confusion_matrices_grid(final_results)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Aguardando resultados dos modelos para plotar grid de matrizes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae08c4a",
   "metadata": {},
   "source": [
    "## 7. Relat√≥rio Final de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49fba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report(final_results, metrics_df):\n",
    "    \"\"\"Gera um relat√≥rio final consolidado com ranking dos modelos\"\"\"\n",
    "    \n",
    "    if not final_results or metrics_df.empty:\n",
    "        print(\"‚ùå N√£o h√° dados suficientes para gerar o relat√≥rio.\")\n",
    "        return\n",
    "    \n",
    "    # Ranking por F1-Score no teste\n",
    "    ranking_f1 = metrics_df.sort_values('F1_Teste', ascending=False)\n",
    "    \n",
    "    print(\"\\nü•á RANKING POR F1-SCORE (TESTE):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(ranking_f1.iterrows(), 1):\n",
    "        medal = \"ü•á\" if idx == 1 else \"ü•à\" if idx == 2 else \"ü•â\" if idx == 3 else f\"{idx}¬∫\"\n",
    "        print(f\"{medal} {row['Modelo']}: {row['F1_Teste']:.4f}\")\n",
    "    \n",
    "    # Ranking por Acur√°cia no teste\n",
    "    ranking_acc = metrics_df.sort_values('Acur√°cia_Teste', ascending=False)\n",
    "    \n",
    "    print(\"\\nüéØ RANKING POR ACUR√ÅCIA (TESTE):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(ranking_acc.iterrows(), 1):\n",
    "        medal = \"ü•á\" if idx == 1 else \"ü•à\" if idx == 2 else \"ü•â\" if idx == 3 else f\"{idx}¬∫\"\n",
    "        print(f\"{medal} {row['Modelo']}: {row['Acur√°cia_Teste']:.4f}\")\n",
    "    \n",
    "    \n",
    "    # Melhor modelo geral\n",
    "    print(\"\\nüèÜ MODELO RECOMENDADO:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    # Crit√©rio: melhor F1 no teste com menor overfitting\n",
    "    best_model_idx = ranking_f1.index[0]\n",
    "    best_model = ranking_f1.loc[best_model_idx]\n",
    "    \n",
    "    print(f\"ü•á {best_model['Modelo']}\")\n",
    "    print(f\"   üìà F1-Score (Teste): {best_model['F1_Teste']:.4f}\")\n",
    "    print(f\"   üéØ Acur√°cia (Teste): {best_model['Acur√°cia_Teste']:.4f}\")\n",
    "    print(f\"   üìä Precis√£o: {best_model['Precis√£o_Teste']:.4f}\")\n",
    "    print(f\"   üîç Recall: {best_model['Recall_Teste']:.4f}\")\n",
    "    \n",
    "    # Informa√ß√µes adicionais do melhor modelo\n",
    "    best_model_name = best_model['Modelo']\n",
    "    if best_model_name in final_results:\n",
    "        best_results = final_results[best_model_name]\n",
    "        \n",
    "        print(f\"\\nüîß CONFIGURA√á√ÉO DO MELHOR MODELO:\")\n",
    "        print(\"-\" * 35)\n",
    "        if 'best_params' in best_results:\n",
    "            for param, value in best_results['best_params'].items():\n",
    "                print(f\"   {param}: {value}\")\n",
    "        \n",
    "        if 'evaluation_info' in best_results:\n",
    "            eval_info = best_results['evaluation_info']\n",
    "            print(f\"\\nüìä INFORMA√á√ïES DE AVALIA√á√ÉO:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"   Amostras de treino: {eval_info.get('train_samples_used', 'N/A'):,}\")\n",
    "            print(f\"   Amostras de teste: {eval_info.get('test_samples_used', 'N/A'):,}\")\n",
    "    \n",
    "    # An√°lise de desempenho por classe (se dispon√≠vel)\n",
    "    print(f\"\\nüìã AN√ÅLISE DE DESEMPENHO POR CLASSE:\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    if best_model_name in final_results:\n",
    "        best_results = final_results[best_model_name]\n",
    "        y_test = best_results['test_labels']\n",
    "        y_pred = best_results['predictions']\n",
    "        \n",
    "        # Calcular m√©tricas por classe\n",
    "        from sklearn.metrics import classification_report\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        print(\"Desempenho por d√≠gito (F1-Score):\")\n",
    "        for digit in range(10):\n",
    "            if str(digit) in report:\n",
    "                f1_digit = report[str(digit)]['f1-score']\n",
    "                support = report[str(digit)]['support']\n",
    "                status = \"üü¢\" if f1_digit > 0.8 else \"üü°\" if f1_digit > 0.6 else \"üî¥\"\n",
    "                print(f\"   D√≠gito {digit}: {f1_digit:.3f} ({support:,} amostras) {status}\")\n",
    "        \n",
    "        # Macro e weighted averages\n",
    "        print(f\"\\n   üìä Macro F1: {report['macro avg']['f1-score']:.4f}\")\n",
    "        print(f\"   ‚öñÔ∏è  Weighted F1: {report['weighted avg']['f1-score']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Relat√≥rio gerado com sucesso!\")\n",
    "\n",
    "# Gerar relat√≥rio final se temos dados\n",
    "if final_results and not metrics_df.empty:\n",
    "    generate_final_report(final_results, metrics_df)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Execute os notebooks dos modelos primeiro para gerar o relat√≥rio.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
