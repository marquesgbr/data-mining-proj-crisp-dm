{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ba373a",
   "metadata": {},
   "source": [
    "### 1.1 Importar Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa60bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas com sucesso!\n",
      "Pandas: 2.3.2\n",
      "NumPy: 2.3.3\n",
      "Scikit-learn: 1.7.2\n",
      "XGBoost: 3.1.2\n"
     ]
    }
   ],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, roc_auc_score, roc_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import uniform, randint\n",
    "from joblib import dump, load\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importar funções dos módulos customizados\n",
    "from ml_utils import gmean_score, evaluate_model, load_and_prepare_datasets\n",
    "from search_utils import (plot_search_history, multiple_randomized_search,\n",
    "                          plot_search_history_from_loaded, \n",
    "                          load_search_results, get_best_params_from_saved,\n",
    "                          save_search_results, save_final_results, DEFAULT_CV_STRATEGY)\n",
    "\n",
    "# Configurações de plotagem\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"XGBoost: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247fcd8c",
   "metadata": {},
   "source": [
    "### 1.2 Definir Nome do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e20e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir nome do modelo para uso em salvamento e exibição\n",
    "MODEL_NAME = \"Stacking\"\n",
    "print(f\"Modelo: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab4a6c",
   "metadata": {},
   "source": [
    "### 1.3 Definir Comitê Heterogêneo (Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ec8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comitê Heterogêneo (Stacking) com Decision Tree, Random Forest e XGBoost definido com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# COMITÊ HETEROGÊNEO (STACKING) - IMPLEMENTAÇÃO COM WEAK LEARNERS\n",
    "# ======================================================================\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "class HeterogeneousStackingCommittee(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, \n",
    "                 # === DECISION TREE - Weak Learner ===\n",
    "                 dt_max_depth=3,\n",
    "                 dt_min_samples_split=20,\n",
    "                 dt_min_samples_leaf=10,\n",
    "                 dt_criterion='gini',\n",
    "                 dt_max_features='sqrt',\n",
    "                 \n",
    "                 # === MLP - Weak Learner ===\n",
    "                 mlp_hidden_layers=(50,),\n",
    "                 mlp_alpha=1.0,\n",
    "                 mlp_learning_rate_init=0.01,\n",
    "                 mlp_max_iter=100,\n",
    "                 mlp_activation='relu',\n",
    "                 \n",
    "                 # === XGBOOST - Weak Learner ===\n",
    "                 xgb_n_estimators=30,\n",
    "                 xgb_max_depth=2,\n",
    "                 xgb_learning_rate=0.05,\n",
    "                 xgb_subsample=0.5,\n",
    "                 xgb_colsample_bytree=0.5,\n",
    "                 xgb_min_child_weight=5,\n",
    "                 xgb_gamma=0.1,\n",
    "                 xgb_reg_alpha=1.0,\n",
    "                 xgb_reg_lambda=1.0,\n",
    "                 \n",
    "                 # === META-ESTIMADOR ===\n",
    "                 meta_C=1.0,\n",
    "                 meta_max_iter=1000,\n",
    "                 \n",
    "                 # === CONFIGURAÇÕES GERAIS ===\n",
    "                 cv=5,\n",
    "                 random_state=None):\n",
    "        \"\"\"\n",
    "        Comitê Heterogêneo com Weak Learners Diversos\n",
    "        \n",
    "        Estimadores Base (Weak Learners):\n",
    "        ----------------------------------\n",
    "        - Decision Tree: Árvore rasa com regras simples\n",
    "        - MLP: Rede neural shallow com regularização forte\n",
    "        - XGBoost: Gradient boosting conservador\n",
    "        \n",
    "        Meta-Estimador:\n",
    "        ---------------\n",
    "        - Logistic Regression: Aprende combinação linear ótima\n",
    "        \n",
    "        Estratégia:\n",
    "        -----------\n",
    "        - Cada modelo é \"fraco\" individualmente (~60-70% acurácia)\n",
    "        - Diversidade de paradigmas garante erros não-correlacionados\n",
    "        - Stacking combina forças e corrige fraquezas\n",
    "        \"\"\"\n",
    "        # Decision Tree\n",
    "        self.dt_max_depth = dt_max_depth\n",
    "        self.dt_min_samples_split = dt_min_samples_split\n",
    "        self.dt_min_samples_leaf = dt_min_samples_leaf\n",
    "        self.dt_criterion = dt_criterion\n",
    "        self.dt_max_features = dt_max_features\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp_hidden_layers = mlp_hidden_layers\n",
    "        self.mlp_alpha = mlp_alpha\n",
    "        self.mlp_learning_rate_init = mlp_learning_rate_init\n",
    "        self.mlp_max_iter = mlp_max_iter\n",
    "        self.mlp_activation = mlp_activation\n",
    "        \n",
    "        # XGBoost\n",
    "        self.xgb_n_estimators = xgb_n_estimators\n",
    "        self.xgb_max_depth = xgb_max_depth\n",
    "        self.xgb_learning_rate = xgb_learning_rate\n",
    "        self.xgb_subsample = xgb_subsample\n",
    "        self.xgb_colsample_bytree = xgb_colsample_bytree\n",
    "        self.xgb_min_child_weight = xgb_min_child_weight\n",
    "        self.xgb_gamma = xgb_gamma\n",
    "        self.xgb_reg_alpha = xgb_reg_alpha\n",
    "        self.xgb_reg_lambda = xgb_reg_lambda\n",
    "        \n",
    "        # Meta-estimador\n",
    "        self.meta_C = meta_C\n",
    "        self.meta_max_iter = meta_max_iter\n",
    "        \n",
    "        # Geral\n",
    "        self.cv = cv\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Criar weak estimators\n",
    "        base_estimators = [\n",
    "            ('weak_dt', DecisionTreeClassifier(\n",
    "                max_depth=self.dt_max_depth,\n",
    "                min_samples_split=self.dt_min_samples_split,\n",
    "                min_samples_leaf=self.dt_min_samples_leaf,\n",
    "                criterion=self.dt_criterion,\n",
    "                max_features=self.dt_max_features,\n",
    "                random_state=self.random_state\n",
    "            )),\n",
    "            ('weak_mlp', MLPClassifier(\n",
    "                hidden_layer_sizes=self.mlp_hidden_layers,\n",
    "                alpha=self.mlp_alpha,\n",
    "                learning_rate_init=self.mlp_learning_rate_init,\n",
    "                max_iter=self.mlp_max_iter,\n",
    "                activation=self.mlp_activation,\n",
    "                solver='lbfgs',\n",
    "                early_stopping=True,\n",
    "                validation_fraction=0.2,\n",
    "                random_state=self.random_state,\n",
    "                verbose=False\n",
    "            )),\n",
    "            ('weak_xgb', xgb.XGBClassifier(\n",
    "                n_estimators=self.xgb_n_estimators,\n",
    "                max_depth=self.xgb_max_depth,\n",
    "                learning_rate=self.xgb_learning_rate,\n",
    "                subsample=self.xgb_subsample,\n",
    "                colsample_bytree=self.xgb_colsample_bytree,\n",
    "                min_child_weight=self.xgb_min_child_weight,\n",
    "                gamma=self.xgb_gamma,\n",
    "                reg_alpha=self.xgb_reg_alpha,\n",
    "                reg_lambda=self.xgb_reg_lambda,\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='logloss',\n",
    "                random_state=self.random_state,\n",
    "                verbosity=0\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        # Meta-estimador\n",
    "        meta_estimator = LogisticRegression(\n",
    "            C=self.meta_C,\n",
    "            max_iter=self.meta_max_iter,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Criar Stacking\n",
    "        self.stacking_classifier = StackingClassifier(\n",
    "            estimators=base_estimators,\n",
    "            final_estimator=meta_estimator,\n",
    "            cv=self.cv,\n",
    "            stack_method='predict_proba',\n",
    "            n_jobs=1\n",
    "        )\n",
    "        \n",
    "        # Treinar\n",
    "        self.stacking_classifier.fit(X, y)\n",
    "        self.classes_ = self.stacking_classifier.classes_\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.stacking_classifier.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.stacking_classifier.predict_proba(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return accuracy_score(y, self.predict(X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88b273",
   "metadata": {},
   "source": [
    "### 1.4 Carregar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando datasets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de treino: (853006, 19)\n",
      "Dataset de teste: (215171, 19)\n",
      "\n",
      "Distribuição das classes:\n",
      "Treino: {0.0: 831112, 1.0: 21894}\n",
      "Teste: {0.0: 209675, 1.0: 5496}\n",
      "\n",
      "Distribuição das classes:\n",
      "Treino: {0.0: 831112, 1.0: 21894}\n",
      "Teste: {0.0: 209675, 1.0: 5496}\n"
     ]
    }
   ],
   "source": [
    "# Carregamento e preparação inicial dos dados\n",
    "print(\"Carregando datasets...\")\n",
    "\n",
    "# Carregar e preparar datasets usando função do módulo\n",
    "(X_train, X_test, y_train, y_test, \n",
    " X_train_scaled, X_test_scaled, \n",
    " train_data, test_data, scaler) = load_and_prepare_datasets()\n",
    "\n",
    "print(f\"Dataset de treino: {train_data.shape}\")\n",
    "print(f\"Dataset de teste: {test_data.shape}\")\n",
    "\n",
    "print(\"\\nDistribuição das classes:\")\n",
    "print(\"Treino:\", y_train.value_counts().to_dict())\n",
    "print(\"Teste:\", y_test.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68e2b91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Hour",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "HR",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "O2Sat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Temp",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SBP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DBP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Resp",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "BUN",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "WBC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Platelets",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Gender",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Unit1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Unit2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "HospAdmTime",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ICULOS",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Critical_Risk_Window",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Time_Category",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SepsisLabel",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "35d75c7f-cfa3-4f39-8c69-412f6b51502f",
       "rows": [
        [
         "0",
         "8",
         "-1.1097944366836745",
         "-0.4601797972892955",
         "-0.936182396179708",
         "2.87336491135296",
         "3.044200612267326",
         "2.159974443333858",
         "-0.0736012350859598",
         "-0.2771860514420799",
         "-1.39357951670772",
         "0.4507157624350807",
         "1.0",
         "1.0",
         "0.0",
         "-12.06",
         "9.0",
         "0",
         "0",
         "0.0"
        ],
        [
         "1",
         "47",
         "0.5699710093354073",
         "-2.437770480712409",
         "0.1734770445575556",
         "0.3939602121785264",
         "0.6507831363841678",
         "0.4309426287535458",
         "-0.9973237405476244",
         "0.3091707774149635",
         "0.2456159065449318",
         "-0.2751078157149356",
         "1.0",
         "1.0",
         "0.0",
         "-0.05",
         "48.0",
         "0",
         "1",
         "0.0"
        ],
        [
         "2",
         "6",
         "0.1500296478306369",
         "0.978067972472969",
         "0.0161139406337461",
         "-0.9834868429183812",
         "-0.5538501262934915",
         "-0.1986829337362194",
         "-0.0736012350859598",
         "-0.310738624275361",
         "0.1213677719195975",
         "-0.1912664785221958",
         "1.0",
         "1.0",
         "0.0",
         "-0.02",
         "7.0",
         "0",
         "0",
         "0.0"
        ],
        [
         "3",
         "39",
         "-0.2699117136741334",
         "0.2589440875918367",
         "0.2893547834449668",
         "0.8531092305441623",
         "0.4050938398230071",
         "-0.1154602785560446",
         "0.7520149316733524",
         "0.2985914374982416",
         "-0.003964924479364",
         "-3.1529195642522194",
         "0.0",
         "0.0",
         "1.0",
         "-75.85",
         "43.0",
         "0",
         "1",
         "0.0"
        ],
        [
         "4",
         "127",
         "0.5699710093354073",
         "-0.4601797972892955",
         "0.0070124212356604",
         "0.7612794268710351",
         "1.8342546872935597",
         "1.0550498228769023",
         "-0.5209867333216573",
         "0.7021396354059021",
         "-0.1805445000968947",
         "0.0304330288288612",
         "0.0",
         "0.0",
         "1.0",
         "-0.03",
         "128.0",
         "1",
         "2",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>BUN</th>\n",
       "      <th>WBC</th>\n",
       "      <th>Platelets</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Unit1</th>\n",
       "      <th>Unit2</th>\n",
       "      <th>HospAdmTime</th>\n",
       "      <th>ICULOS</th>\n",
       "      <th>Critical_Risk_Window</th>\n",
       "      <th>Time_Category</th>\n",
       "      <th>SepsisLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>-1.109794</td>\n",
       "      <td>-0.460180</td>\n",
       "      <td>-0.936182</td>\n",
       "      <td>2.873365</td>\n",
       "      <td>3.044201</td>\n",
       "      <td>2.159974</td>\n",
       "      <td>-0.073601</td>\n",
       "      <td>-0.277186</td>\n",
       "      <td>-1.393580</td>\n",
       "      <td>0.450716</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.06</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>0.569971</td>\n",
       "      <td>-2.437770</td>\n",
       "      <td>0.173477</td>\n",
       "      <td>0.393960</td>\n",
       "      <td>0.650783</td>\n",
       "      <td>0.430943</td>\n",
       "      <td>-0.997324</td>\n",
       "      <td>0.309171</td>\n",
       "      <td>0.245616</td>\n",
       "      <td>-0.275108</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.150030</td>\n",
       "      <td>0.978068</td>\n",
       "      <td>0.016114</td>\n",
       "      <td>-0.983487</td>\n",
       "      <td>-0.553850</td>\n",
       "      <td>-0.198683</td>\n",
       "      <td>-0.073601</td>\n",
       "      <td>-0.310739</td>\n",
       "      <td>0.121368</td>\n",
       "      <td>-0.191266</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>-0.269912</td>\n",
       "      <td>0.258944</td>\n",
       "      <td>0.289355</td>\n",
       "      <td>0.853109</td>\n",
       "      <td>0.405094</td>\n",
       "      <td>-0.115460</td>\n",
       "      <td>0.752015</td>\n",
       "      <td>0.298591</td>\n",
       "      <td>-0.003965</td>\n",
       "      <td>-3.152920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-75.85</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>127</td>\n",
       "      <td>0.569971</td>\n",
       "      <td>-0.460180</td>\n",
       "      <td>0.007012</td>\n",
       "      <td>0.761279</td>\n",
       "      <td>1.834255</td>\n",
       "      <td>1.055050</td>\n",
       "      <td>-0.520987</td>\n",
       "      <td>0.702140</td>\n",
       "      <td>-0.180545</td>\n",
       "      <td>0.030433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hour        HR     O2Sat      Temp       SBP       MAP       DBP      Resp  \\\n",
       "0     8 -1.109794 -0.460180 -0.936182  2.873365  3.044201  2.159974 -0.073601   \n",
       "1    47  0.569971 -2.437770  0.173477  0.393960  0.650783  0.430943 -0.997324   \n",
       "2     6  0.150030  0.978068  0.016114 -0.983487 -0.553850 -0.198683 -0.073601   \n",
       "3    39 -0.269912  0.258944  0.289355  0.853109  0.405094 -0.115460  0.752015   \n",
       "4   127  0.569971 -0.460180  0.007012  0.761279  1.834255  1.055050 -0.520987   \n",
       "\n",
       "        BUN       WBC  Platelets  Gender  Unit1  Unit2  HospAdmTime  ICULOS  \\\n",
       "0 -0.277186 -1.393580   0.450716     1.0    1.0    0.0       -12.06     9.0   \n",
       "1  0.309171  0.245616  -0.275108     1.0    1.0    0.0        -0.05    48.0   \n",
       "2 -0.310739  0.121368  -0.191266     1.0    1.0    0.0        -0.02     7.0   \n",
       "3  0.298591 -0.003965  -3.152920     0.0    0.0    1.0       -75.85    43.0   \n",
       "4  0.702140 -0.180545   0.030433     0.0    0.0    1.0        -0.03   128.0   \n",
       "\n",
       "   Critical_Risk_Window  Time_Category  SepsisLabel  \n",
       "0                     0              0          0.0  \n",
       "1                     0              1          0.0  \n",
       "2                     0              0          0.0  \n",
       "3                     0              1          0.0  \n",
       "4                     1              2          0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef93427",
   "metadata": {},
   "source": [
    "## 2. Sampling para Busca de Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a10ac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPARAÇÃO DE AMOSTRA PARA BUSCA DE HIPERPARÂMETROS ===\n",
      "Dataset original de treino: 853,006 amostras\n",
      "Amostra para busca de hiperparâmetros: 8,531 amostras\n",
      "Redução: 99.0%\n",
      "\n",
      "Distribuição das classes na amostra:\n",
      "Amostra: {0.0: 8312, 1.0: 219}\n",
      "Original: {0.0: 831112, 1.0: 21894}\n",
      "Dataset original de treino: 853,006 amostras\n",
      "Amostra para busca de hiperparâmetros: 8,531 amostras\n",
      "Redução: 99.0%\n",
      "\n",
      "Distribuição das classes na amostra:\n",
      "Amostra: {0.0: 8312, 1.0: 219}\n",
      "Original: {0.0: 831112, 1.0: 21894}\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# SAMPLING ESTRATIFICADO PARA BUSCA DE HIPERPARÂMETROS\n",
    "# ======================================================================\n",
    "\n",
    "print(\"=== PREPARAÇÃO DE AMOSTRA PARA BUSCA DE HIPERPARÂMETROS ===\")\n",
    "\n",
    "# Amostra estratificada do dataset de treino (muito pequena devido à complexidade)\n",
    "_, X_sample, _, y_sample = train_test_split(\n",
    "    X_train_scaled, y_train, \n",
    "    test_size=0.01, \n",
    "    stratify=y_train,\n",
    "    random_state=10\n",
    ")\n",
    "\n",
    "print(f\"Dataset original de treino: {X_train_scaled.shape[0]:,} amostras\")\n",
    "print(f\"Amostra para busca de hiperparâmetros: {X_sample.shape[0]:,} amostras\")\n",
    "print(f\"Redução: {(1 - X_sample.shape[0]/X_train_scaled.shape[0])*100:.1f}%\")\n",
    "\n",
    "print(\"\\nDistribuição das classes na amostra:\")\n",
    "print(\"Amostra:\", pd.Series(y_sample).value_counts().to_dict())\n",
    "print(\"Original:\", y_train.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5045f5bf",
   "metadata": {},
   "source": [
    "## 3.1 Definir Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5214135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração da validação cruzada estratificada\n",
    "cv_strategy = DEFAULT_CV_STRATEGY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda55987",
   "metadata": {},
   "source": [
    "## 4. Stacking - Busca de Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f34c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# 4.1 BUSCA DE HIPERPARAMETROS - WEAK LEARNERS\n",
    "# ======================================================================\n",
    "\n",
    "print(f\"=== BUSCA DE HIPERPARÂMETROS - {MODEL_NAME} ===\")\n",
    "print(\"Estratégia: Weak Learners (versões restritas dos modelos individuais)\")\n",
    "print(\"  - Decision Tree: Versão rasa e conservadora\")\n",
    "print(\"  - MLP: Rede neural shallow com regularização forte\")\n",
    "print(\"  - XGBoost: Gradient boosting fraco e conservador\\n\")\n",
    "\n",
    "# Definição do Espaço de Hiperparâmetros para Weak Learners\n",
    "# Cada range é uma VERSÃO MENOR do espaço original do modelo individual\n",
    "\n",
    "param_distributions = {\n",
    "    # === DECISION TREE - Weak (comparar com notebook 3) ===\n",
    "    # Original: max_depth(3-50), min_samples_split(2-40), min_samples_leaf(1-50)\n",
    "    'dt_max_depth': randint(1, 5),              # Weak: 1-4 (vs 3-50)\n",
    "    'dt_min_samples_split': randint(10, 30),    # Weak: 10-29 (vs 2-40)\n",
    "    'dt_min_samples_leaf': randint(15, 50),     # Weak: 15-49 (vs 1-50)\n",
    "    \n",
    "    # === MLP - Weak (comparar com notebook 8) ===\n",
    "    # Original: 8 arquiteturas (até 4 camadas), alpha(0.01-0.1), LR(0.001-0.1), iter(200-600)\n",
    "    'mlp_hidden_layers': [(30,), (50,), (100,), (150,), (200,), # Weak: 1-2 camadas (vs até 4)\n",
    "                          (50, 25), (100, 50), (100, 100), (100)\n",
    "                          (50, 50), (25, 200), (25, 25), (50, 200),\n",
    "                          (150, 50), (100, 150), (150, 25), ()], \n",
    "    'mlp_alpha': uniform(0.05, 0.15),             # Weak: 0.05-0.20 (vs 0.01-0.1)\n",
    "    'mlp_learning_rate_init': uniform(0.001, 0.05), # Weak: 0.001-0.051 (vs 0.001-0.1)\n",
    "    'mlp_max_iter': [50, 100, 150],               # Weak: 50-150 (vs 200-600)\n",
    "    'mlp_activation': ['relu', 'tanh'],           # Subset de ['relu', 'tanh', 'logistic']\n",
    "    \n",
    "    # === XGBOOST - Weak (comparar com notebook 6) ===\n",
    "    # Original: n_est(140-381), depth(3-11), LR(0.5-1.5), subsample(0.6-0.9), reg(0.5-1.5)\n",
    "    'xgb_n_estimators': randint(20, 60),        # Weak: 20-59 (vs 140-381)\n",
    "    'xgb_max_depth': randint(1, 4),             # Weak: 1-3 (vs 3-11)\n",
    "    'xgb_learning_rate': uniform(0.01, 0.15),   # Weak: 0.01-0.16 (vs 0.5-1.5)\n",
    "    'xgb_subsample': uniform(0.3, 0.3),         # Weak: 0.3-0.6 (vs 0.6-0.9)\n",
    "    'xgb_colsample_bytree': uniform(0.3, 0.3),  # Weak: 0.3-0.6 (vs 0.2-0.7)\n",
    "    'xgb_min_child_weight': randint(5, 15),     # Weak: 5-14 (vs 1-20) \n",
    "    'xgb_gamma': uniform(0.1, 0.3),             # Weak: 0.1-0.4 (vs 0.1-0.5)\n",
    "    'xgb_reg_alpha': uniform(0.5, 0.5),         # Weak: 0.5-1.0 (vs 0.5-2.0) \n",
    "    'xgb_reg_lambda': uniform(0.5, 0.5),        # Weak: 0.5-1.0 (vs 0.5-2.0)\n",
    "    \n",
    "    # === META-ESTIMADOR ===\n",
    "    'meta_C': uniform(0.1, 10),                 # LogisticRegression\n",
    "    'meta_max_iter': [1000, 2000, 3000],\n",
    "}\n",
    "\n",
    "# Múltiplas execuções do RandomizedSearchCV\n",
    "print(f\"Iniciando busca de hiperparâmetros para {MODEL_NAME}...\")\n",
    "model_search, model_all_searches, best_params = multiple_randomized_search(\n",
    "    estimator=HeterogeneousStackingCommittee(random_state=42, cv=3),\n",
    "    param_distributions=param_distributions,\n",
    "    X=X_sample,                  \n",
    "    y=y_sample,\n",
    "    cv_strategy=cv_strategy,\n",
    "    n_searches=20,  \n",
    "    n_iter_per_search=2,       \n",
    "    scoring='f1',\n",
    "    n_jobs=1,  # Processamento sequencial\n",
    ")\n",
    "\n",
    "# Seleção da Melhor Configuração\n",
    "print(f\"\\n--- RESULTADOS {MODEL_NAME} ---\")\n",
    "print(\"Melhores hiperparâmetros:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nMelhor F1-Score (CV): {model_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e072ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registro de Desempenho - plotar evolução\n",
    "plot_search_history(model_all_searches, model_search, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c802f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# ANÁLISE DAS MELHORES CONFIGURAÇÕES ENCONTRADAS\n",
    "# ======================================================================\n",
    "\n",
    "print(f\"=== MELHORES CONFIGURAÇÕES ENCONTRADAS POR BUSCA - {MODEL_NAME} ===\")\n",
    "\n",
    "# Extrair os melhores resultados de cada busca\n",
    "best_configs = []\n",
    "\n",
    "for i, search_result in enumerate(model_all_searches):\n",
    "    config = {\n",
    "        'Busca': i + 1,\n",
    "        'F1_Score': search_result['best_score'],\n",
    "        **search_result['best_params']\n",
    "    }\n",
    "    best_configs.append(config)\n",
    "\n",
    "# Criar DataFrame e exibir top configs\n",
    "results_df = pd.DataFrame(best_configs)\n",
    "results_df = results_df.sort_values('F1_Score', ascending=False).round(4)\n",
    "\n",
    "print(f\"\\nTop configurações (de {len(results_df)} buscas):\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nEstatísticas dos F1-Scores encontrados:\")\n",
    "print(f\"  Média: {results_df['F1_Score'].mean():.4f}\")\n",
    "print(f\"  Mediana: {results_df['F1_Score'].median():.4f}\")\n",
    "print(f\"  Desvio padrão: {results_df['F1_Score'].std():.4f}\")\n",
    "print(f\"  Min: {results_df['F1_Score'].min():.4f}\")\n",
    "print(f\"  Max: {results_df['F1_Score'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc179c4e",
   "metadata": {},
   "source": [
    "## 5. Salvar Resultados de Busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar Resultados da Busca de Hiperparâmetros usando função do módulo\n",
    "search_df = save_search_results(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_search=model_search,\n",
    "    model_all_searches=model_all_searches,\n",
    "    n_searches=5,\n",
    "    n_iter_per_search=3,\n",
    "    scoring='f1',\n",
    "    cv_folds=3,\n",
    "    top_params_columns=['rf_n_estimators', 'svm_C', 'knn_n_neighbors', 'meta_C', 'cv'],\n",
    "    searches_folder='searches'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda1278",
   "metadata": {},
   "source": [
    "## 5.2 Carregar Resultado de busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4.2 Carregar Resultados Salvos (Função Auxiliar)\n",
    "# Exemplo de uso da função (não executar se já temos os resultados)\n",
    "loaded_results = load_search_results(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f841878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar a história da busca a partir dos resultados carregados\n",
    "plot_search_history_from_loaded(loaded_results, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### 4.3 Recuperar Melhores Parâmetros para Uso Posterior\n",
    "# Exemplo de uso (descomente se precisar carregar parâmetros salvos):\n",
    "if 'loaded_results' in locals():\n",
    "    best_params = get_best_params_from_saved(MODEL_NAME)\n",
    "    if best_params:\n",
    "        print(f\"✅ Parâmetros carregados: {best_params}\")\n",
    "    best_score = loaded_results['summary']['best_overall_score']\n",
    "    print(f\"✅ Melhor F1-Score carregado: {best_score:.4f}\")\n",
    "else:\n",
    "    best_params = model_search.best_params_\n",
    "    best_score = model_search.best_score_\n",
    "    print(f\"✅ Usando parâmetros da busca atual: {best_params}\")\n",
    "    print(f\"✅ Melhor F1-Score da busca atual: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821aef65",
   "metadata": {},
   "source": [
    "## 6. Treinar Modelo Final e Salvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento Final com melhores hiperparâmetros\n",
    "best_model = HeterogeneousStackingCommittee(**best_params, random_state=42)\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nModelo final {MODEL_NAME} treinado: {best_model}\")\n",
    "\n",
    "# Criar pasta se não existir\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Salvar modelo treinado\n",
    "dump(best_model, f'models/{MODEL_NAME.lower()}_trained.joblib')\n",
    "print(f\"Modelo salvo: models/{MODEL_NAME.lower()}_trained.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b617a75",
   "metadata": {},
   "source": [
    "## 7. Avaliação Final e Salvamento dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo\n",
    "loaded_model = load(f'models/{MODEL_NAME.lower()}_trained.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17206097",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"=== AVALIAÇÃO E SALVAMENTO DOS RESULTADOS - {MODEL_NAME} ===\")\n",
    "\n",
    "# Criar pastas se não existirem\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Avaliação completa do modelo\n",
    "print(\"\\nAvaliando performance do modelo...\")\n",
    "\n",
    "if 'loaded_model' in locals():\n",
    "    model = loaded_model\n",
    "else:\n",
    "    model = best_model\n",
    "\n",
    "X_train_eval = X_train_scaled\n",
    "y_train_eval = y_train\n",
    "X_test_eval = X_test_scaled\n",
    "y_test_eval = y_test\n",
    "\n",
    "# Avaliar modelo\n",
    "train_metrics, test_metrics, y_pred = evaluate_model(\n",
    "    model, X_train_eval, X_test_eval, y_train_eval, y_test_eval, MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86722da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados finais usando função do módulo\n",
    "model_final_results = save_final_results(\n",
    "    model_name=MODEL_NAME,\n",
    "    best_params=best_params,\n",
    "    best_score=best_score,\n",
    "    train_metrics=train_metrics,\n",
    "    test_metrics=test_metrics,\n",
    "    y_pred=y_pred,\n",
    "    y_test=y_test_eval,\n",
    "    X_train_scaled=X_train_eval,\n",
    "    X_test_scaled=X_test_eval,\n",
    "    results_folder='results'\n",
    ")\n",
    "\n",
    "# Mostrar resumo\n",
    "print(f\"\\n--- RESUMO {MODEL_NAME} ---\")\n",
    "print(f\"F1-Score CV: {model_final_results['best_cv_score']:.4f}\")\n",
    "print(f\"F1-Score Teste: {test_metrics['f1']:.4f}\")\n",
    "print(f\"Acurácia Teste: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precisão Teste: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall Teste: {test_metrics['recall']:.4f}\")\n",
    "print(f\"G-Mean Teste: {test_metrics['gmean']:.4f}\")\n",
    "if test_metrics['auc_roc']:\n",
    "    print(f\"AUC-ROC Teste: {test_metrics['auc_roc']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
